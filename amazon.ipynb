{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4479a2-21f7-4d6a-ab7b-0b6147d72f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "base_path = kagglehub.dataset_download(\"naveedhn/amazon-product-review-spam-and-non-spam\")\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5512ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\n",
      "Files inside: ['Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry', 'Electronics', 'Home_and_Kitchen', 'part.json', 'separate.json', 'Sports_and_Outdoors', 'Toys_and_Games']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Dataset path:\", base_path)\n",
    "print(\"Files inside:\", os.listdir(base_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4746f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ana klasör: C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\n",
      "['Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry', 'Electronics', 'Home_and_Kitchen', 'part.json', 'separate.json', 'Sports_and_Outdoors', 'Toys_and_Games']\n",
      "\\Electronics içeriği:\n",
      "['Electronics.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\aadil\\AppData\\Local\\Temp\\ipykernel_21024\\2652339389.py:9: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  print(\"\\Electronics içeriği:\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ana klasörde neler var\n",
    "print(\"Ana klasör:\", base_path)\n",
    "print(os.listdir(base_path))\n",
    "\n",
    "# Örneğin ilk klasörün içine bakalım\n",
    "first_folder = os.path.join(base_path, \"Electronics\")\n",
    "print(\"\\Electronics içeriği:\")\n",
    "print(os.listdir(first_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd6c0fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics klasörü içeriği:\n",
      "['Electronics.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "electronics_path = os.path.join(base_path, \"Electronics\")\n",
    "print(\"Electronics klasörü içeriği:\")\n",
    "print(os.listdir(electronics_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc243fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seçilen dosya: C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Electronics\\Electronics.json\n",
      "Dosya boyutu: 5492.40 MB\n",
      "Dosya çok büyük, ilk 1000 satırı okuyorum...\n",
      "Yüklenen veri shape: (1000, 12)\n",
      "İlk 5 satır:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'$oid': '5a13242d741a2384e88f3c11'}</td>\n",
       "      <td>A2WNBOD3WNDNKT</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>JAL</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Some of the functions did not work properly.  ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Disappointing</td>\n",
       "      <td>1374451200</td>\n",
       "      <td>07 22, 2013</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'$oid': '5a13242d741a2384e88f3c0f'}</td>\n",
       "      <td>AKM1MP6P0OYPR</td>\n",
       "      <td>0132793040</td>\n",
       "      <td>Vicki Gibson \"momo4\"</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Corey Barker does a great job of explaining Bl...</td>\n",
       "      <td>5</td>\n",
       "      <td>Very thorough</td>\n",
       "      <td>1365811200</td>\n",
       "      <td>04 13, 2013</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id      reviewerID        asin  \\\n",
       "0  {'$oid': '5a13242d741a2384e88f3c11'}  A2WNBOD3WNDNKT  0439886341   \n",
       "1  {'$oid': '5a13242d741a2384e88f3c0f'}   AKM1MP6P0OYPR  0132793040   \n",
       "\n",
       "           reviewerName helpful  \\\n",
       "0                   JAL  [1, 1]   \n",
       "1  Vicki Gibson \"momo4\"  [1, 1]   \n",
       "\n",
       "                                          reviewText  overall        summary  \\\n",
       "0  Some of the functions did not work properly.  ...        3  Disappointing   \n",
       "1  Corey Barker does a great job of explaining Bl...        5  Very thorough   \n",
       "\n",
       "   unixReviewTime   reviewTime     category  class  \n",
       "0      1374451200  07 22, 2013  Electronics      0  \n",
       "1      1365811200  04 13, 2013  Electronics      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(electronics_path, os.listdir(electronics_path)[0])  # ilk dosyayı seç\n",
    "print(\"Seçilen dosya:\", file_path)\n",
    "\n",
    "# Dosya boyutunu kontrol et\n",
    "file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB cinsinden\n",
    "print(f\"Dosya boyutu: {file_size:.2f} MB\")\n",
    "\n",
    "# Çok büyük dosyalar için güvenli okuma\n",
    "if file_size > 100:  # 100 MB'dan büyükse\n",
    "    print(\"Dosya çok büyük, ilk 1000 satırı okuyorum...\")\n",
    "    df_electronics = pd.read_json(file_path, lines=True, nrows=1000)\n",
    "else:\n",
    "    print(\"Dosya boyutu uygun, tamamını okuyorum...\")\n",
    "    df_electronics = pd.read_json(file_path, lines=True)\n",
    "\n",
    "print(f\"Yüklenen veri shape: {df_electronics.shape}\")\n",
    "print(\"İlk 5 satır:\")\n",
    "df_electronics.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e63968",
   "metadata": {},
   "source": [
    "## Adım 1: Dosyaları bul ve örnek dosya seç\n",
    "Bütün JSON dosyalarını `base_path` altında geziyoruz, ilk örnek dosyayı seçip boyutunu görüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1171f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON dosya sayısı: 10\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Cell_Phones_and_Accessories\\Cell_Phones_and_Accessories.json  (1.87 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Clothing_Shoes_and_Jewelry\\Clothing_Shoes_and_Jewelry.json  (2.99 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Electronics\\Electronics.json  (5.36 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Home_and_Kitchen\\Home_and_Kitchen.json  (2.50 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Sports_and_Outdoors\\Sports_and_Outdoors.json  (1.86 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Toys_and_Games\\Toys_and_Games.json  (1.20 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\part.json  (0.00 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\part.json\\part.json  (0.08 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\separate.json  (0.00 GB)\n",
      "- C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\separate.json\\separate.json  (1.26 GB)\n",
      "Seçilen örnek dosya: C:\\Users\\aadil\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Cell_Phones_and_Accessories\\Cell_Phones_and_Accessories.json\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "base = base_path if 'base_path' in globals() else os.getcwd()\n",
    "json_files = sorted(glob.glob(os.path.join(base, '**', '*.json'), recursive=True))\n",
    "print(f'JSON dosya sayısı: {len(json_files)}')\n",
    "for p in json_files[:10]:\n",
    "    try:\n",
    "        sz = os.path.getsize(p)/(1024*1024*1024)\n",
    "        print(f'- {p}  ({sz:.2f} GB)')\n",
    "    except Exception:\n",
    "        print(f'- {p}')\n",
    "\n",
    "if not json_files:\n",
    "    raise SystemExit('JSON dosyası bulunamadı. İndirme adımını kontrol edin.')\n",
    "sample_path = json_files[0]\n",
    "print('Seçilen örnek dosya:', sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172cdad",
   "metadata": {},
   "source": [
    "## Adım 2: Küçük bir örnek oku (güvenli)\n",
    "Önce `lines=True` (NDJSON) deneriz, olmazsa normal JSON olarak sınırlı satır okuruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da6c0283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Örnek shape: (10000, 12)\n",
      "Sütunlar (ilk 30): ['_id', 'reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText', 'overall', 'summary', 'unixReviewTime', 'reviewTime', 'category', 'class']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'$oid': '5a1321d5741a2384e802c552'}</td>\n",
       "      <td>A3HVRXV0LVJN7</td>\n",
       "      <td>0110400550</td>\n",
       "      <td>BiancaNicole</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>Best phone case ever . Everywhere I go I get a...</td>\n",
       "      <td>5</td>\n",
       "      <td>A++++</td>\n",
       "      <td>1358035200</td>\n",
       "      <td>01 13, 2013</td>\n",
       "      <td>Cell_Phones_and_Accessories</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'$oid': '5a1321d5741a2384e802c557'}</td>\n",
       "      <td>A1BJGDS0L1IO6I</td>\n",
       "      <td>0110400550</td>\n",
       "      <td>cf \"t\"</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>ITEM NOT SENT from Blue Top Company in Hong Ko...</td>\n",
       "      <td>1</td>\n",
       "      <td>ITEM NOT SENT!!</td>\n",
       "      <td>1359504000</td>\n",
       "      <td>01 30, 2013</td>\n",
       "      <td>Cell_Phones_and_Accessories</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'$oid': '5a1321d5741a2384e802c550'}</td>\n",
       "      <td>A1YX2RBMS1L9L</td>\n",
       "      <td>0110400550</td>\n",
       "      <td>Andrea Busch</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Saw this same case at a theme park store for 2...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>1353542400</td>\n",
       "      <td>11 22, 2012</td>\n",
       "      <td>Cell_Phones_and_Accessories</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id      reviewerID        asin  \\\n",
       "0  {'$oid': '5a1321d5741a2384e802c552'}   A3HVRXV0LVJN7  0110400550   \n",
       "1  {'$oid': '5a1321d5741a2384e802c557'}  A1BJGDS0L1IO6I  0110400550   \n",
       "2  {'$oid': '5a1321d5741a2384e802c550'}   A1YX2RBMS1L9L  0110400550   \n",
       "\n",
       "   reviewerName helpful                                         reviewText  \\\n",
       "0  BiancaNicole  [4, 4]  Best phone case ever . Everywhere I go I get a...   \n",
       "1        cf \"t\"  [0, 3]  ITEM NOT SENT from Blue Top Company in Hong Ko...   \n",
       "2  Andrea Busch  [0, 0]  Saw this same case at a theme park store for 2...   \n",
       "\n",
       "   overall          summary  unixReviewTime   reviewTime  \\\n",
       "0        5            A++++      1358035200  01 13, 2013   \n",
       "1        1  ITEM NOT SENT!!      1359504000  01 30, 2013   \n",
       "2        5    Great product      1353542400  11 22, 2012   \n",
       "\n",
       "                      category  class  \n",
       "0  Cell_Phones_and_Accessories      1  \n",
       "1  Cell_Phones_and_Accessories      0  \n",
       "2  Cell_Phones_and_Accessories      1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_json_sample(path: str, nrows: int = 10000) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_json(path, lines=True, nrows=nrows)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return pd.read_json(path, nrows=nrows)\n",
    "        except Exception as e:\n",
    "            print('Okuma hatası:', e)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "df_sample = read_json_sample(sample_path, nrows=10000)\n",
    "print('Örnek shape:', df_sample.shape)\n",
    "print('Sütunlar (ilk 30):', list(df_sample.columns)[:30])\n",
    "df_sample.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758059b",
   "metadata": {},
   "source": [
    "## Adım 3: Tip normalizasyonu ve yararlı alanların türetilmesi\n",
    "- class → label (int)\n",
    "- unixReviewTime → datetime\n",
    "- helpful → helpful_up, helpful_tot, helpful_ratio\n",
    "- reviewText + summary → text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a0c2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dönüşüm sonrası sütunlar (ilk 30): ['_id', 'reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText', 'overall', 'summary', 'unixReviewTime', 'reviewTime', 'category', 'class', 'text', 'text_clean', 'char_len', 'word_len', 'bang_cnt', 'qmark_cnt', 'label', 'review_dt', 'helpful_up', 'helpful_tot', 'helpful_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>label</th>\n",
       "      <th>review_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110400550</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0110400550</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0110400550</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  label  review_dt\n",
       "0  0110400550      1 2013-01-13\n",
       "1  0110400550      0 2013-01-30\n",
       "2  0110400550      1 2012-11-22"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_sample = df_sample.copy()\n",
    "# Etiket\n",
    "if 'class' in df_sample.columns:\n",
    "    df_sample['label'] = pd.to_numeric(df_sample['class'], errors='coerce').fillna(0).astype('int8')\n",
    "elif 'label' in df_sample.columns:\n",
    "    df_sample['label'] = pd.to_numeric(df_sample['label'], errors='coerce').fillna(0).astype('int8')\n",
    "\n",
    "# Tarih\n",
    "if 'unixReviewTime' in df_sample.columns:\n",
    "    df_sample['review_dt'] = pd.to_datetime(df_sample['unixReviewTime'], unit='s', errors='coerce')\n",
    "\n",
    "# Helpful parçalama\n",
    "if 'helpful' in df_sample.columns:\n",
    "    up = df_sample['helpful'].apply(lambda x: x[0] if isinstance(x, (list, tuple)) and len(x) > 0 else np.nan)\n",
    "    tot = df_sample['helpful'].apply(lambda x: x[1] if isinstance(x, (list, tuple)) and len(x) > 1 else np.nan)\n",
    "    df_sample['helpful_up'] = up.astype('float32')\n",
    "    df_sample['helpful_tot'] = tot.astype('float32')\n",
    "    df_sample['helpful_ratio'] = (df_sample['helpful_up'] / df_sample['helpful_tot'].replace(0, np.nan)).astype('float32')\n",
    "\n",
    "# Metin birleştirme\n",
    "rt = df_sample['reviewText'] if 'reviewText' in df_sample.columns else ''\n",
    "sm = df_sample['summary'] if 'summary' in df_sample.columns else ''\n",
    "df_sample['text'] = (rt.fillna('') + ' ' + sm.fillna('')).str.strip()\n",
    "\n",
    "print('Dönüşüm sonrası sütunlar (ilk 30):', list(df_sample.columns)[:30])\n",
    "df_sample[['asin' if 'asin' in df_sample.columns else df_sample.columns[0], 'label' if 'label' in df_sample.columns else df_sample.columns[1], 'review_dt' if 'review_dt' in df_sample.columns else df_sample.columns[2]]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18163e",
   "metadata": {},
   "source": [
    "## Adım 4: Metin temizliği ve basit özellikler\n",
    "Küçük bir temizleme uygular ve uzunluk/işaret sayıları gibi basit özellikler ekleriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "184de396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temizlik sonrası shape: (10000, 23) | Kaldırılan kısa kayıt: 0\n",
      "                                          text_clean  word_len  bang_cnt  \\\n",
      "0  best phone case ever . everywhere i go i get a...        24         0   \n",
      "1  item not sent from blue top company in hong ko...        32         4   \n",
      "2  saw this same case at a theme park store for 2...        23         0   \n",
      "\n",
      "   qmark_cnt  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 'text' kolonu yoksa oluşturalım (reviewText + summary)\n",
    "if 'text' not in df_sample.columns:\n",
    "    rt = df_sample['reviewText'] if 'reviewText' in df_sample.columns else ''\n",
    "    sm = df_sample['summary'] if 'summary' in df_sample.columns else ''\n",
    "    df_sample['text'] = (rt.fillna('') + ' ' + sm.fillna('')).str.strip()\n",
    "\n",
    "\n",
    "def clean_text_basic(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'https?://\\S+|www\\.\\S+', ' ', s)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    s = re.sub(r'[\\r\\n\\t]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "# Temiz metin ve basit özellikler\n",
    "df_sample['text_clean'] = df_sample['text'].fillna('').map(clean_text_basic)\n",
    "\n",
    "df_sample['char_len'] = df_sample['text_clean'].str.len().astype('int32')\n",
    "df_sample['word_len'] = df_sample['text_clean'].str.split().map(len).astype('int32')\n",
    "# .str.count regex kullanır; ? işaretini saymak için kaçış gerekir\n",
    "df_sample['bang_cnt'] = df_sample['text_clean'].str.count(r'!').astype('int16')\n",
    "df_sample['qmark_cnt'] = df_sample['text_clean'].str.count(r'\\?').astype('int16')\n",
    "\n",
    "# Çok kısa metinleri ele (isteğe bağlı)\n",
    "before = len(df_sample)\n",
    "df_sample = df_sample[df_sample['word_len'] >= 3].reset_index(drop=True)\n",
    "removed = before - len(df_sample)\n",
    "\n",
    "print('Temizlik sonrası shape:', df_sample.shape, '| Kaldırılan kısa kayıt:', removed)\n",
    "print(df_sample[['text_clean','word_len','bang_cnt','qmark_cnt']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf15c17",
   "metadata": {},
   "source": [
    "## Adım 5: Önizleme ve (isteğe bağlı) kayıt\n",
    "Önemli sütunları gösterir ve küçük bir örneği Parquet olarak kaydederiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3248fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Önizleme sütunları: ['asin', 'reviewerID', 'category', 'overall', 'label', 'review_dt', 'helpful_up', 'helpful_tot', 'helpful_ratio', 'word_len', 'char_len', 'bang_cnt', 'qmark_cnt', 'text_clean']\n",
      "         asin      reviewerID                     category  overall  label  \\\n",
      "0  0110400550   A3HVRXV0LVJN7  Cell_Phones_and_Accessories        5      1   \n",
      "1  0110400550  A1BJGDS0L1IO6I  Cell_Phones_and_Accessories        1      0   \n",
      "2  0110400550   A1YX2RBMS1L9L  Cell_Phones_and_Accessories        5      1   \n",
      "3  0110400550  A180NNPPKWCCU0  Cell_Phones_and_Accessories        5      1   \n",
      "4  0110400550  A30P2CYOUYAJM8  Cell_Phones_and_Accessories        4      1   \n",
      "\n",
      "   review_dt  helpful_up  helpful_tot  helpful_ratio  word_len  char_len  \\\n",
      "0 2013-01-13         4.0          4.0            1.0        24       115   \n",
      "1 2013-01-30         0.0          3.0            0.0        32       158   \n",
      "2 2012-11-22         0.0          0.0            NaN        23       115   \n",
      "3 2013-07-18         3.0          3.0            1.0        21       127   \n",
      "4 2013-03-20         1.0          1.0            1.0        22       114   \n",
      "\n",
      "   bang_cnt  qmark_cnt                                         text_clean  \n",
      "0         0          0  best phone case ever . everywhere i go i get a...  \n",
      "1         4          0  item not sent from blue top company in hong ko...  \n",
      "2         0          0  saw this same case at a theme park store for 2...  \n",
      "3         0          0  case fits perfectly and i always gets complime...  \n",
      "4         0          0  i got this for my 14 year old sister. she love...  \n",
      "Kaydedildi: c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\preprocessed_sample.parquet\n"
     ]
    }
   ],
   "source": [
    "keep_cols = [c for c in [\n",
    "    'asin','reviewerID','category','overall','label',\n",
    "    'review_dt','helpful_up','helpful_tot','helpful_ratio',\n",
    "    'word_len','char_len','bang_cnt','qmark_cnt','text_clean'\n",
    "] if c in df_sample.columns]\n",
    "\n",
    "print('Önizleme sütunları:', keep_cols)\n",
    "print(df_sample[keep_cols].head(5))\n",
    "\n",
    "out_path = os.path.join(os.getcwd(), 'preprocessed_sample.parquet')\n",
    "try:\n",
    "    df_sample[keep_cols].to_parquet(out_path, index=False)\n",
    "    print('Kaydedildi:', out_path)\n",
    "except Exception as e:\n",
    "    print('Parquet kaydı başarısız olabilir (pyarrow gerekli). Hata:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052111dc",
   "metadata": {},
   "source": [
    "## Adım 6: Kaydedilen Parquet dosyasını özetle\n",
    "Bu hücre, 'preprocessed_sample.parquet' dosyasını yükleyip satır/sütun sayısı, sütun tipleri, label ve kategori dağılımı, tarih aralığı ve null değerleri raporlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14c44c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya: c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\preprocessed_sample.parquet\n",
      "Shape: 10000 satır x 14 sütun | ~5.76 MB\n",
      "\n",
      "Sütun tipleri:\n",
      "asin                     object\n",
      "reviewerID               object\n",
      "category                 object\n",
      "overall                   int64\n",
      "label                      int8\n",
      "review_dt        datetime64[ns]\n",
      "helpful_up              float32\n",
      "helpful_tot             float32\n",
      "helpful_ratio           float32\n",
      "word_len                  int32\n",
      "char_len                  int32\n",
      "bang_cnt                  int16\n",
      "qmark_cnt                 int16\n",
      "text_clean               object\n",
      "dtype: object\n",
      "\n",
      "Head:\n",
      "         asin      reviewerID                     category  overall  label  \\\n",
      "0  0110400550   A3HVRXV0LVJN7  Cell_Phones_and_Accessories        5      1   \n",
      "1  0110400550  A1BJGDS0L1IO6I  Cell_Phones_and_Accessories        1      0   \n",
      "2  0110400550   A1YX2RBMS1L9L  Cell_Phones_and_Accessories        5      1   \n",
      "\n",
      "   review_dt  helpful_up  helpful_tot  helpful_ratio  word_len  char_len  \\\n",
      "0 2013-01-13         4.0          4.0            1.0        24       115   \n",
      "1 2013-01-30         0.0          3.0            0.0        32       158   \n",
      "2 2012-11-22         0.0          0.0            NaN        23       115   \n",
      "\n",
      "   bang_cnt  qmark_cnt                                         text_clean  \n",
      "0         0          0  best phone case ever . everywhere i go i get a...  \n",
      "1         4          0  item not sent from blue top company in hong ko...  \n",
      "2         0          0  saw this same case at a theme park store for 2...  \n",
      "\n",
      "Label dağılımı (adet / oran):\n",
      "label\n",
      "1    6347\n",
      "0    3653\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    0.635\n",
      "0    0.365\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Kategori dağılımı (ilk 10):\n",
      "category\n",
      "Cell_Phones_and_Accessories    10000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tarih aralığı:\n",
      "min: 1999-11-17 00:00:00 | max: 2014-07-23 00:00:00\n",
      "\n",
      "Sayısal kolon özetleri:\n",
      "                 count        mean         std   min         25%    50%  \\\n",
      "overall        10000.0    3.640100    1.560261   1.0    2.000000    4.0   \n",
      "label          10000.0    0.634700    0.481538   0.0    0.000000    1.0   \n",
      "helpful_up     10000.0    1.267600    6.576293   0.0    0.000000    0.0   \n",
      "helpful_tot    10000.0    1.528800    7.018715   0.0    0.000000    0.0   \n",
      "helpful_ratio   3054.0    0.776776    0.345067   0.0    0.666667    1.0   \n",
      "word_len       10000.0   59.391900   59.027100   3.0   27.000000   40.0   \n",
      "char_len       10000.0  316.587100  322.326359  13.0  144.000000  210.0   \n",
      "bang_cnt       10000.0    0.639600    1.676303   0.0    0.000000    0.0   \n",
      "qmark_cnt      10000.0    0.062800    0.444383   0.0    0.000000    0.0   \n",
      "\n",
      "                 75%     max  \n",
      "overall          5.0     5.0  \n",
      "label            1.0     1.0  \n",
      "helpful_up       1.0   428.0  \n",
      "helpful_tot      1.0   441.0  \n",
      "helpful_ratio    1.0     1.0  \n",
      "word_len        69.0  1568.0  \n",
      "char_len       366.0  8948.0  \n",
      "bang_cnt         1.0    47.0  \n",
      "qmark_cnt        0.0    14.0  \n",
      "\n",
      "Null değer sayıları:\n",
      "asin                0\n",
      "reviewerID          0\n",
      "category            0\n",
      "overall             0\n",
      "label               0\n",
      "review_dt           0\n",
      "helpful_up          0\n",
      "helpful_tot         0\n",
      "helpful_ratio    6946\n",
      "word_len            0\n",
      "char_len            0\n",
      "bang_cnt            0\n",
      "qmark_cnt           0\n",
      "text_clean          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pp_path = os.path.join(os.getcwd(), 'preprocessed_sample.parquet')\n",
    "print('Dosya:', pp_path)\n",
    "if not os.path.exists(pp_path):\n",
    "    raise SystemExit('Parquet bulunamadı. Önce Adım 5 ile üretin.')\n",
    "\n",
    "# Parquet'i yükle\n",
    "try:\n",
    "    df_pp = pd.read_parquet(pp_path)\n",
    "except Exception as e:\n",
    "    print('read_parquet başarısız:', e)\n",
    "    print('PyArrow veya fastparquet kurulu mu?')\n",
    "    raise\n",
    "\n",
    "# Boyut / hafıza\n",
    "rows, cols = df_pp.shape\n",
    "mem_mb = df_pp.memory_usage(deep=True).sum() / (1024*1024)\n",
    "print(f'Shape: {rows} satır x {cols} sütun | ~{mem_mb:.2f} MB')\n",
    "\n",
    "# Sütunlar ve tipler\n",
    "print('\\nSütun tipleri:')\n",
    "print(df_pp.dtypes)\n",
    "\n",
    "# İlk satırlar\n",
    "print('\\nHead:')\n",
    "print(df_pp.head(3))\n",
    "\n",
    "# Label dağılımı\n",
    "if 'label' in df_pp.columns:\n",
    "    print('\\nLabel dağılımı (adet / oran):')\n",
    "    vc = df_pp['label'].value_counts(dropna=False)\n",
    "    print(vc)\n",
    "    print((vc/len(df_pp)).round(3))\n",
    "\n",
    "# Kategori dağılımı\n",
    "if 'category' in df_pp.columns:\n",
    "    print('\\nKategori dağılımı (ilk 10):')\n",
    "    print(df_pp['category'].value_counts().head(10))\n",
    "\n",
    "# Tarih aralığı\n",
    "if 'review_dt' in df_pp.columns:\n",
    "    print('\\nTarih aralığı:')\n",
    "    print('min:', df_pp['review_dt'].min(), '| max:', df_pp['review_dt'].max())\n",
    "\n",
    "# Numerik özet\n",
    "num_cols = df_pp.select_dtypes(include=['number']).columns.tolist()\n",
    "if num_cols:\n",
    "    print('\\nSayısal kolon özetleri:')\n",
    "    print(df_pp[num_cols].describe().T.head(15))\n",
    "\n",
    "# Null değerler\n",
    "print('\\nNull değer sayıları:')\n",
    "print(df_pp.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f3130",
   "metadata": {},
   "source": [
    "# Machine Learning: Spam Detection Model\n",
    "\n",
    "Şimdi temizlenmiş veriyi kullanarak spam detection modeli kuracağız:\n",
    "1. **TF-IDF Vectorization**: Metni sayısal özelliklere dönüştür\n",
    "2. **Train/Test Split**: Veriyi eğitim ve test setlerine böl  \n",
    "3. **LogisticRegression**: Baseline model ile başla\n",
    "4. **Evaluation**: Accuracy, precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9eff14",
   "metadata": {},
   "source": [
    "## Adım 7: TF-IDF Feature Extraction\n",
    "Temizlenmiş metinleri sayısal özelliklere dönüştürüyoruz. TF-IDF hem kelime hem de karakter n-gramları kullanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5eaa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yüklenen veri shape: (10000, 14)\n",
      "Text kolonu: text_clean\n",
      "Numerik kolonlar: ['word_len', 'char_len', 'bang_cnt', 'qmark_cnt', 'helpful_ratio', 'overall']\n",
      "TF-IDF word features hesaplanıyor...\n",
      "Word TF-IDF shape: (10000, 5000)\n",
      "TF-IDF char features hesaplanıyor...\n",
      "Char TF-IDF shape: (10000, 2000)\n",
      "Numeric features shape: (10000, 6)\n",
      "Feature extraction tamamlandı!\n"
     ]
    }
   ],
   "source": [
    "# Önce parquet'i yükle (eğer df_pp zaten mevcut değilse)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if 'df_pp' not in globals():\n",
    "    pp_path = os.path.join(os.getcwd(), 'preprocessed_sample.parquet')\n",
    "    if not os.path.exists(pp_path):\n",
    "        raise FileNotFoundError(f'Parquet dosyası bulunamadı: {pp_path}. Önce Adım 5 çalıştırın.')\n",
    "    df_pp = pd.read_parquet(pp_path)\n",
    "\n",
    "print('Yüklenen veri shape:', df_pp.shape)\n",
    "\n",
    "# TF-IDF için gerekli importlar\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Text ve numerik özellikleri ayır\n",
    "text_col = 'text_clean'\n",
    "numeric_cols = [c for c in ['word_len', 'char_len', 'bang_cnt', 'qmark_cnt', 'helpful_ratio', 'overall'] \n",
    "                if c in df_pp.columns]\n",
    "\n",
    "print('Text kolonu:', text_col)\n",
    "print('Numerik kolonlar:', numeric_cols)\n",
    "\n",
    "# Text kolonu kontrolü\n",
    "if text_col not in df_pp.columns:\n",
    "    raise ValueError(f\"'{text_col}' kolonu bulunamadı! Önce Adım 4 çalıştırın.\")\n",
    "\n",
    "# TF-IDF parametreleri (spam detection için optimize edilmiş)\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),        # unigram + bigram\n",
    "    max_features=5000,         # top 5K word features\n",
    "    min_df=3,                  # en az 3 dokümanda geçmeli\n",
    "    max_df=0.8,               # max %80 dokümanda geçebilir\n",
    "    stop_words='english',     # İngilizce stop words\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\b[a-zA-Z]{2,}\\b'  # en az 2 harfli kelimeler\n",
    ")\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),        # 3-5 karakter n-gramları\n",
    "    max_features=2000,         # top 2K char features\n",
    "    min_df=3,\n",
    "    max_df=0.8,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Text features\n",
    "print('TF-IDF word features hesaplanıyor...')\n",
    "X_text_word = tfidf_word.fit_transform(df_pp[text_col].fillna(''))\n",
    "print('Word TF-IDF shape:', X_text_word.shape)\n",
    "\n",
    "print('TF-IDF char features hesaplanıyor...')\n",
    "X_text_char = tfidf_char.fit_transform(df_pp[text_col].fillna(''))\n",
    "print('Char TF-IDF shape:', X_text_char.shape)\n",
    "\n",
    "# Numerik features (standardize)\n",
    "if numeric_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric = scaler.fit_transform(df_pp[numeric_cols].fillna(0))\n",
    "    print('Numeric features shape:', X_numeric.shape)\n",
    "else:\n",
    "    X_numeric = np.empty((len(df_pp), 0))\n",
    "\n",
    "print('Feature extraction tamamlandı!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949806c",
   "metadata": {},
   "source": [
    "## Adım 8: Train/Test Split ve Label Hazırlığı\n",
    "Veriyi eğitim ve test setlerine bölüyoruz. Aynı reviewerID'nin farklı setlerde olmamasını sağlayarak data leakage'i önlüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5ab7e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label dağılımı:\n",
      "1    6347\n",
      "0    3653\n",
      "Name: count, dtype: int64\n",
      "Spam oranı: 0.6347\n",
      "Birleşik feature matrix shape: (10000, 7006)\n",
      "Feature sayısı: 7006\n",
      "Reviewer-based split yapıldı\n",
      "Train reviewers: 7802, Test reviewers: 1951\n",
      "Train shape: (8002, 7006), y_train spam oranı: 0.634\n",
      "Test shape: (1998, 7006), y_test spam oranı: 0.636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "\n",
    "# Label kontrolü\n",
    "if 'label' not in df_pp.columns:\n",
    "    raise ValueError('Label kolonu bulunamadı! Adım 3 çalıştırıldı mı?')\n",
    "\n",
    "y = df_pp['label'].values\n",
    "print('Label dağılımı:')\n",
    "print(pd.Series(y).value_counts())\n",
    "print('Spam oranı:', pd.Series(y).mean())\n",
    "\n",
    "# Tüm feature'ları birleştir (sparse matrices)\n",
    "if X_numeric.shape[1] > 0:\n",
    "    # Convert numeric to sparse for concatenation\n",
    "    from scipy.sparse import csr_matrix\n",
    "    X_numeric_sparse = csr_matrix(X_numeric)\n",
    "    X_combined = hstack([X_text_word, X_text_char, X_numeric_sparse])\n",
    "    feature_names = (['word_' + str(i) for i in range(X_text_word.shape[1])] + \n",
    "                    ['char_' + str(i) for i in range(X_text_char.shape[1])] + \n",
    "                    [f'num_{col}' for col in numeric_cols])\n",
    "else:\n",
    "    X_combined = hstack([X_text_word, X_text_char])\n",
    "    feature_names = (['word_' + str(i) for i in range(X_text_word.shape[1])] + \n",
    "                    ['char_' + str(i) for i in range(X_text_char.shape[1])])\n",
    "\n",
    "print('Birleşik feature matrix shape:', X_combined.shape)\n",
    "print('Feature sayısı:', len(feature_names))\n",
    "\n",
    "# Train/test split (reviewer-based split data leakage'i önlemek için)\n",
    "if 'reviewerID' in df_pp.columns:\n",
    "    # ReviewerID'ye göre split yap\n",
    "    unique_reviewers = df_pp['reviewerID'].unique()\n",
    "    train_reviewers, test_reviewers = train_test_split(\n",
    "        unique_reviewers, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mask = df_pp['reviewerID'].isin(train_reviewers)\n",
    "    test_mask = df_pp['reviewerID'].isin(test_reviewers)\n",
    "    \n",
    "    # Sparse matrix için boolean mask'i numpy array'e çevir\n",
    "    train_idx = train_mask.values  # pandas Series → numpy array\n",
    "    test_idx = test_mask.values\n",
    "    \n",
    "    X_train, X_test = X_combined[train_idx], X_combined[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    print('Reviewer-based split yapıldı')\n",
    "    print(f'Train reviewers: {len(train_reviewers)}, Test reviewers: {len(test_reviewers)}')\n",
    "else:\n",
    "    # Normal random split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print('Random split yapıldı')\n",
    "\n",
    "print(f'Train shape: {X_train.shape}, y_train spam oranı: {y_train.mean():.3f}')\n",
    "print(f'Test shape: {X_test.shape}, y_test spam oranı: {y_test.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323bdd7",
   "metadata": {},
   "source": [
    "## Adım 9: LogisticRegression Model Eğitimi\n",
    "Spam detection için LogisticRegression baseline modeli eğitiyoruz. Class imbalance için `class_weight='balanced'` kullanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fa75ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model eğitimi başlıyor...\n",
      "Eğitim süresi: 4.27 saniye\n",
      "Tahminler yapılıyor...\n",
      "Model eğitimi ve tahmin tamamlandı!\n",
      "Eğitim süresi: 4.27 saniye\n",
      "Tahminler yapılıyor...\n",
      "Model eğitimi ve tahmin tamamlandı!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import time\n",
    "\n",
    "# LogisticRegression model (class imbalance için balanced)\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',   # imbalanced data için ağırlık ayarı\n",
    "    max_iter=1000,            # yeterli iterasyon\n",
    "    random_state=42,\n",
    "    n_jobs=-1                 # paralel işlem\n",
    ")\n",
    "\n",
    "print('Model eğitimi başlıyor...')\n",
    "start_time = time.time()\n",
    "\n",
    "# Eğitim\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f'Eğitim süresi: {train_time:.2f} saniye')\n",
    "\n",
    "# Tahminler\n",
    "print('Tahminler yapılıyor...')\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Probability scores (ROC-AUC için)\n",
    "y_train_proba = lr_model.predict_proba(X_train)[:, 1]\n",
    "y_test_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('Model eğitimi ve tahmin tamamlandı!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67f6b7",
   "metadata": {},
   "source": [
    "## Adım 10: Model Performans Değerlendirmesi\n",
    "Train ve test setleri için accuracy, precision, recall, F1-score ve ROC-AUC metriklerini hesaplıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd787666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL PERFORMANSI ===\n",
      "Metrik          Train      Test       Fark      \n",
      "--------------------------------------------------\n",
      "Accuracy        1.0000     1.0000     0.0000    \n",
      "Precision       1.0000     1.0000     0.0000    \n",
      "Recall          1.0000     1.0000     0.0000    \n",
      "F1-Score        1.0000     1.0000     0.0000    \n",
      "ROC-AUC         1.0000     1.0000     0.0000    \n",
      "\n",
      "=== CONFUSION MATRIX (Test Set) ===\n",
      "Predicted:   0 (Not Spam)  1 (Spam)\n",
      "Actual 0:    728          0      \n",
      "Actual 1:    0            1270   \n",
      "\n",
      "=== CLASSIFICATION REPORT (Test Set) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Spam       1.00      1.00      1.00       728\n",
      "        Spam       1.00      1.00      1.00      1270\n",
      "\n",
      "    accuracy                           1.00      1998\n",
      "   macro avg       1.00      1.00      1.00      1998\n",
      "weighted avg       1.00      1.00      1.00      1998\n",
      "\n",
      "\n",
      "✅ İyi: Train-test F1 farkı 0.0000 < 0.05, overfitting düşük\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Train set performansı\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "train_auc = roc_auc_score(y_train, y_train_proba)\n",
    "\n",
    "# Test set performansı  \n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print('=== MODEL PERFORMANSI ===')\n",
    "print(f'{\"Metrik\":<15} {\"Train\":<10} {\"Test\":<10} {\"Fark\":<10}')\n",
    "print('-' * 50)\n",
    "print(f'{\"Accuracy\":<15} {train_acc:<10.4f} {test_acc:<10.4f} {abs(train_acc-test_acc):<10.4f}')\n",
    "print(f'{\"Precision\":<15} {train_prec:<10.4f} {test_prec:<10.4f} {abs(train_prec-test_prec):<10.4f}')\n",
    "print(f'{\"Recall\":<15} {train_rec:<10.4f} {test_rec:<10.4f} {abs(train_rec-test_rec):<10.4f}')\n",
    "print(f'{\"F1-Score\":<15} {train_f1:<10.4f} {test_f1:<10.4f} {abs(train_f1-test_f1):<10.4f}')\n",
    "print(f'{\"ROC-AUC\":<15} {train_auc:<10.4f} {test_auc:<10.4f} {abs(train_auc-test_auc):<10.4f}')\n",
    "\n",
    "print('\\n=== CONFUSION MATRIX (Test Set) ===')\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print('Predicted:   0 (Not Spam)  1 (Spam)')\n",
    "print(f'Actual 0:    {cm[0,0]:<12} {cm[0,1]:<7}')\n",
    "print(f'Actual 1:    {cm[1,0]:<12} {cm[1,1]:<7}')\n",
    "\n",
    "print('\\n=== CLASSIFICATION REPORT (Test Set) ===')\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Not Spam', 'Spam']))\n",
    "\n",
    "# Overfitting kontrolü\n",
    "train_test_diff = abs(train_f1 - test_f1)\n",
    "if train_test_diff > 0.05:\n",
    "    print(f'\\n⚠️  UYARI: Train-test F1 farkı {train_test_diff:.4f} > 0.05, overfitting olabilir')\n",
    "else:\n",
    "    print(f'\\n✅ İyi: Train-test F1 farkı {train_test_diff:.4f} < 0.05, overfitting düşük')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22d8ce",
   "metadata": {},
   "source": [
    "## Adım 11: Feature Importance Analizi\n",
    "En önemli spam göstergesi kelime/özellikler hangileri? LogisticRegression katsayılarını inceliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "781e3339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EN ÖNEMLİ FEATURES (Top 20) ===\n",
      "Rank  Feature                   Coefficient  Importance  \n",
      "------------------------------------------------------------\n",
      "1     num_overall               12.150793    12.150793   \n",
      "2     WORD: ok                  -0.387522    0.387522    \n",
      "3     WORD: great               0.221093     0.221093    \n",
      "4     WORD: good                0.199387     0.199387    \n",
      "5     num_bang_cnt              0.198303     0.198303    \n",
      "6     WORD: works               0.190480     0.190480    \n",
      "7     CHAR: but                 -0.175405    0.175405    \n",
      "8     CHAR: but                 -0.172204    0.172204    \n",
      "9     CHAR:  but                -0.171759    0.171759    \n",
      "10    CHAR:  but                -0.170550    0.170550    \n",
      "11    CHAR: ut                  -0.158115    0.158115    \n",
      "12    WORD: broke               -0.149833    0.149833    \n",
      "13    CHAR:  bu                 -0.140862    0.140862    \n",
      "14    CHAR:  not                -0.135660    0.135660    \n",
      "15    CHAR:  not                -0.134234    0.134234    \n",
      "16    CHAR: not                 -0.130293    0.130293    \n",
      "17    WORD: nice                0.125457     0.125457    \n",
      "18    WORD: okay                -0.124453    0.124453    \n",
      "19    CHAR: ot                  -0.122425    0.122425    \n",
      "20    CHAR: not                 -0.121877    0.121877    \n",
      "\n",
      "=== SPAM vs NOT SPAM FEATURES ===\n",
      "Pozitif katsayı = SPAM indicator\n",
      "Negatif katsayı = NOT SPAM indicator\n",
      "\n",
      "Top 10 SPAM kelimeler:\n",
      "  great: 0.2211\n",
      "  good: 0.1994\n",
      "  works: 0.1905\n",
      "  nice: 0.1255\n",
      "  far: 0.1136\n",
      "  easy: 0.1093\n",
      "  love: 0.1026\n",
      "  ve: 0.0978\n",
      "  works great: 0.0955\n",
      "  recommend: 0.0936\n",
      "\n",
      "Top 10 NOT SPAM kelimeler:\n",
      "  ok: -0.3875\n",
      "  broke: -0.1498\n",
      "  okay: -0.1245\n",
      "  long: -0.1034\n",
      "\n",
      "Top 10 SPAM kelimeler:\n",
      "  great: 0.2211\n",
      "  good: 0.1994\n",
      "  works: 0.1905\n",
      "  nice: 0.1255\n",
      "  far: 0.1136\n",
      "  easy: 0.1093\n",
      "  love: 0.1026\n",
      "  ve: 0.0978\n",
      "  works great: 0.0955\n",
      "  recommend: 0.0936\n",
      "\n",
      "Top 10 NOT SPAM kelimeler:\n",
      "  ok: -0.3875\n",
      "  broke: -0.1498\n",
      "  okay: -0.1245\n",
      "  long: -0.1034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# LogisticRegression katsayıları\n",
    "coefficients = lr_model.coef_[0]\n",
    "\n",
    "# Feature importance (mutlak değerler)\n",
    "feature_importance = np.abs(coefficients)\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "print('=== EN ÖNEMLİ FEATURES (Top 20) ===')\n",
    "print(f'{\"Rank\":<5} {\"Feature\":<25} {\"Coefficient\":<12} {\"Importance\":<12}')\n",
    "print('-' * 60)\n",
    "\n",
    "for i, idx in enumerate(sorted_idx[:20]):\n",
    "    if i < len(feature_names):\n",
    "        feat_name = feature_names[idx]\n",
    "        coef_val = coefficients[idx]\n",
    "        importance = feature_importance[idx]\n",
    "        \n",
    "        # Feature tipini belirle\n",
    "        if feat_name.startswith('word_'):\n",
    "            # Word feature'dan gerçek kelimeyi çıkar\n",
    "            word_idx = int(feat_name.split('_')[1])\n",
    "            if word_idx < len(tfidf_word.get_feature_names_out()):\n",
    "                actual_word = tfidf_word.get_feature_names_out()[word_idx]\n",
    "                display_name = f'WORD: {actual_word}'\n",
    "            else:\n",
    "                display_name = feat_name\n",
    "        elif feat_name.startswith('char_'):\n",
    "            # Char feature'dan gerçek n-gramı çıkar  \n",
    "            char_idx = int(feat_name.split('_')[1])\n",
    "            if char_idx < len(tfidf_char.get_feature_names_out()):\n",
    "                actual_char = tfidf_char.get_feature_names_out()[char_idx]\n",
    "                display_name = f'CHAR: {actual_char}'\n",
    "            else:\n",
    "                display_name = feat_name\n",
    "        else:\n",
    "            display_name = feat_name\n",
    "            \n",
    "        print(f'{i+1:<5} {display_name:<25} {coef_val:<12.6f} {importance:<12.6f}')\n",
    "\n",
    "# Spam vs Not Spam indicator katsayıları\n",
    "print('\\n=== SPAM vs NOT SPAM FEATURES ===')\n",
    "print('Pozitif katsayı = SPAM indicator')\n",
    "print('Negatif katsayı = NOT SPAM indicator')\n",
    "\n",
    "spam_features = []\n",
    "not_spam_features = []\n",
    "\n",
    "for i, idx in enumerate(sorted_idx[:50]):  # top 50'yi incele\n",
    "    if i < len(feature_names):\n",
    "        feat_name = feature_names[idx]\n",
    "        coef_val = coefficients[idx]\n",
    "        \n",
    "        if feat_name.startswith('word_'):\n",
    "            word_idx = int(feat_name.split('_')[1])\n",
    "            if word_idx < len(tfidf_word.get_feature_names_out()):\n",
    "                actual_word = tfidf_word.get_feature_names_out()[word_idx]\n",
    "                if coef_val > 0:\n",
    "                    spam_features.append((actual_word, coef_val))\n",
    "                else:\n",
    "                    not_spam_features.append((actual_word, coef_val))\n",
    "\n",
    "print('\\nTop 10 SPAM kelimeler:')\n",
    "for word, coef in sorted(spam_features, key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f'  {word}: {coef:.4f}')\n",
    "    \n",
    "print('\\nTop 10 NOT SPAM kelimeler:')\n",
    "for word, coef in sorted(not_spam_features, key=lambda x: x[1])[:10]:\n",
    "    print(f'  {word}: {coef:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overfits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
