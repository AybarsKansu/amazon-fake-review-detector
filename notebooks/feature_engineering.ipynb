{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurulum tamamlandı.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import save_npz, vstack, hstack\n",
    "\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords indiriliyor...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "INPUT_PARQUET_PATH = \"..\\\\data\\\\raw\\\\labeled_reviews.parquet\"\n",
    "OUTPUT_DIR = \"..\\\\data\\\\processed\\\\\"\n",
    "\n",
    "TEXT_ONLY_DIR = os.path.join(OUTPUT_DIR, \"text_data\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "os.makedirs(TEXT_ONLY_DIR, exist_ok=True)\n",
    "os.makedirs(HYBRID_DIR, exist_ok=True)\n",
    "INPUT_BASE_PATH = 'C:\\\\Users\\\\Aybars\\\\.cache\\\\kagglehub\\\\datasets\\\\naveedhn\\\\amazon-product-review-spam-and-non-spam\\\\versions\\\\1'\n",
    "BASE_DIR = \"C:\\\\work environment\\\\Projects\\\\amazon-spam-review\\\\\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"data\\\\processed\\\\\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "\n",
    "VECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def create_behavioral_features(df, reviewer_counts_train):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    df_out['review_count'] = df['reviewerID'].map(reviewer_counts_train).fillna(0)\n",
    "    return df_out.values\n",
    "\n",
    "print(\"Kurulum tamamlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e222669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Tüm kategoriler için hibrit veri setini sıfırdan oluşturma işlemi başlıyor.\n",
      "Kaynak: Ham JSON verileri okunacak (C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1)\n",
      "✅ Toplam 6 kategori bulundu: ['Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry', 'Electronics', 'Home_and_Kitchen', 'Sports_and_Outdoors', 'Toys_and_Games']\n",
      "\n",
      "============================================================\n",
      "--- İşleniyor: Cell_Phones_and_Accessories ---\n",
      "============================================================\n",
      "✅ 'Cell_Phones_and_Accessories' için dosyalar zaten mevcut, bu kategori atlanıyor.\n",
      "\n",
      "============================================================\n",
      "--- İşleniyor: Clothing_Shoes_and_Jewelry ---\n",
      "============================================================\n",
      "✅ 'Clothing_Shoes_and_Jewelry' için dosyalar zaten mevcut, bu kategori atlanıyor.\n",
      "\n",
      "============================================================\n",
      "--- İşleniyor: Electronics ---\n",
      "============================================================\n",
      "💾 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Electronics\\Electronics.json' okunuyor...\n",
      "✅ 'Electronics' için 7574169 etiketli yorum yüklendi.\n",
      "🪓 Veri, eğitim ve test setlerine ayrılıyor...\n",
      "🏷️  Cevaplar (y) ayrılıyor...\n",
      "🧹 Metin verileri temizleniyor...\n",
      "📝 TF-IDF vektörleştirici SADECE eğitim verisiyle eğitiliyor...\n",
      "📊 Davranışsal özellikler oluşturuluyor...\n",
      "➕ Özellikler birleştiriliyor ve kaydediliyor...\n",
      "✅ 'Electronics' için sızıntısız veri seti başarıyla oluşturuldu.\n",
      "\n",
      "============================================================\n",
      "--- İşleniyor: Home_and_Kitchen ---\n",
      "============================================================\n",
      "💾 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Home_and_Kitchen\\Home_and_Kitchen.json' okunuyor...\n",
      "✅ 'Home_and_Kitchen' için 3988482 etiketli yorum yüklendi.\n",
      "🪓 Veri, eğitim ve test setlerine ayrılıyor...\n",
      "🏷️  Cevaplar (y) ayrılıyor...\n",
      "🧹 Metin verileri temizleniyor...\n",
      "📝 TF-IDF vektörleştirici SADECE eğitim verisiyle eğitiliyor...\n",
      "📊 Davranışsal özellikler oluşturuluyor...\n",
      "➕ Özellikler birleştiriliyor ve kaydediliyor...\n",
      "✅ 'Home_and_Kitchen' için sızıntısız veri seti başarıyla oluşturuldu.\n",
      "\n",
      "============================================================\n",
      "--- İşleniyor: Sports_and_Outdoors ---\n",
      "============================================================\n",
      "💾 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Sports_and_Outdoors\\Sports_and_Outdoors.json' okunuyor...\n",
      "✅ 'Sports_and_Outdoors' için 3013256 etiketli yorum yüklendi.\n",
      "🪓 Veri, eğitim ve test setlerine ayrılıyor...\n",
      "🏷️  Cevaplar (y) ayrılıyor...\n",
      "🧹 Metin verileri temizleniyor...\n",
      "📝 TF-IDF vektörleştirici SADECE eğitim verisiyle eğitiliyor...\n",
      "📊 Davranışsal özellikler oluşturuluyor...\n",
      "➕ Özellikler birleştiriliyor ve kaydediliyor...\n",
      "✅ 'Sports_and_Outdoors' için sızıntısız veri seti başarıyla oluşturuldu.\n",
      "\n",
      "============================================================\n",
      "--- İşleniyor: Toys_and_Games ---\n",
      "============================================================\n",
      "💾 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Toys_and_Games\\Toys_and_Games.json' okunuyor...\n",
      "✅ 'Toys_and_Games' için 1997140 etiketli yorum yüklendi.\n",
      "🪓 Veri, eğitim ve test setlerine ayrılıyor...\n",
      "🏷️  Cevaplar (y) ayrılıyor...\n",
      "🧹 Metin verileri temizleniyor...\n",
      "📝 TF-IDF vektörleştirici SADECE eğitim verisiyle eğitiliyor...\n",
      "📊 Davranışsal özellikler oluşturuluyor...\n",
      "➕ Özellikler birleştiriliyor ve kaydediliyor...\n",
      "✅ 'Toys_and_Games' için sızıntısız veri seti başarıyla oluşturuldu.\n",
      "\n",
      "============================================================\n",
      "🎉 Tüm kategorilerin işlenmesi tamamlandı!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.sparse import hstack, save_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(\"🚀 Tüm kategoriler için hibrit veri setini sıfırdan oluşturma işlemi başlıyor.\")\n",
    "print(f\"Kaynak: Ham JSON verileri okunacak ({INPUT_BASE_PATH})\")\n",
    "\n",
    "# Işlenecek tüm kategori klasörlerinin tam yollarını bul\n",
    "category_paths = glob.glob(os.path.join(INPUT_BASE_PATH, '*/'))\n",
    "\n",
    "if not category_paths:\n",
    "    print(f\"🚨 HATA: '{INPUT_BASE_PATH}' altında hiçbir kategori klasörü bulunamadı. Lütfen yolu kontrol edin.\")\n",
    "else:\n",
    "    all_categories = [os.path.basename(os.path.normpath(p)) for p in category_paths if os.path.isdir(p)]\n",
    "    print(f\"✅ Toplam {len(all_categories)} kategori bulundu: {all_categories}\")\n",
    "\n",
    "# --- 2. Ana Kategori Döngüsü ---\n",
    "for category_name in all_categories:\n",
    "    print(f\"\\n{'='*60}\\n--- İşleniyor: {category_name} ---\\n{'='*60}\")\n",
    "    \n",
    "    category_output_dir = os.path.join(HYBRID_DIR, category_name)\n",
    "\n",
    "    # Dosyaların zaten var olup olmadığını kontrol et, varsa bu kategoriyi atla.\n",
    "    final_files_check = [os.path.join(category_output_dir, f) for f in [\"X_train.npz\", \"X_test.npz\", \"y_train.npy\", \"y_test.npy\"]]\n",
    "    if all(os.path.exists(p) for p in final_files_check):\n",
    "        print(f\"✅ '{category_name}' için dosyalar zaten mevcut, bu kategori atlanıyor.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # <<< DEĞİŞİKLİK BURADA >>>\n",
    "        # Klasörü silmek yerine, sadece var olduğundan emin oluyoruz.\n",
    "        # exist_ok=True parametresi, klasör zaten varsa hata vermesini engeller.\n",
    "        os.makedirs(category_output_dir, exist_ok=True)\n",
    "        # <<< DEĞİŞİKLİĞİN SONU >>>\n",
    "\n",
    "        # Doğrudan bulunan kategori adını kullanarak JSON dosyasını ara\n",
    "        json_files = glob.glob(os.path.join(INPUT_BASE_PATH, category_name, \"*.json\"))\n",
    "        if not json_files:\n",
    "            print(f\"🚨 UYARI: '{category_name}' içinde JSON dosyası bulunamadı. Atlanıyor.\")\n",
    "            continue\n",
    "        \n",
    "        file_path = json_files[0]\n",
    "        print(f\"💾 '{file_path}' okunuyor...\")\n",
    "        \n",
    "        labeled_chunks = []\n",
    "        chunk_iterator = pd.read_json(file_path, lines=True, chunksize=500000)\n",
    "        for chunk in chunk_iterator:\n",
    "            if \"class\" in chunk.columns:\n",
    "                labeled_chunks.append(chunk.dropna(subset=['class']))\n",
    "        \n",
    "        if not labeled_chunks:\n",
    "            print(f\"🚨 UYARI: '{category_name}' içinde etiketli veri bulunamadı. Atlanıyor.\")\n",
    "            continue\n",
    "\n",
    "        category_df = pd.concat(labeled_chunks, ignore_index=True)\n",
    "        del labeled_chunks\n",
    "        print(f\"✅ '{category_name}' için {len(category_df)} etiketli yorum yüklendi.\")\n",
    "\n",
    "        print(\"🪓 Veri, eğitim ve test setlerine ayrılıyor...\")\n",
    "        train_df, test_df = train_test_split(\n",
    "            category_df, test_size=0.25, random_state=42, stratify=category_df['class']\n",
    "        )\n",
    "        del category_df\n",
    "\n",
    "        print(\"🏷️  Cevaplar (y) ayrılıyor...\")\n",
    "        y_train = train_df['class'].to_numpy()\n",
    "        y_test = test_df['class'].to_numpy()\n",
    "        train_df = train_df.drop(columns=['class'])\n",
    "        test_df = test_df.drop(columns=['class'])\n",
    "\n",
    "        print(\"🧹 Metin verileri temizleniyor...\")\n",
    "        train_df['cleaned_text'] = (train_df['summary'].astype(str) + ' ' + train_df['reviewText'].astype(str)).apply(clean_text)\n",
    "        test_df['cleaned_text'] = (test_df['summary'].astype(str) + ' ' + train_df['reviewText'].astype(str)).apply(clean_text)\n",
    "\n",
    "        print(\"📝 TF-IDF vektörleştirici SADECE eğitim verisiyle eğitiliyor...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text'])\n",
    "\n",
    "        print(\"📊 Davranışsal özellikler oluşturuluyor...\")\n",
    "        local_reviewer_counts_train = train_df['reviewerID'].value_counts()\n",
    "        X_train_behavioral = create_behavioral_features(train_df, local_reviewer_counts_train)\n",
    "        X_test_behavioral = create_behavioral_features(test_df, local_reviewer_counts_train)\n",
    "\n",
    "        print(\"➕ Özellikler birleştiriliyor ve kaydediliyor...\")\n",
    "        X_train = hstack([X_train_tfidf, X_train_behavioral]).tocsr()\n",
    "        X_test = hstack([X_test_tfidf, X_test_behavioral]).tocsr()\n",
    "\n",
    "        save_npz(os.path.join(category_output_dir, \"X_train.npz\"), X_train)\n",
    "        save_npz(os.path.join(category_output_dir, \"X_test.npz\"), X_test)\n",
    "        np.save(os.path.join(category_output_dir, \"y_train.npy\"), y_train)\n",
    "        np.save(os.path.join(category_output_dir, \"y_test.npy\"), y_test)\n",
    "        \n",
    "        print(f\"✅ '{category_name}' için sızıntısız veri seti başarıyla oluşturuldu.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n🚨 HATA: '{category_name}' işlenirken bir sorun oluştu: {e}\")\n",
    "        print(\"-> Sonraki kategoriye devam ediliyor...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\\n🎉 Tüm kategorilerin işlenmesi tamamlandı!\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46759a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import save_npz, vstack, hstack\n",
    "\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords indiriliyor...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "INPUT_PARQUET_PATH = \"..\\\\data\\\\raw\\\\labeled_reviews.parquet\"\n",
    "OUTPUT_DIR = \"..\\\\data\\\\processed\\\\\"\n",
    "\n",
    "TEXT_ONLY_DIR = os.path.join(OUTPUT_DIR, \"text_data\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "os.makedirs(TEXT_ONLY_DIR, exist_ok=True)\n",
    "os.makedirs(HYBRID_DIR, exist_ok=True)\n",
    "INPUT_BASE_PATH = 'C:\\\\Users\\\\Aybars\\\\.cache\\\\kagglehub\\\\datasets\\\\naveedhn\\\\amazon-product-review-spam-and-non-spam\\\\versions\\\\1'\n",
    "BASE_DIR = \"C:\\\\work environment\\\\Projects\\\\amazon-spam-review\\\\\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"data\\\\processed\\\\\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "\n",
    "VECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def create_behavioral_features(df, reviewer_counts_train):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    df_out['review_count'] = df['reviewerID'].map(reviewer_counts_train).fillna(0)\n",
    "    return df_out.values\n",
    "\n",
    "print(\"Kurulum tamamlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text-only veri seti oluşturuluyor...\")\n",
    "\n",
    "# Parquet dosyasından sadece gerekli kolonları oku\n",
    "df = pd.read_parquet(INPUT_PARQUET_PATH, columns=['summary', 'reviewText', 'class'])\n",
    "\n",
    "# Metin verilerini birleştir ve temizle\n",
    "df['full_text'] = (df['summary'].astype(str) + ' ' + df['reviewText'].astype(str))\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_text)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_text'],\n",
    "    df['class'],\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=df['class']\n",
    ")\n",
    "\n",
    "# TF-IDF vektörizasyonu\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "# Dosyaları kaydet\n",
    "os.makedirs(TEXT_ONLY_DIR, exist_ok=True)\n",
    "save_npz(os.path.join(TEXT_ONLY_DIR, \"X_train.npz\"), X_train)\n",
    "save_npz(os.path.join(TEXT_ONLY_DIR, \"X_test.npz\"), X_test)\n",
    "np.save(os.path.join(TEXT_ONLY_DIR, \"y_train.npy\"), y_train.values)\n",
    "np.save(os.path.join(TEXT_ONLY_DIR, \"y_test.npy\"), y_test.values)\n",
    "\n",
    "print(f\"✅ Text-only veri seti oluşturuldu. Boyutlar: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# Hybrid veri seti işlemleri buradan devam eder..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
