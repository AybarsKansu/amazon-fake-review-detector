{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurulum tamamlandÄ±.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import save_npz, vstack, hstack\n",
    "\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords indiriliyor...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "INPUT_PARQUET_PATH = \"..\\\\data\\\\raw\\\\labeled_reviews.parquet\"\n",
    "OUTPUT_DIR = \"..\\\\data\\\\processed\\\\\"\n",
    "\n",
    "TEXT_ONLY_DIR = os.path.join(OUTPUT_DIR, \"text_data\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "os.makedirs(TEXT_ONLY_DIR, exist_ok=True)\n",
    "os.makedirs(HYBRID_DIR, exist_ok=True)\n",
    "INPUT_BASE_PATH = 'C:\\\\Users\\\\Aybars\\\\.cache\\\\kagglehub\\\\datasets\\\\naveedhn\\\\amazon-product-review-spam-and-non-spam\\\\versions\\\\1'\n",
    "BASE_DIR = \"C:\\\\work environment\\\\Projects\\\\amazon-spam-review\\\\\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"data\\\\processed\\\\\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "\n",
    "VECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def create_behavioral_features(df, reviewer_counts_train):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    df_out['review_count'] = df['reviewerID'].map(reviewer_counts_train).fillna(0)\n",
    "    return df_out.values\n",
    "\n",
    "print(\"Kurulum tamamlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e222669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ TÃ¼m kategoriler iÃ§in hibrit veri setini sÄ±fÄ±rdan oluÅŸturma iÅŸlemi baÅŸlÄ±yor.\n",
      "Kaynak: Ham JSON verileri okunacak (C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1)\n",
      "âœ… Toplam 6 kategori bulundu: ['Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry', 'Electronics', 'Home_and_Kitchen', 'Sports_and_Outdoors', 'Toys_and_Games']\n",
      "\n",
      "============================================================\n",
      "--- Ä°ÅŸleniyor: Cell_Phones_and_Accessories ---\n",
      "============================================================\n",
      "âœ… 'Cell_Phones_and_Accessories' iÃ§in dosyalar zaten mevcut, bu kategori atlanÄ±yor.\n",
      "\n",
      "============================================================\n",
      "--- Ä°ÅŸleniyor: Clothing_Shoes_and_Jewelry ---\n",
      "============================================================\n",
      "âœ… 'Clothing_Shoes_and_Jewelry' iÃ§in dosyalar zaten mevcut, bu kategori atlanÄ±yor.\n",
      "\n",
      "============================================================\n",
      "--- Ä°ÅŸleniyor: Electronics ---\n",
      "============================================================\n",
      "ğŸ’¾ 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Electronics\\Electronics.json' okunuyor...\n",
      "âœ… 'Electronics' iÃ§in 7574169 etiketli yorum yÃ¼klendi.\n",
      "ğŸª“ Veri, eÄŸitim ve test setlerine ayrÄ±lÄ±yor...\n",
      "ğŸ·ï¸  Cevaplar (y) ayrÄ±lÄ±yor...\n",
      "ğŸ§¹ Metin verileri temizleniyor...\n",
      "ğŸ“ TF-IDF vektÃ¶rleÅŸtirici SADECE eÄŸitim verisiyle eÄŸitiliyor...\n",
      "ğŸ“Š DavranÄ±ÅŸsal Ã¶zellikler oluÅŸturuluyor...\n",
      "â• Ã–zellikler birleÅŸtiriliyor ve kaydediliyor...\n",
      "âœ… 'Electronics' iÃ§in sÄ±zÄ±ntÄ±sÄ±z veri seti baÅŸarÄ±yla oluÅŸturuldu.\n",
      "\n",
      "============================================================\n",
      "--- Ä°ÅŸleniyor: Home_and_Kitchen ---\n",
      "============================================================\n",
      "ğŸ’¾ 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Home_and_Kitchen\\Home_and_Kitchen.json' okunuyor...\n",
      "âœ… 'Home_and_Kitchen' iÃ§in 3988482 etiketli yorum yÃ¼klendi.\n",
      "ğŸª“ Veri, eÄŸitim ve test setlerine ayrÄ±lÄ±yor...\n",
      "ğŸ·ï¸  Cevaplar (y) ayrÄ±lÄ±yor...\n",
      "ğŸ§¹ Metin verileri temizleniyor...\n",
      "ğŸ“ TF-IDF vektÃ¶rleÅŸtirici SADECE eÄŸitim verisiyle eÄŸitiliyor...\n",
      "ğŸ“Š DavranÄ±ÅŸsal Ã¶zellikler oluÅŸturuluyor...\n",
      "â• Ã–zellikler birleÅŸtiriliyor ve kaydediliyor...\n",
      "âœ… 'Home_and_Kitchen' iÃ§in sÄ±zÄ±ntÄ±sÄ±z veri seti baÅŸarÄ±yla oluÅŸturuldu.\n",
      "\n",
      "============================================================\n",
      "--- Ä°ÅŸleniyor: Sports_and_Outdoors ---\n",
      "============================================================\n",
      "ğŸ’¾ 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Sports_and_Outdoors\\Sports_and_Outdoors.json' okunuyor...\n",
      "âœ… 'Sports_and_Outdoors' iÃ§in 3013256 etiketli yorum yÃ¼klendi.\n",
      "ğŸª“ Veri, eÄŸitim ve test setlerine ayrÄ±lÄ±yor...\n",
      "ğŸ·ï¸  Cevaplar (y) ayrÄ±lÄ±yor...\n",
      "ğŸ§¹ Metin verileri temizleniyor...\n",
      "ğŸ“ TF-IDF vektÃ¶rleÅŸtirici SADECE eÄŸitim verisiyle eÄŸitiliyor...\n",
      "ğŸ“Š DavranÄ±ÅŸsal Ã¶zellikler oluÅŸturuluyor...\n",
      "â• Ã–zellikler birleÅŸtiriliyor ve kaydediliyor...\n",
      "âœ… 'Sports_and_Outdoors' iÃ§in sÄ±zÄ±ntÄ±sÄ±z veri seti baÅŸarÄ±yla oluÅŸturuldu.\n",
      "\n",
      "============================================================\n",
      "--- Ä°ÅŸleniyor: Toys_and_Games ---\n",
      "============================================================\n",
      "ğŸ’¾ 'C:\\Users\\Aybars\\.cache\\kagglehub\\datasets\\naveedhn\\amazon-product-review-spam-and-non-spam\\versions\\1\\Toys_and_Games\\Toys_and_Games.json' okunuyor...\n",
      "âœ… 'Toys_and_Games' iÃ§in 1997140 etiketli yorum yÃ¼klendi.\n",
      "ğŸª“ Veri, eÄŸitim ve test setlerine ayrÄ±lÄ±yor...\n",
      "ğŸ·ï¸  Cevaplar (y) ayrÄ±lÄ±yor...\n",
      "ğŸ§¹ Metin verileri temizleniyor...\n",
      "ğŸ“ TF-IDF vektÃ¶rleÅŸtirici SADECE eÄŸitim verisiyle eÄŸitiliyor...\n",
      "ğŸ“Š DavranÄ±ÅŸsal Ã¶zellikler oluÅŸturuluyor...\n",
      "â• Ã–zellikler birleÅŸtiriliyor ve kaydediliyor...\n",
      "âœ… 'Toys_and_Games' iÃ§in sÄ±zÄ±ntÄ±sÄ±z veri seti baÅŸarÄ±yla oluÅŸturuldu.\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ TÃ¼m kategorilerin iÅŸlenmesi tamamlandÄ±!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from scipy.sparse import hstack, save_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(\"ğŸš€ TÃ¼m kategoriler iÃ§in hibrit veri setini sÄ±fÄ±rdan oluÅŸturma iÅŸlemi baÅŸlÄ±yor.\")\n",
    "print(f\"Kaynak: Ham JSON verileri okunacak ({INPUT_BASE_PATH})\")\n",
    "\n",
    "# IÅŸlenecek tÃ¼m kategori klasÃ¶rlerinin tam yollarÄ±nÄ± bul\n",
    "category_paths = glob.glob(os.path.join(INPUT_BASE_PATH, '*/'))\n",
    "\n",
    "if not category_paths:\n",
    "    print(f\"ğŸš¨ HATA: '{INPUT_BASE_PATH}' altÄ±nda hiÃ§bir kategori klasÃ¶rÃ¼ bulunamadÄ±. LÃ¼tfen yolu kontrol edin.\")\n",
    "else:\n",
    "    all_categories = [os.path.basename(os.path.normpath(p)) for p in category_paths if os.path.isdir(p)]\n",
    "    print(f\"âœ… Toplam {len(all_categories)} kategori bulundu: {all_categories}\")\n",
    "\n",
    "# --- 2. Ana Kategori DÃ¶ngÃ¼sÃ¼ ---\n",
    "for category_name in all_categories:\n",
    "    print(f\"\\n{'='*60}\\n--- Ä°ÅŸleniyor: {category_name} ---\\n{'='*60}\")\n",
    "    \n",
    "    category_output_dir = os.path.join(HYBRID_DIR, category_name)\n",
    "\n",
    "    # DosyalarÄ±n zaten var olup olmadÄ±ÄŸÄ±nÄ± kontrol et, varsa bu kategoriyi atla.\n",
    "    final_files_check = [os.path.join(category_output_dir, f) for f in [\"X_train.npz\", \"X_test.npz\", \"y_train.npy\", \"y_test.npy\"]]\n",
    "    if all(os.path.exists(p) for p in final_files_check):\n",
    "        print(f\"âœ… '{category_name}' iÃ§in dosyalar zaten mevcut, bu kategori atlanÄ±yor.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # <<< DEÄÄ°ÅÄ°KLÄ°K BURADA >>>\n",
    "        # KlasÃ¶rÃ¼ silmek yerine, sadece var olduÄŸundan emin oluyoruz.\n",
    "        # exist_ok=True parametresi, klasÃ¶r zaten varsa hata vermesini engeller.\n",
    "        os.makedirs(category_output_dir, exist_ok=True)\n",
    "        # <<< DEÄÄ°ÅÄ°KLÄ°ÄÄ°N SONU >>>\n",
    "\n",
    "        # DoÄŸrudan bulunan kategori adÄ±nÄ± kullanarak JSON dosyasÄ±nÄ± ara\n",
    "        json_files = glob.glob(os.path.join(INPUT_BASE_PATH, category_name, \"*.json\"))\n",
    "        if not json_files:\n",
    "            print(f\"ğŸš¨ UYARI: '{category_name}' iÃ§inde JSON dosyasÄ± bulunamadÄ±. AtlanÄ±yor.\")\n",
    "            continue\n",
    "        \n",
    "        file_path = json_files[0]\n",
    "        print(f\"ğŸ’¾ '{file_path}' okunuyor...\")\n",
    "        \n",
    "        labeled_chunks = []\n",
    "        chunk_iterator = pd.read_json(file_path, lines=True, chunksize=500000)\n",
    "        for chunk in chunk_iterator:\n",
    "            if \"class\" in chunk.columns:\n",
    "                labeled_chunks.append(chunk.dropna(subset=['class']))\n",
    "        \n",
    "        if not labeled_chunks:\n",
    "            print(f\"ğŸš¨ UYARI: '{category_name}' iÃ§inde etiketli veri bulunamadÄ±. AtlanÄ±yor.\")\n",
    "            continue\n",
    "\n",
    "        category_df = pd.concat(labeled_chunks, ignore_index=True)\n",
    "        del labeled_chunks\n",
    "        print(f\"âœ… '{category_name}' iÃ§in {len(category_df)} etiketli yorum yÃ¼klendi.\")\n",
    "\n",
    "        print(\"ğŸª“ Veri, eÄŸitim ve test setlerine ayrÄ±lÄ±yor...\")\n",
    "        train_df, test_df = train_test_split(\n",
    "            category_df, test_size=0.25, random_state=42, stratify=category_df['class']\n",
    "        )\n",
    "        del category_df\n",
    "\n",
    "        print(\"ğŸ·ï¸  Cevaplar (y) ayrÄ±lÄ±yor...\")\n",
    "        y_train = train_df['class'].to_numpy()\n",
    "        y_test = test_df['class'].to_numpy()\n",
    "        train_df = train_df.drop(columns=['class'])\n",
    "        test_df = test_df.drop(columns=['class'])\n",
    "\n",
    "        print(\"ğŸ§¹ Metin verileri temizleniyor...\")\n",
    "        train_df['cleaned_text'] = (train_df['summary'].astype(str) + ' ' + train_df['reviewText'].astype(str)).apply(clean_text)\n",
    "        test_df['cleaned_text'] = (test_df['summary'].astype(str) + ' ' + train_df['reviewText'].astype(str)).apply(clean_text)\n",
    "\n",
    "        print(\"ğŸ“ TF-IDF vektÃ¶rleÅŸtirici SADECE eÄŸitim verisiyle eÄŸitiliyor...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text'])\n",
    "\n",
    "        print(\"ğŸ“Š DavranÄ±ÅŸsal Ã¶zellikler oluÅŸturuluyor...\")\n",
    "        local_reviewer_counts_train = train_df['reviewerID'].value_counts()\n",
    "        X_train_behavioral = create_behavioral_features(train_df, local_reviewer_counts_train)\n",
    "        X_test_behavioral = create_behavioral_features(test_df, local_reviewer_counts_train)\n",
    "\n",
    "        print(\"â• Ã–zellikler birleÅŸtiriliyor ve kaydediliyor...\")\n",
    "        X_train = hstack([X_train_tfidf, X_train_behavioral]).tocsr()\n",
    "        X_test = hstack([X_test_tfidf, X_test_behavioral]).tocsr()\n",
    "\n",
    "        save_npz(os.path.join(category_output_dir, \"X_train.npz\"), X_train)\n",
    "        save_npz(os.path.join(category_output_dir, \"X_test.npz\"), X_test)\n",
    "        np.save(os.path.join(category_output_dir, \"y_train.npy\"), y_train)\n",
    "        np.save(os.path.join(category_output_dir, \"y_test.npy\"), y_test)\n",
    "        \n",
    "        print(f\"âœ… '{category_name}' iÃ§in sÄ±zÄ±ntÄ±sÄ±z veri seti baÅŸarÄ±yla oluÅŸturuldu.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nğŸš¨ HATA: '{category_name}' iÅŸlenirken bir sorun oluÅŸtu: {e}\")\n",
    "        print(\"-> Sonraki kategoriye devam ediliyor...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\\nğŸ‰ TÃ¼m kategorilerin iÅŸlenmesi tamamlandÄ±!\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46759a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import save_npz, vstack, hstack\n",
    "\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords indiriliyor...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "INPUT_PARQUET_PATH = \"..\\\\data\\\\raw\\\\labeled_reviews.parquet\"\n",
    "OUTPUT_DIR = \"..\\\\data\\\\processed\\\\\"\n",
    "\n",
    "TEXT_ONLY_DIR = os.path.join(OUTPUT_DIR, \"text_data\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "os.makedirs(TEXT_ONLY_DIR, exist_ok=True)\n",
    "os.makedirs(HYBRID_DIR, exist_ok=True)\n",
    "INPUT_BASE_PATH = 'C:\\\\Users\\\\Aybars\\\\.cache\\\\kagglehub\\\\datasets\\\\naveedhn\\\\amazon-product-review-spam-and-non-spam\\\\versions\\\\1'\n",
    "BASE_DIR = \"C:\\\\work environment\\\\Projects\\\\amazon-spam-review\\\\\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"data\\\\processed\\\\\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "\n",
    "VECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def create_behavioral_features(df, reviewer_counts_train):\n",
    "    df_out = pd.DataFrame(index=df.index)\n",
    "    df_out['review_count'] = df['reviewerID'].map(reviewer_counts_train).fillna(0)\n",
    "    return df_out.values\n",
    "\n",
    "print(\"Kurulum tamamlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text-only veri seti oluÅŸturuluyor...\")\n",
    "\n",
    "# Parquet dosyasÄ±ndan sadece gerekli kolonlarÄ± oku\n",
    "df = pd.read_parquet(INPUT_PARQUET_PATH, columns=['summary', 'reviewText', 'class'])\n",
    "\n",
    "# Metin verilerini birleÅŸtir ve temizle\n",
    "df['full_text'] = (df['summary'].astype(str) + ' ' + df['reviewText'].astype(str))\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_text)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_text'],\n",
    "    df['class'],\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=df['class']\n",
    ")\n",
    "\n",
    "# TF-IDF vektÃ¶rizasyonu\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "# DosyalarÄ± kaydet\n",
    "os.makedirs(TEXT_ONLY_DIR, exist_ok=True)\n",
    "save_npz(os.path.join(TEXT_ONLY_DIR, \"X_train.npz\"), X_train)\n",
    "save_npz(os.path.join(TEXT_ONLY_DIR, \"X_test.npz\"), X_test)\n",
    "np.save(os.path.join(TEXT_ONLY_DIR, \"y_train.npy\"), y_train.values)\n",
    "np.save(os.path.join(TEXT_ONLY_DIR, \"y_test.npy\"), y_test.values)\n",
    "\n",
    "print(f\"âœ… Text-only veri seti oluÅŸturuldu. Boyutlar: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# Hybrid veri seti iÅŸlemleri buradan devam eder..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
