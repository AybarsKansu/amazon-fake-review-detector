{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41425b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reviewerID', 'reviewText', 'summary', 'overall', 'helpful', 'unixReviewTime', 'class', 'category']\n",
      "              reviewerID\n",
      "0          A3HVRXV0LVJN7\n",
      "1         A1BJGDS0L1IO6I\n",
      "2          A1YX2RBMS1L9L\n",
      "3         A180NNPPKWCCU0\n",
      "4         A30P2CYOUYAJM8\n",
      "...                  ...\n",
      "25263218  A3SDSFPFY8WS8C\n",
      "25263219   AVJ4N5LBKAOG5\n",
      "25263220  A3JB0C3QWIN61Q\n",
      "25263221   AJ062DSRHW9RX\n",
      "25263222   AVJ84HKPIGBMH\n",
      "\n",
      "[25263223 rows x 1 columns]\n",
      "                                                    summary\n",
      "0                                                     A++++\n",
      "1                                           ITEM NOT SENT!!\n",
      "2                                             Great product\n",
      "3                                                   Perfect\n",
      "4                                            Cool purchase.\n",
      "...                                                     ...\n",
      "25263218                      Awesome toy Price not Awesome\n",
      "25263219  Excellent quality and fun to fly. Certainly wo...\n",
      "25263220                                         Great Set!\n",
      "25263221  I bought a few packs for a birthday party and ...\n",
      "25263222                                          Awesome!!\n",
      "\n",
      "[25263223 rows x 1 columns]\n",
      "          overall\n",
      "0               5\n",
      "1               1\n",
      "2               5\n",
      "3               5\n",
      "4               4\n",
      "...           ...\n",
      "25263218        4\n",
      "25263219        5\n",
      "25263220        5\n",
      "25263221        5\n",
      "25263222        5\n",
      "\n",
      "[25263223 rows x 1 columns]\n",
      "          class\n",
      "0             1\n",
      "1             0\n",
      "2             1\n",
      "3             1\n",
      "4             1\n",
      "...         ...\n",
      "25263218      1\n",
      "25263219      1\n",
      "25263220      1\n",
      "25263221      1\n",
      "25263222      1\n",
      "\n",
      "[25263223 rows x 1 columns]\n",
      "                             category\n",
      "0         Cell_Phones_and_Accessories\n",
      "1         Cell_Phones_and_Accessories\n",
      "2         Cell_Phones_and_Accessories\n",
      "3         Cell_Phones_and_Accessories\n",
      "4         Cell_Phones_and_Accessories\n",
      "...                               ...\n",
      "25263218               Toys_and_Games\n",
      "25263219               Toys_and_Games\n",
      "25263220               Toys_and_Games\n",
      "25263221               Toys_and_Games\n",
      "25263222               Toys_and_Games\n",
      "\n",
      "[25263223 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import save_npz, vstack\n",
    "\n",
    "try:\n",
    "  stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "  print(\"words downloading...\")\n",
    "  nltk.download(\"stopwords\")\n",
    "\n",
    "file_path = \"../data/raw/labeled_reviews.parquet\"\n",
    "out_dir = \"../data/processed/\"\n",
    "\n",
    "vectorizer_path = os.path.join(out_dir, \"tfidf_vectorizer.joblib\")\n",
    "x_tfidf_path = os.path.join(out_dir, \"X_tfidf_features.npz\")\n",
    "y_labels_path = os.path.join(out_dir, \"y_labels.npy\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "\n",
    "  text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "  text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "  words = text.split()\n",
    "  words = [word for word in words if word not in stop_words]\n",
    "\n",
    "  return \" \".join(words)\n",
    "\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "sample = next(parquet_file.iter_batches(batch_size=200000))\n",
    "df_sample = sample.to_pandas()\n",
    "\n",
    "df_sample[\"full text\"] = df_sample[\"summary\"].astype(str) + \" \" + df_sample[\"review\"].astype(str)\n",
    "df_sample[\"clean_text\"] = df_sample[\"full text\"].apply(clean_text)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "tfidf_vectorizer.fit(df_sample[\"cleaned_text\"])\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
    "\n",
    "all_tfidf_chunks = []\n",
    "all_labels = []\n",
    "\n",
    "batch_iterator = parquet_file.iter_batches(batch_size=100000)\n",
    "\n",
    "for i, batch in enumerate(batch_iterator):\n",
    "  df_chunk = batch.to_pandas()\n",
    "\n",
    "  df_chunk['full_text'] = df_chunk['summary'].astype(str) + ' ' + df_chunk['reviewText'].astype(str)\n",
    "  df_chunk['cleaned_text'] = df_chunk['full_text'].apply(clean_text)\n",
    "\n",
    "  X_chunk_tfidf = tfidf_vectorizer.transform(df_chunk['cleaned_text'])\n",
    "\n",
    "  all_tfidf_chunks.append(X_chunk_tfidf)\n",
    "  all_labels.extend(df_chunk['class'].values)\n",
    "\n",
    "X_final_tfidf = vstack(all_tfidf_chunks)\n",
    "\n",
    "y_final = np.array(all_labels)\n",
    "\n",
    "save_npz(x_tfidf_path, X_final_tfidf)\n",
    "\n",
    "np.save(y_labels_path, y_final)\n",
    "\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "reviewerIDs = pd.read_parquet(file_path, columns=[\"reviewerID\"])\n",
    "summaries = pd.read_parquet(file_path, columns=[\"summary\"])\n",
    "overalls = pd.read_parquet(file_path, columns=[\"overall\"])\n",
    "classes = pd.read_parquet(file_path, columns=[\"class\"])\n",
    "categories = pd.read_parquet(file_path, columns=[\"category\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_load = ['reviewText', 'summary', 'overall', 'class', 'category']\n",
    "\n",
    "df = pd.read_parquet(file_path, columns=columns_to_load)\n",
    "\n",
    "print(\"Data loaded successfully with selected columns!\")\n",
    "df.info()\n",
    "\n",
    "print(df[\"category\"].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
