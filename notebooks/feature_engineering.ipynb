{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33df9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurulum tamamlandı.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import save_npz, vstack, hstack\n",
    "\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords indiriliyor...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "INPUT_PARQUET_PATH = \"../data/raw/labeled_reviews.parquet\"\n",
    "OUTPUT_DIR = \"../data/processed/\"\n",
    "\n",
    "TEXT_ONLY_DIR = os.path.join(OUTPUT_DIR, \"text_data\")\n",
    "HYBRID_DIR = os.path.join(OUTPUT_DIR, \"hybrid\")\n",
    "os.makedirs(TEXT_ONLY_DIR, exist_ok=True)\n",
    "os.makedirs(HYBRID_DIR, exist_ok=True)\n",
    "\n",
    "VECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def create_behavioral_features(df):\n",
    "    helpful_list = df[\"helpful\"].apply(eval)\n",
    "    df['helpful_votes'] = helpful_list.apply(lambda x: x[0])\n",
    "    df['total_votes'] = helpful_list.apply(lambda x: x[1])\n",
    "    df['helpfulness_ratio'] = df['helpful_votes'] / (df['total_votes'] + 0.001)\n",
    "\n",
    "    df['text_length'] = df['full_text'].str.len()\n",
    "    \n",
    "    feature_df = df[['overall', 'helpfulness_ratio', 'text_length']].fillna(0)\n",
    "    return feature_df.to_numpy()\n",
    "\n",
    "print(\"Kurulum tamamlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc200af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektörleştirici zaten mevcut, bu adım atlanıyor.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(VECTORIZER_PATH):\n",
    "    print(\"Vektörleştirici oluşturuluyor...\")\n",
    "    parquet_file_vec = pq.ParquetFile(INPUT_PARQUET_PATH)\n",
    "    sample_batch_vec = next(parquet_file_vec.iter_batches(batch_size=200000))\n",
    "    df_sample = sample_batch_vec.to_pandas()\n",
    "    df_sample[\"full_text\"] = df_sample[\"summary\"].astype(str) + \" \" + df_sample[\"reviewText\"].astype(str)\n",
    "    df_sample[\"cleaned_text\"] = df_sample[\"full_text\"].apply(clean_text)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "    tfidf_vectorizer.fit(df_sample[\"cleaned_text\"])\n",
    "    joblib.dump(tfidf_vectorizer, VECTORIZER_PATH)\n",
    "    print(f\"Vektörleştirici '{VECTORIZER_PATH}' adresine kaydedildi.\")\n",
    "else:\n",
    "    print(\"Vektörleştirici zaten mevcut, bu adım atlanıyor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481fb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviewer_ids = pd.read_parquet(INPUT_PARQUET_PATH, columns=['reviewerID'])\n",
    "reviewer_counts = all_reviewer_ids['reviewerID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bir veya daha fazla veri seti eksik. Veri işleme adımları başlatılıyor...\n",
      "-> Part 1 -> Part 2 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mextend(df_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Hem metin hem de hibrit özellikleri AYNI ANDA oluştur\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m X_chunk_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(df_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     36\u001b[0m X_chunk_behavioral_part1 \u001b[38;5;241m=\u001b[39m create_behavioral_features(df_chunk)\n\u001b[0;32m     37\u001b[0m df_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewer_review_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewerID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(reviewer_counts)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2128\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2112\u001b[0m \n\u001b[0;32m   2113\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2128\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1421\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1421\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1423\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m             doc \u001b[38;5;241m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m             doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m         )\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_word_ngrams\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_text_files = [\n",
    "    os.path.join(TEXT_ONLY_DIR, \"X_train.npz\"), os.path.join(TEXT_ONLY_DIR, \"X_test.npz\"),\n",
    "    os.path.join(TEXT_ONLY_DIR, \"y_train.npy\"), os.path.join(TEXT_ONLY_DIR, \"y_test.npy\")\n",
    "]\n",
    "final_hybrid_files = [\n",
    "    os.path.join(HYBRID_DIR, \"X_train.npz\"), os.path.join(HYBRID_DIR, \"X_test.npz\"),\n",
    "    os.path.join(HYBRID_DIR, \"y_train.npy\"), os.path.join(HYBRID_DIR, \"y_test.npy\")\n",
    "]\n",
    "\n",
    "text_files_exist = all(os.path.exists(p) for p in final_text_files)\n",
    "hybrid_files_exist = all(os.path.exists(p) for p in final_hybrid_files)\n",
    "\n",
    "# 2. Eğer TÜM dosyalar zaten varsa, hiçbir şey yapma.\n",
    "if text_files_exist and hybrid_files_exist:\n",
    "    print(\"✅ Tüm text-only ve hybrid veri setleri zaten mevcut. Hiçbir işlem yapılmadı.\")\n",
    "else:\n",
    "    # --- ORTAK VE HIZLI ADIMLAR (SADECE BİR KEZ ÇALIŞIR) ---\n",
    "    print(\"Bir veya daha fazla veri seti eksik. Veri işleme adımları başlatılıyor...\")\n",
    "    \n",
    "    # Tüm etiketleri ve indeksleri önceden al\n",
    "    print(\"Etiketler okunuyor ve eğitim/test indeksleri oluşturuluyor...\")\n",
    "    y_final = pd.read_parquet(INPUT_PARQUET_PATH, columns=['class'])['class'].to_numpy()\n",
    "    indices = np.arange(len(y_final))\n",
    "\n",
    "    # Sadece indeksleri bölerek y_train ve y_test'i oluştur\n",
    "    train_indices, test_indices, y_train, y_test = train_test_split(\n",
    "        indices, y_final, test_size=0.20, random_state=42, stratify=y_final\n",
    "    )\n",
    "    train_indices_set = set(train_indices) # Hızlı arama için\n",
    "\n",
    "    # --- YAVAŞ VERİ OKUMA DÖNGÜSÜ (SADECE BİR KEZ ÇALIŞIR) ---\n",
    "    print(\"Veri parça parça işleniyor ve setlere ayrılıyor...\")\n",
    "    tfidf_vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "    parquet_file = pq.ParquetFile(INPUT_PARQUET_PATH)\n",
    "    all_reviewer_ids = pd.read_parquet(INPUT_PARQUET_PATH, columns=['reviewerID'])\n",
    "    reviewer_counts = all_reviewer_ids['reviewerID'].value_counts()\n",
    "    \n",
    "    # Eksik olan setler için listeleri başlat\n",
    "    if not text_files_exist:\n",
    "        X_train_text_chunks, X_test_text_chunks = [], []\n",
    "    if not hybrid_files_exist:\n",
    "        X_train_hybrid_chunks, X_test_hybrid_chunks = [], []\n",
    "    \n",
    "    current_row_index = 0\n",
    "    batch_iterator = parquet_file.iter_batches(batch_size=100000)\n",
    "    for i, batch in enumerate(batch_iterator):\n",
    "        print(f\"{i+1}\", end=\"-\")\n",
    "        df_chunk = batch.to_pandas()\n",
    "        \n",
    "        # Özellik oluşturma\n",
    "        df_chunk['full_text'] = df_chunk['summary'].astype(str) + ' ' + df_chunk['reviewText'].astype(str)\n",
    "        df_chunk['cleaned_text'] = df_chunk['full_text'].apply(clean_text)\n",
    "        \n",
    "        chunk_indices = np.arange(current_row_index, current_row_index + len(df_chunk))\n",
    "        train_mask = [idx in train_indices_set for idx in chunk_indices]\n",
    "        test_mask = np.invert(train_mask)\n",
    "        \n",
    "        # Eksik olan setler için özellikleri oluştur ve biriktir\n",
    "        if not text_files_exist:\n",
    "            X_chunk_tfidf = tfidf_vectorizer.transform(df_chunk['cleaned_text'])\n",
    "            X_train_text_chunks.append(X_chunk_tfidf[train_mask])\n",
    "            X_test_text_chunks.append(X_chunk_tfidf[test_mask])\n",
    "        \n",
    "        if not hybrid_files_exist:\n",
    "            # Eğer text özellikleri zaten hesaplanmadıysa, burada hesapla\n",
    "            if 'X_chunk_tfidf' not in locals() or X_chunk_tfidf.shape[0] != len(df_chunk):\n",
    "                 X_chunk_tfidf = tfidf_vectorizer.transform(df_chunk['cleaned_text'])\n",
    "            \n",
    "            X_chunk_behavioral_part1 = create_behavioral_features(df_chunk)\n",
    "            df_chunk['reviewer_review_count'] = df_chunk['reviewerID'].map(reviewer_counts).fillna(1)\n",
    "            X_chunk_behavioral_part2 = df_chunk[['reviewer_review_count']].to_numpy()\n",
    "            X_chunk_hybrid = hstack([X_chunk_tfidf, X_chunk_behavioral_part1, X_chunk_behavioral_part2])\n",
    "            \n",
    "            X_train_hybrid_chunks.append(X_chunk_hybrid[train_mask])\n",
    "            X_test_hybrid_chunks.append(X_chunk_hybrid[test_mask])\n",
    "            \n",
    "        current_row_index += len(df_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not text_files_exist:\n",
    "        print(\"\\nSadece metin verisi birleştiriliyor ve kaydediliyor...\")\n",
    "        X_train_text = vstack(X_train_text_chunks)\n",
    "        X_test_text = vstack(X_test_text_chunks)\n",
    "        save_npz(final_text_files[0], X_train_text)\n",
    "        save_npz(final_text_files[1], X_test_text)\n",
    "        np.save(final_text_files[2], y_train)\n",
    "        np.save(final_text_files[3], y_test)\n",
    "        print(\"✅ Sadece metin verisi kaydedildi.\")\n",
    "\n",
    "    if not hybrid_files_exist:\n",
    "        print(\"\\nHibrit veri birleştiriliyor ve kaydediliyor...\")\n",
    "        X_train_hybrid = vstack(X_train_hybrid_chunks)\n",
    "        X_test_hybrid = vstack(X_test_hybrid_chunks)\n",
    "        save_npz(final_hybrid_files[0], X_train_hybrid)\n",
    "        save_npz(final_hybrid_files[1], X_test_hybrid)\n",
    "        np.save(final_hybrid_files[2], y_train)\n",
    "        np.save(final_hybrid_files[3], y_test)\n",
    "        print(\"✅ Hibrit veri kaydedildi.\")\n",
    "\n",
    "    print(\"\\n🚀 Tüm işlemler tamamlandı.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
