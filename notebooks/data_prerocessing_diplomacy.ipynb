{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "418ab344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c90330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "except LookupError:\n",
    "    print(\"NLTK 'stopwords' ve 'wordnet' paketleri indiriliyor...\")\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac8f4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def flatten_jsonl_data(file_path):\n",
    "    \"\"\"\n",
    "    Konuşma bazlı JSONL dosyasını okur, boş satırları atlar ve \n",
    "    mesaj bazlı bir DataFrame'e dönüştürür.\n",
    "    \"\"\"\n",
    "    print(f\"{os.path.basename(file_path)} dosyası işleniyor...\")\n",
    "    \n",
    "    all_messages = []\n",
    "    \n",
    "    # Dosyayı satır satır aç ve oku\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Satırın başındaki/sonundaki boşlukları temizle\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Eğer satır boşsa (dosya sonundaki boş satır gibi), bu satırı atla\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Satırı bir JSON objesi olarak yükle\n",
    "            row = json.loads(line)\n",
    "            \n",
    "            # Şimdi bu 'row' objesi (konuşma) üzerinden döngüye gir\n",
    "            # (Burada sendeki 'iterrows' ve 'messages' hatalarını da düzelttim)\n",
    "            for i in range(len(row[\"messages\"])):\n",
    "                message_data = {\n",
    "                    'game_id': row['game_id'],\n",
    "                    'speaker': row['speakers'][i],\n",
    "                    'receiver': row['receivers'][i],\n",
    "                    'message_text': row['messages'][i],\n",
    "                    'sender_intention': row['sender_labels'][i],\n",
    "                    'game_score': row['game_score'][i],\n",
    "                    'game_score_delta': row['game_score_delta'][i],\n",
    "                    'year': row['years'][i],\n",
    "                    'season': row['seasons'][i],\n",
    "                    'original_fold': row.get('acl2020_fold', os.path.basename(file_path).split('.')[0])\n",
    "                }\n",
    "                all_messages.append(message_data)\n",
    "                \n",
    "    return pd.DataFrame(all_messages)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56696767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.jsonl dosyası işleniyor...\n",
      "validation.jsonl dosyası işleniyor...\n",
      "test.jsonl dosyası işleniyor...\n",
      "\n",
      "Tüm veri setleri EDA için birleştirildi.\n",
      "Toplam mesaj (satır) sayısı: 17289\n",
      "Veri temizleme ve özellik mühendisliği başlıyor...\n",
      "Temizlik tamamlandı.\n",
      "şuraya kaydediliyor:  c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\processed\\diplomacy_processed.parquet\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17289 entries, 0 to 17288\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   game_id           17289 non-null  int64 \n",
      " 1   speaker           17289 non-null  object\n",
      " 2   receiver          17289 non-null  object\n",
      " 3   message_text      17289 non-null  object\n",
      " 4   sender_intention  17289 non-null  bool  \n",
      " 5   game_score        17289 non-null  int64 \n",
      " 6   game_score_delta  17289 non-null  int64 \n",
      " 7   year              17289 non-null  int64 \n",
      " 8   season            17289 non-null  object\n",
      " 9   original_fold     17289 non-null  object\n",
      " 10  target            17289 non-null  int64 \n",
      " 11  cleaned_text      17289 non-null  object\n",
      " 12  message_length    17289 non-null  int64 \n",
      "dtypes: bool(1), int64(6), object(6)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\")\n",
    "train_path = os.path.join(data_folder_path, \"train.jsonl\")\n",
    "val_path = os.path.join(data_folder_path, \"validation.jsonl\")\n",
    "test_path = os.path.join(data_folder_path, \"test.jsonl\")\n",
    "\n",
    "df_train = flatten_jsonl_data(train_path)\n",
    "df_val = flatten_jsonl_data(val_path)\n",
    "df_test = flatten_jsonl_data(test_path)\n",
    "\n",
    "df = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "print(\"\\nTüm veri setleri EDA için birleştirildi.\")\n",
    "print(f\"Toplam mesaj (satır) sayısı: {len(df)}\")\n",
    "\n",
    "print(\"Veri temizleme ve özellik mühendisliği başlıyor...\")\n",
    "df['target'] = df['sender_intention'].apply(lambda x: 1 if x == False else 0)\n",
    "\n",
    "df['cleaned_text'] = df['message_text'].apply(clean_text)\n",
    "\n",
    "numeric_cols = ['game_score', 'game_score_delta', 'year']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['message_length'] = df['message_text'].apply(len)\n",
    "print(\"Temizlik tamamlandı.\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(data_folder_path, \"processed\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_FILE_PATH = os.path.join(OUTPUT_DIR, \"diplomacy_processed.parquet\")\n",
    "print(\"şuraya kaydediliyor: \", OUTPUT_FILE_PATH)\n",
    "df.to_parquet(OUTPUT_FILE_PATH, index=False)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3af219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(base_path, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"processed\", \"diplomacy_eda_processed.parquet\")\n",
    "reports_dir = os.path.join(base_path, \"reports\", \"figures\")\n",
    "os.makedirs(reports_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e98365f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\processed\\diplomacy_eda_processed.parquet' dosyasından 17289 satır veri başarıyla yüklendi.\n",
      "Grafikler 'c:\\work environment\\Projects\\amazon-spam-review\\reports\\figures' klasörüne kaydedilecek.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_parquet(data_path)\n",
    "    print(f\"'{data_path}' dosyasından {len(data)} satır veri başarıyla yüklendi.\")\n",
    "    print(f\"Grafikler '{reports_dir}' klasörüne kaydedilecek.\")\n",
    "    # Grafik stilini ayarla\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"HATA: '{data_path}' dosyası bulunamadı!\")\n",
    "    print(\"Lütfen bir önceki ön işleme (preprocessing) kodunu çalıştırdığından emin ol.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
