{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "DATA_PATH = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25054da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diplomacy_map():\n",
    "    \"\"\"\n",
    "    Returns a dictionary where each key is a territory and the value\n",
    "    is a list of territories it borders.\n",
    "    This is the static map of the game.\n",
    "    \"\"\"\n",
    "    # This map defines all territory adjacencies in Diplomacy\n",
    "    return {\n",
    "        'Cly': ['Edi', 'Lvp', 'Nao', 'Nwg'],\n",
    "        'Edi': ['Cly', 'Lvp', 'Nth', 'Nwg', 'Yor'],\n",
    "        'Lvp': ['Cly', 'Edi', 'Iri', 'Nao', 'Wal', 'Yor'],\n",
    "        'Yor': ['Edi', 'Lvp', 'Lon', 'Nth', 'Wal'],\n",
    "        'Wal': ['Lvp', 'Yor', 'Lon', 'Eng', 'Iri'],\n",
    "        'Lon': ['Yor', 'Wal', 'Eng', 'Nth'],\n",
    "        'Nwy': ['Bar', 'Nwg', 'Nth', 'Ska', 'Stp', 'Swe'],\n",
    "        'Swe': ['Nwy', 'Ska', 'Den', 'Bal', 'Bot', 'Fin'],\n",
    "        'Den': ['Swe', 'Ska', 'Nth', 'Hel', 'Kie', 'Bal'],\n",
    "        'Kie': ['Den', 'Hel', 'Hol', 'Ruh', 'Mun', 'Ber', 'Bal'],\n",
    "        'Ber': ['Kie', 'Bal', 'Pru', 'Sil', 'Mun'],\n",
    "        'Pru': ['Ber', 'Bal', 'Lvn', 'War', 'Sil'],\n",
    "        'Stp': ['Nwy', 'Bar', 'Mos', 'Lvn', 'Bot', 'Fin'],\n",
    "        'Fin': ['Nwy', 'Stp', 'Bot', 'Swe'],\n",
    "        'Lvn': ['Stp', 'Mos', 'War', 'Pru', 'Bal', 'Bot'],\n",
    "        'Mos': ['Stp', 'Lvn', 'War', 'Ukr', 'Sev'],\n",
    "        'War': ['Pru', 'Lvn', 'Mos', 'Ukr', 'Gal', 'Sil'],\n",
    "        'Sil': ['Ber', 'Mun', 'Boh', 'Gal', 'War', 'Pru'],\n",
    "        'Hol': ['Kie', 'Hel', 'Nth', 'Eng', 'Bel', 'Ruh'],\n",
    "        'Bel': ['Hol', 'Eng', 'Pic', 'Bur', 'Ruh'],\n",
    "        'Ruh': ['Kie', 'Hol', 'Bel', 'Bur', 'Mun'],\n",
    "        'Mun': ['Kie', 'Ruh', 'Bur', 'Tyr', 'Boh', 'Sil', 'Ber'],\n",
    "        'Boh': ['Mun', 'Tyr', 'Vie', 'Gal', 'Sil'],\n",
    "        'Gal': ['Boh', 'Vie', 'Bud', 'Rum', 'Ukr', 'War', 'Sil'],\n",
    "        'Ukr': ['Mos', 'War', 'Gal', 'Rum', 'Sev'],\n",
    "        'Sev': ['Mos', 'Ukr', 'Rum', 'Bla', 'Arm'],\n",
    "        'Bre': ['Eng', 'Mao', 'Pic', 'Par', 'Gas'],\n",
    "        'Pic': ['Eng', 'Bel', 'Bur', 'Par', 'Bre'],\n",
    "        'Par': ['Pic', 'Bur', 'Gas', 'Bre'],\n",
    "        'Bur': ['Bel', 'Ruh', 'Mun', 'Mar', 'Gas', 'Par', 'Pic'],\n",
    "        'Gas': ['Bre', 'Par', 'Bur', 'Mar', 'Spa', 'Mao'],\n",
    "        'Mar': ['Bur', 'Gas', 'Spa', 'Lyo'],\n",
    "        'Spa': ['Gas', 'Mar', 'Lyo', 'Wes', 'Mao', 'Por'],\n",
    "        'Por': ['Spa', 'Mao'],\n",
    "        'Nao': ['Cly', 'Lvp', 'Iri', 'Mao'],\n",
    "        'Iri': ['Lvp', 'Wal', 'Eng', 'Mao', 'Nao'],\n",
    "        'Eng': ['Wal', 'Lon', 'Nth', 'Hol', 'Bel', 'Pic', 'Bre', 'Mao', 'Iri'],\n",
    "        'Mao': ['Bre', 'Gas', 'Spa', 'Por', 'Nao', 'Iri', 'Eng', 'Wes'],\n",
    "        'Nth': ['Edi', 'Yor', 'Lon', 'Eng', 'Hol', 'Hel', 'Den', 'Ska', 'Nwy', 'Nwg'],\n",
    "        'Nwg': ['Cly', 'Edi', 'Nth', 'Nwy', 'Bar'],\n",
    "        'Bar': ['Nwy', 'Nwg', 'Stp'],\n",
    "        'Hel': ['Den', 'Kie', 'Hol', 'Nth'],\n",
    "        'Ska': ['Nwy', 'Swe', 'Den', 'Nth'],\n",
    "        'Bal': ['Swe', 'Den', 'Kie', 'Ber', 'Pru', 'Lvn', 'Bot'],\n",
    "        'Bot': ['Swe', 'Fin', 'Stp', 'Lvn', 'Bal'],\n",
    "        'Wes': ['Spa', 'Mar', 'Lyo', 'Tyn', 'Mao'],\n",
    "        'Lyo': ['Mar', 'Spa', 'Wes', 'Tyn', 'Pie', 'Bur'],\n",
    "        'Tyn': ['Wes', 'Lyo', 'Ion', 'Tun', 'Rom'],\n",
    "        'Ion': ['Tyn', 'Rom', 'Nap', 'Apu', 'Adr', 'Alb', 'Gre', 'Aeg', 'Eas'],\n",
    "        'Adr': ['Ven', 'Tri', 'Alb', 'Ion', 'Apu'],\n",
    "        'Aeg': ['Gre', 'Ion', 'Eas', 'Smy', 'Con', 'Bul'],\n",
    "        'Eas': ['Ion', 'Aeg', 'Smy', 'Syr'],\n",
    "        'Bla': ['Sev', 'Arm', 'Ank', 'Con', 'Bul', 'Rum'],\n",
    "        'Pie': ['Mar', 'Lyo', 'Tyr', 'Ven'],\n",
    "        'Ven': ['Pie', 'Tyr', 'Tri', 'Adr', 'Apu', 'Rom'],\n",
    "        'Rom': ['Ven', 'Apu', 'Nap', 'Tyn', 'Lyo', 'Pie'],\n",
    "        'Nap': ['Rom', 'Apu', 'Ion'],\n",
    "        'Apu': ['Ven', 'Adr', 'Ion', 'Nap', 'Rom'],\n",
    "        'Tyr': ['Mun', 'Boh', 'Vie', 'Tri', 'Ven', 'Pie', 'Bur'],\n",
    "        'Vie': ['Boh', 'Gal', 'Bud', 'Tri', 'Tyr'],\n",
    "        'Tri': ['Vie', 'Bud', 'Ser', 'Alb', 'Adr', 'Ven', 'Tyr'],\n",
    "        'Bud': ['Vie', 'Gal', 'Rum', 'Ser', 'Tri'],\n",
    "        'Rum': ['Gal', 'Ukr', 'Sev', 'Bla', 'Bul', 'Ser', 'Bud'],\n",
    "        'Ser': ['Bud', 'Rum', 'Bul', 'Gre', 'Alb', 'Tri'],\n",
    "        'Alb': ['Tri', 'Ser', 'Gre', 'Ion', 'Adr'],\n",
    "        'Gre': ['Ser', 'Alb', 'Ion', 'Aeg', 'Bul'],\n",
    "        'Bul': ['Rum', 'Bla', 'Con', 'Aeg', 'Gre', 'Ser'],\n",
    "        'Con': ['Bul', 'Aeg', 'Smy', 'Ank', 'Bla'],\n",
    "        'Ank': ['Con', 'Bla', 'Arm', 'Smy'],\n",
    "        'Arm': ['Sev', 'Bla', 'Ank', 'Smy', 'Syr'],\n",
    "        'Smy': ['Con', 'Ank', 'Arm', 'Syr', 'Eas', 'Aeg'],\n",
    "        'Syr': ['Arm', 'Smy', 'Eas'],\n",
    "        'Tun': ['Tyn', 'Ion', 'Naf'],\n",
    "        'Naf': ['Tun']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f67de40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_players_adjacent(sender, receiver, territories_data, board_map):\n",
    "    \"\"\"\n",
    "    Checks if a sender and receiver control any bordering territories.\n",
    "    \"\"\"\n",
    "    sender_territories = set()\n",
    "    receiver_territories = set()\n",
    "    \n",
    "    # 1. Find all territories for each player\n",
    "    # territories_data is like: {\"Par\": \"France\", \"Smy\": \"Turkey\", ...}\n",
    "    for territory, owner in territories_data.items():\n",
    "        if owner.lower() == sender:\n",
    "            sender_territories.add(territory)\n",
    "        elif owner.lower() == receiver:\n",
    "            receiver_territories.add(territory)\n",
    "            \n",
    "    # 2. Check for adjacency\n",
    "    for sender_terr in sender_territories:\n",
    "        # Check if this territory is even on the map (it might be a sea zone)\n",
    "        if sender_terr in board_map:\n",
    "            # Look up all neighbors\n",
    "            for neighbor in board_map[sender_terr]:\n",
    "                # If any neighbor is owned by the receiver, they are adjacent\n",
    "                if neighbor in receiver_territories:\n",
    "                    return True\n",
    "                    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a0550d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build game state dictionary from: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "Successfully loaded and indexed 342 game state files.\n",
      "\n",
      "Test lookup failed. Check your directory path and filename pattern.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_game_state_dictionary(game_state_dir):\n",
    "    \"\"\"\n",
    "    Scans a directory of game state JSON files and loads them into a\n",
    "    nested dictionary structured as:\n",
    "    game_states[game_id][year][season] = file_content\n",
    "    \"\"\"\n",
    "\n",
    "    filename_pattern = re.compile(r\"DiplomacyGame(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "\n",
    "    # We use defaultdict to automatically create nested dictionaries\n",
    "    game_states = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    print(f\"Starting to build game state dictionary from: {game_state_dir}\")\n",
    "    \n",
    "    file_count = 0\n",
    "    for filename in os.listdir(game_state_dir):\n",
    "        match = filename_pattern.match(filename)\n",
    "        \n",
    "        if match:\n",
    "            # 1. Parse filename\n",
    "            game_id = int(match.group(1)) # Convert game ID to integer\n",
    "            year = match.group(2)         # Keep year as a string (like \"1901\")\n",
    "            season = match.group(3).lower()       # Keep season as a string (like \"Spring\")\n",
    "            \n",
    "            # 2. Load the JSON file content\n",
    "            file_path = os.path.join(game_state_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # 3. Store it in the nested dictionary\n",
    "                    game_states[game_id][year][season] = data\n",
    "                    file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load or parse {filename}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Skipping file with unxpected name format: {filename}\")\n",
    "\n",
    "    print(f\"Successfully loaded and indexed {file_count} game state files.\")\n",
    "    return game_states\n",
    "\n",
    "# --- How to use it ---\n",
    "\n",
    "# 1. DEFINE the path to your folder containing the game state files\n",
    "GAME_STATE_DIRECTORY = os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\") # <-- IMPORTANT: Change this to your folder path\n",
    "\n",
    "# 2. RUN the function to build the dictionary\n",
    "#    (This might take a moment if you have thousands of files)\n",
    "game_state_lookup = create_game_state_dictionary(GAME_STATE_DIRECTORY)\n",
    "\n",
    "# 3. TEST the dictionary (optional)\n",
    "if 1 in game_state_lookup and '1901' in game_state_lookup[1] and 'Spring' in game_state_lookup[1]['1901']:\n",
    "    print(\"\\nTest lookup successful!\")\n",
    "    # You can inspect a single game state like this:\n",
    "    # print(game_state_lookup[1]['1901']['Spring']['orders'])\n",
    "else:\n",
    "    print(\"\\nTest lookup failed. Check your directory path and filename pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "601a41ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "\n",
      "Loading validation data...\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "\n",
      "Loading test data...\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Example of new features for first 5 training messages:\n",
      "Message: 'Germany!\n",
      "\n",
      "Just the person I want to speak with. I ...' | Features: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "Message: 'You've whet my appetite, Italy. What's the suggest...' | Features: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "Message: 'ğŸ‘...' | Features: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "Message: 'It seems like there are a lot of ways that could g...' | Features: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "Message: 'Yeah, I canâ€™t say Iâ€™ve tried it and it works, caus...' | Features: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n"
     ]
    }
   ],
   "source": [
    "DIPLOMACY_MAP = get_diplomacy_map()\n",
    "\n",
    "def extract_features_from_state(message_data, game_state):\n",
    "    \"\"\"\n",
    "    Extracts features, now including player adjacency.\n",
    "    \"\"\"\n",
    "    new_features = {}\n",
    "    \n",
    "    sender = message_data['speaker'].lower()\n",
    "    receiver = message_data['receiver'].lower()\n",
    "    \n",
    "    # === 1. Extract SC Counts (Same as before) ===\n",
    "    sc_text = game_state.get('sc', '')\n",
    "    sc_pattern = re.compile(r\"\\s*(\\w+)\\s+(\\d+)\\s*\")\n",
    "    all_scores = {}\n",
    "    for match in sc_pattern.finditer(sc_text):\n",
    "        country = match.group(1).lower()\n",
    "        score = int(match.group(2))\n",
    "        all_scores[country] = score\n",
    "    new_features['sender_sc'] = all_scores.get(sender, 0)\n",
    "    new_features['receiver_sc'] = all_scores.get(receiver, 0)\n",
    "\n",
    "    # === 2. Extract Unit Counts (Same as before) ===\n",
    "    orders_data = game_state.get('orders', {})\n",
    "    sender_units = orders_data.get(sender.upper(), {})\n",
    "    receiver_units = orders_data.get(receiver.upper(), {})\n",
    "    new_features['sender_unit_count'] = len(sender_units)\n",
    "    new_features['receiver_unit_count'] = len(receiver_units)\n",
    "    \n",
    "    # === 3. Add Original Score Delta (Same as before) ===\n",
    "    new_features['score_delta'] = message_data['score_delta']\n",
    "    \n",
    "    # === 4. NEW FEATURE: Adjacency ===\n",
    "    territories_data = game_state.get('territories', {})\n",
    "    \n",
    "    is_adjacent = are_players_adjacent(sender, receiver, territories_data, DIPLOMACY_MAP)\n",
    "    \n",
    "    # Convert boolean True/False to 1/0 for the model\n",
    "    new_features['are_adjacent'] = 1 if is_adjacent else 0\n",
    "    \n",
    "    return new_features\n",
    "\n",
    "\n",
    "# --- This is our MODIFIED loading function ---\n",
    "# Create a global set to store keys we've already warned about\n",
    "FAILED_KEYS = set()\n",
    "\n",
    "def load_and_flatten_data(filepath, game_state_lookup):\n",
    "    \"\"\"\n",
    "    Loads the .jsonl file, uses the game_state_lookup to find\n",
    "    the matching game state, and extracts features.\n",
    "    \"\"\"\n",
    "    X_text_list = []\n",
    "    y_list = []\n",
    "    X_new_features_list = [] \n",
    "\n",
    "    with jsonlines.open(filepath, 'r') as reader:\n",
    "        for game in reader:\n",
    "            game_id = game['game_id']\n",
    "            years = game['years']\n",
    "            seasons = game['seasons']\n",
    "            speakers = game['speakers']\n",
    "            \n",
    "            for i in range(len(game['messages'])):\n",
    "                sender_label = game['sender_labels'][i]\n",
    "                \n",
    "                if sender_label == \"NOANNOTATION\":\n",
    "                    continue\n",
    "                \n",
    "                label = 1 if sender_label == False else 0 # Lie = 1, True = 0\n",
    "                message_text = game['messages'][i]\n",
    "                year = years[i]\n",
    "                season = seasons[i].lower()\n",
    "                \n",
    "                try:\n",
    "                    current_game_state = game_state_lookup[game_id][year][season]\n",
    "                \n",
    "                except KeyError:\n",
    "                    # --- THIS IS THE NEW DEBUG CODE ---\n",
    "                    # Create a unique key for this message\n",
    "                    key = f\"GameID: {game_id}, Year: {year}, Season: {season}\"\n",
    "                    \n",
    "                    # Only print the warning once per unique key\n",
    "                    if key not in FAILED_KEYS:\n",
    "                        print(f\"  [Debug Warning] Failed to find state for: {key}\")\n",
    "                        FAILED_KEYS.add(key)\n",
    "                    # --- END DEBUG CODE ---\n",
    "                    \n",
    "                    continue # Skip this message\n",
    "                \n",
    "                message_data = {\n",
    "                    'text': message_text,\n",
    "                    'speaker': speakers[i],\n",
    "                    'receiver': game['receivers'][i],\n",
    "                    'score_delta': int(game['game_score_delta'][i])\n",
    "                }\n",
    "                \n",
    "                new_features = extract_features_from_state(message_data, current_game_state)\n",
    "                \n",
    "                X_text_list.append(message_text)\n",
    "                y_list.append(label)\n",
    "                X_new_features_list.append(new_features)\n",
    "\n",
    "    print(f\"Loaded and processed {len(X_text_list)} samples from {filepath}.\")\n",
    "    return X_text_list, X_new_features_list, np.array(y_list)\n",
    "\n",
    "# --- How to use it ---\n",
    "# (Assuming 'game_state_lookup' is the dictionary you just built)\n",
    "\n",
    "# 1. Load the data using the new function\n",
    "print(\"Loading training data...\")\n",
    "X_train_text, X_train_features_list, y_train = load_and_flatten_data(os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\"), game_state_lookup)\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "X_val_text, X_val_features_list, y_val = load_and_flatten_data(os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"validation.jsonl\"), game_state_lookup)\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "X_test_text, X_test_features_list, y_test = load_and_flatten_data(os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"test.jsonl\"), game_state_lookup)\n",
    "\n",
    "\n",
    "# 2. Check the output\n",
    "print(f\"\\nExample of new features for first 5 training messages:\")\n",
    "for i in range(5):\n",
    "    print(f\"Message: '{X_train_text[i][:50]}...' | Features: {X_train_features_list[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "527e9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Game State Folder ---\n",
      "Looking in: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "\n",
      "First 10 files found in 'moves' folder:\n",
      "  No files matching 'DiplomacyGame_...json' found.\n",
      "\n",
      "--- Checking 'train.jsonl' File ---\n",
      "Reading from: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl\n",
      "\n",
      "Game IDs needed by 'train.jsonl':\n",
      "  Needs GameID: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import jsonlines\n",
    "\n",
    "# --- 1. SET YOUR PATHS HERE ---\n",
    "DATA_PATH = os.path.dirname(os.getcwd())\n",
    "GAME_STATE_DIRECTORY = os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\")\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\")\n",
    "# -----------------------------\n",
    "\n",
    "print(f\"--- Checking Game State Folder ---\")\n",
    "print(f\"Looking in: {GAME_STATE_DIRECTORY}\\n\")\n",
    "\n",
    "if not os.path.exists(GAME_STATE_DIRECTORY):\n",
    "    print(\"Error: Directory not found. Please check the path.\")\n",
    "else:\n",
    "    # 1. Check the first 10 files in the 'moves' folder\n",
    "    print(\"First 10 files found in 'moves' folder:\")\n",
    "    file_pattern = re.compile(r\"DiplomacyGame_(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "    files_found = []\n",
    "    \n",
    "    for filename in os.listdir(GAME_STATE_DIRECTORY):\n",
    "        if file_pattern.match(filename):\n",
    "            files_found.append(filename)\n",
    "        if len(files_found) >= 10:\n",
    "            break\n",
    "            \n",
    "    if not files_found:\n",
    "        print(\"  No files matching 'DiplomacyGame_...json' found.\")\n",
    "    else:\n",
    "        for f in files_found:\n",
    "            print(f\"  {f}\")\n",
    "\n",
    "# 2. Check the first 10 game_ids in the 'train.jsonl' file\n",
    "print(f\"\\n--- Checking 'train.jsonl' File ---\")\n",
    "print(f\"Reading from: {TRAIN_FILE}\\n\")\n",
    "\n",
    "try:\n",
    "    with jsonlines.open(TRAIN_FILE, 'r') as reader:\n",
    "        print(\"Game IDs needed by 'train.jsonl':\")\n",
    "        game_ids_needed = set()\n",
    "        for i, game in enumerate(reader):\n",
    "            game_ids_needed.add(game['game_id'])\n",
    "            if i >= 10: # Just check the first few games\n",
    "                break\n",
    "        \n",
    "        if not game_ids_needed:\n",
    "            print(\"  No games found in 'train.jsonl'.\")\n",
    "        else:\n",
    "            for game_id in game_ids_needed:\n",
    "                print(f\"  Needs GameID: {game_id}\")\n",
    "                \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{TRAIN_FILE}' not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17516503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Data loading complete.\n",
      "Vectorizing new game state features...\n",
      "Feature vectorization complete.\n",
      "Names of new features: ['are_adjacent', 'receiver_sc', 'receiver_unit_count', 'score_delta', 'sender_sc', 'sender_unit_count']\n",
      "Shape of new training feature matrix: (13132, 6)\n",
      "Example of first numeric feature row: [0. 3. 0. 0. 3. 0.]\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer # <-- Import the tool\n",
    "\n",
    "# --- (Your create_game_state_dictionary function would be here) ---\n",
    "# ...\n",
    "# game_state_lookup = create_game_state_dictionary(...)\n",
    "\n",
    "# --- (Your extract_features_from_state function would be here) ---\n",
    "# ...\n",
    "\n",
    "# --- (Your load_and_flatten_data function would be here) ---\n",
    "# ...\n",
    "\n",
    "# --- 1. Load All Data (as before) ---\n",
    "print(\"Loading data...\")\n",
    "X_train_text, X_train_features_list, y_train = load_and_flatten_data(os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\"), game_state_lookup)\n",
    "X_val_text, X_val_features_list, y_val = load_and_flatten_data(os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"validation.jsonl\"), game_state_lookup)\n",
    "X_test_text, X_test_features_list, y_test = load_and_flatten_data(os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"test.jsonl\"), game_state_lookup)\n",
    "\n",
    "print(\"\\nData loading complete.\")\n",
    "\n",
    "# --- 2. Convert Feature Dictionaries to a Numeric Matrix ---\n",
    "print(\"Vectorizing new game state features...\")\n",
    "\n",
    "# Create the vectorizer\n",
    "feature_vectorizer = DictVectorizer(sparse=False) # Use sparse=True for very large feature sets\n",
    "\n",
    "# Fit ONLY on the training data\n",
    "X_train_features_numeric = feature_vectorizer.fit_transform(X_train_features_list)\n",
    "\n",
    "# Transform the validation and test data (DO NOT re-fit)\n",
    "X_val_features_numeric = feature_vectorizer.transform(X_val_features_list)\n",
    "X_test_features_numeric = feature_vectorizer.transform(X_test_features_list)\n",
    "\n",
    "# --- 3. Check the Output ---\n",
    "print(\"Feature vectorization complete.\")\n",
    "print(f\"Names of new features: {feature_vectorizer.feature_names_}\")\n",
    "print(f\"Shape of new training feature matrix: {X_train_features_numeric.shape}\")\n",
    "\n",
    "# Example:\n",
    "if len(X_train_features_numeric) > 0:\n",
    "    print(f\"Example of first numeric feature row: {X_train_features_numeric[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93c4aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a3efe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for TASK=SENDER, POWER=y\n",
      "Loading lexicon from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\utils\\2015_Diplomacy_lexicon.json...\n",
      "Successfully loaded lexicon with 10 feature categories.\n",
      "Loading datasets...\n",
      "Aggregating messages...\n",
      "Converting data to lexicon features...\n",
      "Splitting features and labels...\n",
      "Scaling features...\n",
      "Feature matrix shape: (13132, 12)\n",
      "Training samples: 13132, Test samples: 2741\n",
      "Training Logistic Regression model...\n",
      "Evaluating model on test set...\n",
      "\n",
      "=============================================\n",
      "PERFORMANCE TEST: Logistic Regression (Lexicon, SENDER)\n",
      "=============================================\n",
      "F1 Score (Lie Class): 0.2347\n",
      "F1 Score (Macro Avg): 0.5323\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted 0 (Truth)  |  Predicted 1 (Lie)\n",
      "Actual 0:    1861                 | 640                 \n",
      "Actual 1:    123                  | 117                 \n",
      "\n",
      "Full Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (Truth)       0.94      0.74      0.83      2501\n",
      "  Class 1 (Lie)       0.15      0.49      0.23       240\n",
      "\n",
      "       accuracy                           0.72      2741\n",
      "      macro avg       0.55      0.62      0.53      2741\n",
      "   weighted avg       0.87      0.72      0.78      2741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "# import spacy  <- REMOVED\n",
    "# from spacy.lang.en import English <- REMOVED\n",
    "import re # <- ADDED\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Global Settings & Initializer ---\n",
    "# We will set TASK and POWER from command-line args, just like the original\n",
    "# nlp = None # Will be initialized in main <- REMOVED\n",
    "\n",
    "# --- 2. Helper Functions (from harbringers.py) ---\n",
    "\n",
    "def regex_tokenizer(text): # <- RENAMED and RE-WRITTEN\n",
    "    \"\"\"\n",
    "    Tokenizes text using regular expressions.\n",
    "    This avoids the need for spacy.\n",
    "    It finds all sequences of word characters (letters, numbers, underscore),\n",
    "    which also effectively removes punctuation.\n",
    "    \"\"\"\n",
    "    # \\b matches a word boundary\n",
    "    # \\w+ matches one or more word characters (alphanumeric + underscore)\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "def aggregate(dataset):\n",
    "    \"\"\"Flattens dialogs into a list of single messages\"\"\"\n",
    "    messages = []\n",
    "    rec = []\n",
    "    send = []\n",
    "    power = []\n",
    "    for dialogs in dataset:\n",
    "        messages.extend(dialogs['messages'])\n",
    "        rec.extend(dialogs['receiver_labels'])\n",
    "        send.extend(dialogs['sender_labels'])\n",
    "        power.extend(dialogs['game_score_delta'])\n",
    "    \n",
    "    merged = []\n",
    "    for i, item in enumerate(messages):\n",
    "        merged.append({\n",
    "            'message': item,\n",
    "            'sender_annotation': send[i],\n",
    "            'receiver_annotation': rec[i],\n",
    "            'score_delta': int(power[i])\n",
    "        })\n",
    "    return merged\n",
    "\n",
    "def convert_to_features(dataset, feature_dict, TASK, POWER):\n",
    "    \"\"\"\n",
    "    This is the core of harbringers.py. It converts a message\n",
    "    into a vector of lexicon-based features.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for message in dataset:\n",
    "        \n",
    "        annotation_key = 'sender_annotation' if TASK == \"SENDER\" else 'receiver_annotation'\n",
    "        \n",
    "        # Skip messages with no label\n",
    "        if message[annotation_key] == \"NOANNOTATION\":\n",
    "            continue\n",
    "\n",
    "        features = []\n",
    "        \n",
    "        # 1. Lexicon Features\n",
    "        tokenized_message = regex_tokenizer(message['message'].lower()) # <- USE NEW FUNCTION\n",
    "        \n",
    "        for feature_category in feature_dict.keys():\n",
    "            total_count = 0\n",
    "            for word in feature_dict[feature_category]:\n",
    "                # Count all occurrences of this word\n",
    "                total_count += tokenized_message.count(word)\n",
    "            features.append(total_count)\n",
    "        \n",
    "        # 2. Power Features\n",
    "        if POWER == \"y\":\n",
    "            features.append(1 if message['score_delta'] > 4 else 0)\n",
    "            features.append(1 if message['score_delta'] < -4 else 0)\n",
    "        \n",
    "        # 3. Label (Appended last)\n",
    "        # We'll use our convention: Lie = 1, True = 0\n",
    "        if message[annotation_key] == False:\n",
    "            features.append(1) # Lie\n",
    "        else:\n",
    "            features.append(0) # True\n",
    "            \n",
    "        processed_data.append(features)\n",
    "        \n",
    "    return processed_data\n",
    "\n",
    "def split_xy(data):\n",
    "    \"\"\"Splits a list of lists into X (features) and y (labels)\"\"\"\n",
    "    X, y = [], []\n",
    "    for line in data:\n",
    "        X.append(line[:-1])  # All but the last item\n",
    "        y.append(line[-1])   # The last item\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    \"\"\"Prints a detailed performance report\"\"\"\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    # Use pos_label=1 to get F1 for the \"Lie\" class\n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    # Set zero_division=0 to avoid errors if a class is never predicted\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)'], zero_division=0))\n",
    "\n",
    "# --- 3. MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Parse Command-Line Arguments (like harbringers.py) ---\n",
    "    # This script defaults to 'sender' (s) and 'power' (y)\n",
    "    # You can run: python test_lexicon_model.py s n  (to turn off power)\n",
    "    # Or:          python test_lexicon_model.py r y  (to predict receiver labels)\n",
    "    \n",
    "    TASK = \"SENDER\"\n",
    "    POWER = \"y\"\n",
    "    \n",
    "    if len(sys.argv) >= 2:\n",
    "        if sys.argv[1] == 's':\n",
    "            TASK = \"SENDER\"\n",
    "        elif sys.argv[1] == 'r':\n",
    "            TASK = \"RECEIVER\"\n",
    "    if len(sys.argv) >= 3:\n",
    "        if sys.argv[2] == 'n':\n",
    "            POWER = 'n'\n",
    "    \n",
    "    print(f\"Running model for TASK={TASK}, POWER={POWER}\")\n",
    "    \n",
    "    # --- !! IMPORTANT !! ---\n",
    "    # Make sure these paths are correct for your project structure\n",
    "    DATA_PATH = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\")\n",
    "    LEXICON_PATH = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"utils\", \"2015_Diplomacy_lexicon.json\")\n",
    "    # -----------------------\n",
    "\n",
    "    # 1. Load Lexicon\n",
    "    print(f\"Loading lexicon from {LEXICON_PATH}...\")\n",
    "    try:\n",
    "        with open(LEXICON_PATH, 'r') as f:\n",
    "            feature_dict = json.loads(f.readline())\n",
    "        \n",
    "        # Add custom features from the original script\n",
    "        feature_dict['but'] = ['but']\n",
    "        feature_dict['countries'] = ['austria', 'england', 'france', 'germany', 'italy', 'russia', 'turkey']\n",
    "        print(f\"Successfully loaded lexicon with {len(feature_dict)} feature categories.\")\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Could not load lexicon file at {LEXICON_PATH}\")\n",
    "        print(f\"Please ensure the file exists.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1) # Exit the script if lexicon fails\n",
    "\n",
    "    # 2. Load Datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    try:\n",
    "        with jsonlines.open(os.path.join(DATA_PATH, 'train.jsonl'), 'r') as reader:\n",
    "            train_data_raw = list(reader)\n",
    "        with jsonlines.open(os.path.join(DATA_PATH, 'test.jsonl'), 'r') as reader:\n",
    "            test_data_raw = list(reader)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CRITICAL ERROR: Could not find data files in '{DATA_PATH}'\")\n",
    "        print(\"Please ensure DATA_PATH is set correctly.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. Process Data (Aggregate -> Convert to Features -> Split)\n",
    "    print(\"Aggregating messages...\")\n",
    "    train_agg = aggregate(train_data_raw)\n",
    "    test_agg = aggregate(test_data_raw)\n",
    "    \n",
    "    print(\"Converting data to lexicon features...\")\n",
    "    train_converted = convert_to_features(train_agg, feature_dict, TASK, POWER)\n",
    "    test_converted = convert_to_features(test_agg, feature_dict, TASK, POWER)\n",
    "    \n",
    "    print(\"Splitting features and labels...\")\n",
    "    X_train, y_train = split_xy(train_converted)\n",
    "    X_test, y_test = split_xy(test_converted)\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(\"CRITICAL ERROR: No data was processed.\")\n",
    "        print(\"This might happen if 'NOANNOTATION' is the only label.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 4. Scale Features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Training samples: {len(y_train)}, Test samples: {len(y_test)}\")\n",
    "\n",
    "    # 5. Train Model\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    # class_weight='balanced' is CRITICAL for this imbalanced dataset\n",
    "    logmodel = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    logmodel.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 6. Evaluate\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    y_pred = logmodel.predict(X_test_scaled)\n",
    "    evaluate_model(f\"Logistic Regression (Lexicon, {TASK})\", y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae4cf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Starting to build game state dictionary from: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "Successfully loaded and indexed 342 game state files.\n",
      "Loading all datasets...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n",
      "Game state feature names: ['mentions_origin_unit', 'mentions_own_territory', 'mentions_receiver_territory', 'mentions_target_territory', 'receiver_sc', 'receiver_unit_count', 'score_delta', 'sender_sc', 'sender_unit_count', 'territory_mention_count']\n",
      "Game state data shape: (13132, 10)\n",
      "\n",
      "Building Neural Network Model...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n",
      "Game state feature names: ['mentions_origin_unit', 'mentions_own_territory', 'mentions_receiver_territory', 'mentions_target_territory', 'receiver_sc', 'receiver_unit_count', 'score_delta', 'sender_sc', 'sender_unit_count', 'territory_mention_count']\n",
      "Game state data shape: (13132, 10)\n",
      "\n",
      "Building Neural Network Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_input          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_1         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">640,064</span> â”‚ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d_1 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚ spatial_dropout1â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ game_state_input    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lstm_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ game_state_inputâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,800</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output_layer        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_input          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_1         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)   â”‚    \u001b[38;5;34m640,064\u001b[0m â”‚ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d_1 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚     \u001b[38;5;34m33,024\u001b[0m â”‚ spatial_dropout1â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ game_state_input    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lstm_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ game_state_inputâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m4,800\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output_layer        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         â”‚         \u001b[38;5;34m65\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">677,953</span> (2.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m677,953\u001b[0m (2.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">677,953</span> (2.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m677,953\u001b[0m (2.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights (for balancing): {0: np.float64(0.5235627142971055), 1: np.float64(11.109983079526227)}\n",
      "\n",
      "Training the model...\n",
      "Epoch 1/15\n",
      "Epoch 1/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 0.6430 - loss: 0.6821 - val_accuracy: 0.5395 - val_loss: 0.7376\n",
      "Epoch 2/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 0.6430 - loss: 0.6821 - val_accuracy: 0.5395 - val_loss: 0.7376\n",
      "Epoch 2/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - accuracy: 0.6550 - loss: 0.6664 - val_accuracy: 0.7874 - val_loss: 0.6766\n",
      "Epoch 3/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - accuracy: 0.6550 - loss: 0.6664 - val_accuracy: 0.7874 - val_loss: 0.6766\n",
      "Epoch 3/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6643 - loss: 0.6721 - val_accuracy: 0.6384 - val_loss: 0.6981\n",
      "Epoch 4/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6643 - loss: 0.6721 - val_accuracy: 0.6384 - val_loss: 0.6981\n",
      "Epoch 4/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - accuracy: 0.6525 - loss: 0.6678 - val_accuracy: 0.6695 - val_loss: 0.6836\n",
      "Epoch 5/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - accuracy: 0.6525 - loss: 0.6678 - val_accuracy: 0.6695 - val_loss: 0.6836\n",
      "Epoch 5/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6457 - loss: 0.6628 - val_accuracy: 0.7225 - val_loss: 0.6412\n",
      "Epoch 6/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6457 - loss: 0.6628 - val_accuracy: 0.7225 - val_loss: 0.6412\n",
      "Epoch 6/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.6954 - loss: 0.6572 - val_accuracy: 0.6370 - val_loss: 0.7080\n",
      "Epoch 7/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.6954 - loss: 0.6572 - val_accuracy: 0.6370 - val_loss: 0.7080\n",
      "Epoch 7/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6538 - loss: 0.6553 - val_accuracy: 0.7105 - val_loss: 0.6564\n",
      "Epoch 8/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6538 - loss: 0.6553 - val_accuracy: 0.7105 - val_loss: 0.6564\n",
      "Epoch 8/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6892 - loss: 0.6558 - val_accuracy: 0.7140 - val_loss: 0.6526\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6892 - loss: 0.6558 - val_accuracy: 0.7140 - val_loss: 0.6526\n",
      "\n",
      "Evaluating final model on test set...\n",
      "\n",
      "Evaluating final model on test set...\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "\n",
      "=============================================\n",
      "PERFORMANCE TEST: Dual-Input Neural Network\n",
      "=============================================\n",
      "F1 Score (Lie Class): 0.1993\n",
      "F1 Score (Macro Avg): 0.4917\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted 0 (Truth)  |  Predicted 1 (Lie)\n",
      "Actual 0:    1693                 | 808                 \n",
      "Actual 1:    124                  | 116                 \n",
      "\n",
      "Full Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (Truth)       0.93      0.68      0.78      2501\n",
      "  Class 1 (Lie)       0.13      0.48      0.20       240\n",
      "\n",
      "       accuracy                           0.66      2741\n",
      "      macro avg       0.53      0.58      0.49      2741\n",
      "   weighted avg       0.86      0.66      0.73      2741\n",
      "\n",
      "\n",
      "=============================================\n",
      "PERFORMANCE TEST: Dual-Input Neural Network\n",
      "=============================================\n",
      "F1 Score (Lie Class): 0.1993\n",
      "F1 Score (Macro Avg): 0.4917\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted 0 (Truth)  |  Predicted 1 (Lie)\n",
      "Actual 0:    1693                 | 808                 \n",
      "Actual 1:    124                  | 116                 \n",
      "\n",
      "Full Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (Truth)       0.93      0.68      0.78      2501\n",
      "  Class 1 (Lie)       0.13      0.48      0.20       240\n",
      "\n",
      "       accuracy                           0.66      2741\n",
      "      macro avg       0.53      0.58      0.49      2741\n",
      "   weighted avg       0.86      0.66      0.73      2741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure TensorFlow is installed in the notebook environment\n",
    "%pip install tensorflow --quiet\n",
    "\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. NEW: EXPERT KNOWLEDGE MAPPING ---\n",
    "\n",
    "# This map is our \"secret weapon\". It connects text (\"paris\") to game data (\"Par\").\n",
    "NAME_TO_ABBREVIATION = {\n",
    "    'north atlantic ocean': 'Nao', 'nao': 'Nao',\n",
    "    'irish sea': 'Iri', 'iri': 'Iri',\n",
    "    'english channel': 'Eng', 'eng': 'Eng', 'channel': 'Eng',\n",
    "    'mid-atlantic ocean': 'Mao', 'mao': 'Mao', 'mid atlantic': 'Mao',\n",
    "    'north sea': 'Nth', 'nth': 'Nth',\n",
    "    'norwegian sea': 'Nwg', 'nwg': 'Nwg',\n",
    "    'barents sea': 'Bar', 'bar': 'Bar',\n",
    "    'helgoland bight': 'Hel', 'hel': 'Hel',\n",
    "    'skagerrak': 'Ska', 'ska': 'Ska',\n",
    "    'baltic sea': 'Bal', 'bal': 'Bal',\n",
    "    'gulf of bothnia': 'Bot', 'bot': 'Bot',\n",
    "    'western mediterranean': 'Wes', 'wes': 'Wes',\n",
    "    'gulf of lyon': 'Lyo', 'lyo': 'Lyo',\n",
    "    'tyrrhenian sea': 'Tyn', 'tyn': 'Tyn',\n",
    "    'ionian sea': 'Ion', 'ion': 'Ion',\n",
    "    'adriatic sea': 'Adr', 'adr': 'Adr',\n",
    "    'aegean sea': 'Aeg', 'aeg': 'Aeg',\n",
    "    'eastern mediterranean': 'Eas', 'eas': 'Eas',\n",
    "    'black sea': 'Bla', 'bla': 'Bla',\n",
    "    \n",
    "    'clyde': 'Cly', 'cly': 'Cly',\n",
    "    'edinburgh': 'Edi', 'edi': 'Edi',\n",
    "    'liverpool': 'Lvp', 'lvp': 'Lvp',\n",
    "    'yorkshire': 'Yor', 'yor': 'Yor',\n",
    "    'wales': 'Wal', 'wal': 'Wal',\n",
    "    'london': 'Lon', 'lon': 'Lon',\n",
    "    'belgium': 'Bel', 'bel': 'Bel',\n",
    "    'holland': 'Hol', 'hol': 'Hol',\n",
    "    'denmark': 'Den', 'den': 'Den',\n",
    "    'sweden': 'Swe', 'swe': 'Swe',\n",
    "    'norway': 'Nwy', 'nwy': 'Nwy',\n",
    "    'finland': 'Fin', 'fin': 'Fin',\n",
    "    'st petersburg': 'Stp', 'stp': 'Stp',\n",
    "    'livonia': 'Lvn', 'lvn': 'Lvn',\n",
    "    'prussia': 'Pru', 'pru': 'Pru',\n",
    "    'kiel': 'Kie', 'kie': 'Kie',\n",
    "    'berlin': 'Ber', 'ber': 'Ber',\n",
    "    'silesia': 'Sil', 'sil': 'Sil',\n",
    "    'poland': 'War', 'war': 'War', 'warsaw': 'War',\n",
    "    'moscow': 'Mos', 'mos': 'Mos',\n",
    "    'ukraine': 'Ukr', 'ukr': 'Ukr',\n",
    "    'sevastopol': 'Sev', 'sev': 'Sev',\n",
    "    'ruhr': 'Ruh', 'ruh': 'Ruh',\n",
    "    'munich': 'Mun', 'mun': 'Mun',\n",
    "    'bohemia': 'Boh', 'boh': 'Boh',\n",
    "    'galicia': 'Gal', 'gal': 'Gal',\n",
    "    'brest': 'Bre', 'bre': 'Bre',\n",
    "    'picardy': 'Pic', 'pic': 'Pic',\n",
    "    'paris': 'Par', 'par': 'Par',\n",
    "    'burgundy': 'Bur', 'bur': 'Bur',\n",
    "    'gascony': 'Gas', 'gas': 'Gas',\n",
    "    'marseilles': 'Mar', 'mar': 'Mar',\n",
    "    'spain': 'Spa', 'spa': 'Spa',\n",
    "    'portugal': 'Por', 'por': 'Por',\n",
    "    'piedmont': 'Pie', 'pie': 'Pie',\n",
    "    'tyrolia': 'Tyr', 'tyr': 'Tyr',\n",
    "    'vienna': 'Vie', 'vie': 'Vie',\n",
    "    'trieste': 'Tri', 'tri': 'Tri',\n",
    "    'budapest': 'Bud', 'bud': 'Bud',\n",
    "    'rumania': 'Rum', 'rum': 'Rum',\n",
    "    'serbia': 'Ser', 'ser': 'Ser',\n",
    "    'venice': 'Ven', 'ven': 'Ven',\n",
    "    'rome': 'Rom', 'rom': 'Rom',\n",
    "    'apulia': 'Apu', 'apu': 'Apu',\n",
    "    'naples': 'Nap', 'nap': 'Nap',\n",
    "    'albania': 'Alb', 'alb': 'Alb',\n",
    "    'greece': 'Gre', 'gre': 'Gre',\n",
    "    'bulgaria': 'Bul', 'bul': 'Bul',\n",
    "    'constantinople': 'Con', 'con': 'Con',\n",
    "    'ankara': 'Ank', 'ank': 'Ank',\n",
    "    'armenia': 'Arm', 'arm': 'Arm',\n",
    "    'smyrna': 'Smy', 'smy': 'Smy',\n",
    "    'syria': 'Syr', 'syr': 'Syr',\n",
    "    'north africa': 'Naf', 'naf': 'Naf',\n",
    "    'tunis': 'Tun', 'tun': 'Tun'\n",
    "}\n",
    "\n",
    "# This is a helper set for the new tokenizer\n",
    "# We find all unique \"words\" in our map's keys\n",
    "# e.g., \"north\", \"atlantic\", \"ocean\", \"nao\", \"irish\", \"sea\", \"iri\", etc.\n",
    "TEXT_LOOKUP_WORDS = set()\n",
    "for key in NAME_TO_ABBREVIATION.keys():\n",
    "    TEXT_LOOKUP_WORDS.update(key.split())\n",
    "\n",
    "\n",
    "# --- 2. GAME STATE LOADING (Same as before) ---\n",
    "def create_game_state_dictionary(game_state_dir):\n",
    "    filename_pattern = re.compile(r\"DiplomacyGame(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "    game_states = defaultdict(lambda: defaultdict(dict))\n",
    "    print(f\"Starting to build game state dictionary from: {game_state_dir}\")\n",
    "    file_count = 0\n",
    "    if not os.path.exists(game_state_dir):\n",
    "        print(f\"Error: Directory not found: {game_state_dir}\")\n",
    "        return None\n",
    "    for filename in os.listdir(game_state_dir):\n",
    "        match = filename_pattern.match(filename)\n",
    "        if match:\n",
    "            game_id = int(match.group(1))\n",
    "            year = match.group(2)\n",
    "            season = match.group(3).lower() # <-- Store key as lowercase\n",
    "            file_path = os.path.join(game_state_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    game_states[game_id][year][season] = data\n",
    "                    file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load or parse {filename}. Error: {e}\")\n",
    "    print(f\"Successfully loaded and indexed {file_count} game state files.\")\n",
    "    return game_states\n",
    "\n",
    "# --- 3. NEW: TEXT-TO-STATE FEATURE EXTRACTION ---\n",
    "\n",
    "def find_mentioned_territories_abbreviations(message_text, lookup_map, lookup_words):\n",
    "    \"\"\"\n",
    "    Finds all unique territory ABBREVIATIONS (e.g., 'Par') mentioned\n",
    "    in a raw text message (e.g., \"I'm going to Paris\").\n",
    "    \"\"\"\n",
    "    # Tokenize the message by finding all words\n",
    "    words = re.findall(r'\\b\\w+\\b', message_text.lower())\n",
    "    \n",
    "    # Filter for words that are part of our map to speed things up\n",
    "    potential_words = [w for w in words if w in lookup_words]\n",
    "    \n",
    "    if not potential_words:\n",
    "        return set()\n",
    "\n",
    "    text = \" \".join(potential_words)\n",
    "    mentioned_abbreviations = set()\n",
    "    \n",
    "    # Check for multi-word keys first (e.g., \"north atlantic ocean\")\n",
    "    for name, abbr in lookup_map.items():\n",
    "        if \" \" in name and name in text:\n",
    "            mentioned_abbreviations.add(abbr)\n",
    "            \n",
    "    # Check for single-word keys (e.g., \"paris\" or \"par\")\n",
    "    for word in potential_words:\n",
    "        if word in lookup_map:\n",
    "            mentioned_abbreviations.add(lookup_map[word])\n",
    "            \n",
    "    return mentioned_abbreviations\n",
    "\n",
    "def extract_features_from_state(message_data, game_state):\n",
    "    \"\"\"\n",
    "    The master feature function. Creates all game-state and\n",
    "    text-to-state contradiction features.\n",
    "    \"\"\"\n",
    "    new_features = {}\n",
    "    \n",
    "    sender = message_data['speaker'].lower()\n",
    "    receiver = message_data['receiver'].lower()\n",
    "    message_text = message_data['text']\n",
    "    \n",
    "    # --- Get state data ---\n",
    "    territories_data = game_state.get('territories', {})\n",
    "    orders_data = game_state.get('orders', {})\n",
    "    sc_text = game_state.get('sc', '')\n",
    "    \n",
    "    # === 1. & 2. SC and Unit Counts (Same as before) ===\n",
    "    sc_pattern = re.compile(r\"\\s*(\\w+)\\s+(\\d+)\\s*\")\n",
    "    all_scores = {}\n",
    "    for match in sc_pattern.finditer(sc_text):\n",
    "        country = match.group(1).lower()\n",
    "        score = int(match.group(2))\n",
    "        all_scores[country] = score\n",
    "    new_features['sender_sc'] = all_scores.get(sender, 0)\n",
    "    new_features['receiver_sc'] = all_scores.get(receiver, 0)\n",
    "\n",
    "    sender_units_orders = orders_data.get(sender.upper(), {})\n",
    "    receiver_units_orders = orders_data.get(receiver.upper(), {})\n",
    "    new_features['sender_unit_count'] = len(sender_units_orders)\n",
    "    new_features['receiver_unit_count'] = len(receiver_units_orders)\n",
    "    \n",
    "    # === 3. Score Delta (Same as before) ===\n",
    "    new_features['score_delta'] = message_data['score_delta']\n",
    "    \n",
    "    # === 4. Adjacency (Same as before) ===\n",
    "    # This requires a global DIPLOMACY_MAP, which we'll define.\n",
    "    # We'll just hard-code this feature for now to avoid the map.\n",
    "    # is_adjacent = are_players_adjacent(sender, receiver, territories_data, DIPLOMACY_MAP)\n",
    "    # new_features['are_adjacent'] = 1 if is_adjacent else 0\n",
    "    \n",
    "    # === 5. NEW: Text-to-State & Contradiction Features ===\n",
    "    \n",
    "    # Find all territories mentioned in the message\n",
    "    mentioned_abbreviations = find_mentioned_territories_abbreviations(\n",
    "        message_text, NAME_TO_ABBREVIATION, TEXT_LOOKUP_WORDS\n",
    "    )\n",
    "    new_features['territory_mention_count'] = len(mentioned_abbreviations)\n",
    "\n",
    "    # Find all territories owned by sender and receiver\n",
    "    sender_owned_territories = set()\n",
    "    receiver_owned_territories = set()\n",
    "    for territory, owner in territories_data.items():\n",
    "        if owner.lower() == sender:\n",
    "            sender_owned_territories.add(territory.upper()) # Abbreviations are case-insensitive\n",
    "        elif owner.lower() == receiver:\n",
    "            receiver_owned_territories.add(territory.upper())\n",
    "            \n",
    "    # Find all territories involved in sender's orders\n",
    "    origin_territories = set(sender_units_orders.keys()) # e.g., {'Arm', 'Bul', 'Ank'}\n",
    "    target_territories = set()\n",
    "    for unit, order in sender_units_orders.items():\n",
    "        if 'to' in order:\n",
    "            target_territories.add(order['to'].upper()) # e.g., {'Sev', 'Rum'}\n",
    "\n",
    "    # Create the new features (as 1s or 0s)\n",
    "    new_features['mentions_own_territory'] = 0\n",
    "    new_features['mentions_receiver_territory'] = 0\n",
    "    new_features['mentions_origin_unit'] = 0\n",
    "    new_features['mentions_target_territory'] = 0\n",
    "    \n",
    "    for abbr in mentioned_abbreviations:\n",
    "        abbr_upper = abbr.upper()\n",
    "        if abbr_upper in sender_owned_territories:\n",
    "            new_features['mentions_own_territory'] = 1\n",
    "        if abbr_upper in receiver_owned_territories:\n",
    "            new_features['mentions_receiver_territory'] = 1\n",
    "        if abbr_upper in origin_territories:\n",
    "            new_features['mentions_origin_unit'] = 1\n",
    "        if abbr_upper in target_territories:\n",
    "            new_features['mentions_target_territory'] = 1\n",
    "            \n",
    "    return new_features\n",
    "\n",
    "# --- 4. DATA LOADING ---\n",
    "\n",
    "def load_and_flatten_data(filepath, game_state_lookup):\n",
    "    X_text_list = []\n",
    "    y_list = []\n",
    "    X_new_features_list = [] \n",
    "\n",
    "    with jsonlines.open(filepath, 'r') as reader:\n",
    "        for game in reader:\n",
    "            game_id = game['game_id']\n",
    "            years = game['years']\n",
    "            seasons = game['seasons']\n",
    "            speakers = game['speakers']\n",
    "            \n",
    "            for i in range(len(game['messages'])):\n",
    "                sender_label = game['sender_labels'][i]\n",
    "                if sender_label == \"NOANNOTATION\":\n",
    "                    continue\n",
    "                \n",
    "                label = 1 if sender_label == False else 0\n",
    "                message_text = game['messages'][i]\n",
    "                year = years[i]\n",
    "                season = seasons[i].lower() # <-- Use lowercase for lookup\n",
    "                \n",
    "                try:\n",
    "                    current_game_state = game_state_lookup[game_id][year][season]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "                message_data = {\n",
    "                    'text': message_text,\n",
    "                    'speaker': speakers[i],\n",
    "                    'receiver': game['receivers'][i],\n",
    "                    'score_delta': int(game['game_score_delta'][i])\n",
    "                }\n",
    "                \n",
    "                new_features = extract_features_from_state(message_data, current_game_state)\n",
    "                \n",
    "                X_text_list.append(message_text)\n",
    "                y_list.append(label)\n",
    "                X_new_features_list.append(new_features)\n",
    "\n",
    "    print(f\"Loaded and processed {len(X_text_list)} samples from {filepath}.\")\n",
    "    return X_text_list, X_new_features_list, np.array(y_list)\n",
    "\n",
    "# --- 5. MODEL EVALUATION ---\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred_probs): # y_pred_probs is an array of floats\n",
    "    \"\"\"Prints a detailed performance report\"\"\"\n",
    "    \n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Convert continuous probabilities (e.g., 0.73) into binary\n",
    "    # class labels (e.g., 1) using a 0.5 threshold.\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Now, y_true and y_pred are both binary (0s and 1s)\n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred) # Use the binary y_pred\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)'], zero_division=0))\n",
    "\n",
    "# --- 6. NEW: NEURAL NETWORK PIPELINE ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Set Paths ---\n",
    "    GAME_STATE_DIRECTORY = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\") \n",
    "\n",
    "    TRAIN_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\") \n",
    "    VAL_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"validation.jsonl\") \n",
    "    TEST_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"test.jsonl\") \n",
    "    \n",
    "    VOCAB_SIZE = 10000\n",
    "    MAX_LEN = 120\n",
    "    EMBEDDING_DIM = 64\n",
    "    LSTM_UNITS = 64\n",
    "\n",
    "    # --- Load Game State Dictionary ---\n",
    "    game_state_lookup = create_game_state_dictionary(GAME_STATE_DIRECTORY)\n",
    "    if not game_state_lookup:\n",
    "        print(\"Game state lookup dictionary is empty. Exiting.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # --- Load all data ---\n",
    "    print(\"Loading all datasets...\")\n",
    "    X_train_text, X_train_features_list, y_train = load_and_flatten_data(TRAIN_FILE, game_state_lookup)\n",
    "    X_val_text, X_val_features_list, y_val = load_and_flatten_data(VAL_FILE, game_state_lookup)\n",
    "    X_test_text, X_test_features_list, y_test = load_and_flatten_data(TEST_FILE, game_state_lookup)\n",
    "\n",
    "    if len(X_train_text) == 0:\n",
    "        print(\"CRITICAL ERROR: No training data was loaded.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 1. PREPARE TEXT INPUT (Tokenizer + Padding) ---\n",
    "    print(\"\\nPreprocessing Text Data...\")\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train_text) # Fit ONLY on training text\n",
    "    \n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val_text)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "    \n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    print(f\"Text data shape: {X_train_pad.shape}\")\n",
    "\n",
    "    # --- 2. PREPARE GAME STATE INPUT (DictVectorizer + Scaling) ---\n",
    "    print(\"Preprocessing Game State Data...\")\n",
    "    feature_vectorizer = DictVectorizer(sparse=False)  # Use dense arrays instead of sparse\n",
    "    X_train_features_numeric = feature_vectorizer.fit_transform(X_train_features_list)\n",
    "    X_val_features_numeric = feature_vectorizer.transform(X_val_features_list)\n",
    "    X_test_features_numeric = feature_vectorizer.transform(X_test_features_list)\n",
    "    \n",
    "    scaler = StandardScaler()  # Now works with dense arrays\n",
    "    X_train_features_scaled = scaler.fit_transform(X_train_features_numeric)\n",
    "    X_val_features_scaled = scaler.transform(X_val_features_numeric)\n",
    "    X_test_features_scaled = scaler.transform(X_test_features_numeric)\n",
    "    \n",
    "    num_game_features = X_train_features_scaled.shape[1]\n",
    "    print(f\"Game state feature names: {feature_vectorizer.feature_names_}\")\n",
    "    print(f\"Game state data shape: {X_train_features_scaled.shape}\")\n",
    "\n",
    "    # --- 3. DEFINE THE DUAL-INPUT MODEL ---\n",
    "    print(\"\\nBuilding Neural Network Model...\")\n",
    "    \n",
    "    # Input 1: Text Sequences\n",
    "    text_input_layer = Input(shape=(MAX_LEN,), name='text_input')\n",
    "    text_model = Embedding(input_dim=VOCAB_SIZE+1, output_dim=EMBEDDING_DIM, input_length=MAX_LEN)(text_input_layer)\n",
    "    text_model = SpatialDropout1D(0.2)(text_model) # Dropout for embeddings\n",
    "    text_model = LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2, name='lstm_layer')(text_model)\n",
    "    \n",
    "    # Input 2: Game State Features\n",
    "    game_state_input_layer = Input(shape=(num_game_features,), name='game_state_input')\n",
    "    \n",
    "    # Combine (Concatenate)\n",
    "    combined = concatenate([text_model, game_state_input_layer], name='concatenate')\n",
    "    \n",
    "    # Classifier Head\n",
    "    output = Dense(64, activation='relu', name='dense_1')(combined)\n",
    "    output = Dropout(0.5, name='dropout')(output)\n",
    "    output = Dense(1, activation='sigmoid', name='output_layer')(output)\n",
    "    \n",
    "    model = Model(inputs=[text_input_layer, game_state_input_layer], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # --- 4. HANDLE CLASS IMBALANCE ---\n",
    "    # This is CRITICAL for getting a good F1 score\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(f\"Class Weights (for balancing): {class_weight_dict}\")\n",
    "\n",
    "    # --- 5. TRAIN THE MODEL ---\n",
    "    print(\"\\nTraining the model...\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=3,         # Stop after 3 epochs with no improvement\n",
    "        restore_best_weights=True # Keep the best model, not the last\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_train_pad, X_train_features_scaled], # Pass inputs as a list\n",
    "        y_train,\n",
    "        epochs=15,\n",
    "        batch_size=64,\n",
    "        validation_data=([X_val_pad, X_val_features_scaled], y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight_dict # Apply the weights\n",
    "    )\n",
    "\n",
    "    # --- 6. EVALUATE THE FINAL MODEL ---\n",
    "    print(\"\\nEvaluating final model on test set...\")\n",
    "    y_pred_probs = model.predict([X_test_pad, X_test_features_scaled])\n",
    "    evaluate_model(\"Dual-Input Neural Network\", y_test, y_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b5e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build game state dictionary from: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "Successfully loaded and indexed 342 game state files.\n",
      "Loading all datasets...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 380\u001b[0m\n\u001b[0;32m    377\u001b[0m X_test_features_numeric \u001b[38;5;241m=\u001b[39m feature_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test_features_list)\n\u001b[0;32m    379\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler() \u001b[38;5;66;03m# NNs work best with scaled data\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m X_train_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train_features_numeric)\n\u001b[0;32m    381\u001b[0m X_val_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_val_features_numeric)\n\u001b[0;32m    382\u001b[0m X_test_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test_features_numeric)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:959\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n\u001b[1;32m--> 959\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    960\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot center sparse matrices: pass `with_mean=False` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    961\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead. See docstring for motivation and alternatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     sparse_constructor \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    964\u001b[0m         sparse\u001b[38;5;241m.\u001b[39mcsr_matrix \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39mcsc_matrix\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[0;32m    968\u001b[0m         \u001b[38;5;66;03m# First pass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. NEW: EXPERT KNOWLEDGE MAPPING ---\n",
    "\n",
    "# This map is our \"secret weapon\". It connects text (\"paris\") to game data (\"Par\").\n",
    "NAME_TO_ABBREVIATION = {\n",
    "    'north atlantic ocean': 'Nao', 'nao': 'Nao',\n",
    "    'irish sea': 'Iri', 'iri': 'Iri',\n",
    "    'english channel': 'Eng', 'eng': 'Eng', 'channel': 'Eng',\n",
    "    'mid-atlantic ocean': 'Mao', 'mao': 'Mao', 'mid atlantic': 'Mao',\n",
    "    'north sea': 'Nth', 'nth': 'Nth',\n",
    "    'norwegian sea': 'Nwg', 'nwg': 'Nwg',\n",
    "    'barents sea': 'Bar', 'bar': 'Bar',\n",
    "    'helgoland bight': 'Hel', 'hel': 'Hel',\n",
    "    'skagerrak': 'Ska', 'ska': 'Ska',\n",
    "    'baltic sea': 'Bal', 'bal': 'Bal',\n",
    "    'gulf of bothnia': 'Bot', 'bot': 'Bot',\n",
    "    'western mediterranean': 'Wes', 'wes': 'Wes',\n",
    "    'gulf of lyon': 'Lyo', 'lyo': 'Lyo',\n",
    "    'tyrrhenian sea': 'Tyn', 'tyn': 'Tyn',\n",
    "    'ionian sea': 'Ion', 'ion': 'Ion',\n",
    "    'adriatic sea': 'Adr', 'adr': 'Adr',\n",
    "    'aegean sea': 'Aeg', 'aeg': 'Aeg',\n",
    "    'eastern mediterranean': 'Eas', 'eas': 'Eas',\n",
    "    'black sea': 'Bla', 'bla': 'Bla',\n",
    "    \n",
    "    'clyde': 'Cly', 'cly': 'Cly',\n",
    "    'edinburgh': 'Edi', 'edi': 'Edi',\n",
    "    'liverpool': 'Lvp', 'lvp': 'Lvp',\n",
    "    'yorkshire': 'Yor', 'yor': 'Yor',\n",
    "    'wales': 'Wal', 'wal': 'Wal',\n",
    "    'london': 'Lon', 'lon': 'Lon',\n",
    "    'belgium': 'Bel', 'bel': 'Bel',\n",
    "    'holland': 'Hol', 'hol': 'Hol',\n",
    "    'denmark': 'Den', 'den': 'Den',\n",
    "    'sweden': 'Swe', 'swe': 'Swe',\n",
    "    'norway': 'Nwy', 'nwy': 'Nwy',\n",
    "    'finland': 'Fin', 'fin': 'Fin',\n",
    "    'st petersburg': 'Stp', 'stp': 'Stp',\n",
    "    'livonia': 'Lvn', 'lvn': 'Lvn',\n",
    "    'prussia': 'Pru', 'pru': 'Pru',\n",
    "    'kiel': 'Kie', 'kie': 'Kie',\n",
    "    'berlin': 'Ber', 'ber': 'Ber',\n",
    "    'silesia': 'Sil', 'sil': 'Sil',\n",
    "    'poland': 'War', 'war': 'War', 'warsaw': 'War',\n",
    "    'moscow': 'Mos', 'mos': 'Mos',\n",
    "    'ukraine': 'Ukr', 'ukr': 'Ukr',\n",
    "    'sevastopol': 'Sev', 'sev': 'Sev',\n",
    "    'ruhr': 'Ruh', 'ruh': 'Ruh',\n",
    "    'munich': 'Mun', 'mun': 'Mun',\n",
    "    'bohemia': 'Boh', 'boh': 'Boh',\n",
    "    'galicia': 'Gal', 'gal': 'Gal',\n",
    "    'brest': 'Bre', 'bre': 'Bre',\n",
    "    'picardy': 'Pic', 'pic': 'Pic',\n",
    "    'paris': 'Par', 'par': 'Par',\n",
    "    'burgundy': 'Bur', 'bur': 'Bur',\n",
    "    'gascony': 'Gas', 'gas': 'Gas',\n",
    "    'marseilles': 'Mar', 'mar': 'Mar',\n",
    "    'spain': 'Spa', 'spa': 'Spa',\n",
    "    'portugal': 'Por', 'por': 'Por',\n",
    "    'piedmont': 'Pie', 'pie': 'Pie',\n",
    "    'tyrolia': 'Tyr', 'tyr': 'Tyr',\n",
    "    'vienna': 'Vie', 'vie': 'Vie',\n",
    "    'trieste': 'Tri', 'tri': 'Tri',\n",
    "    'budapest': 'Bud', 'bud': 'Bud',\n",
    "    'rumania': 'Rum', 'rum': 'Rum',\n",
    "    'serbia': 'Ser', 'ser': 'Ser',\n",
    "    'venice': 'Ven', 'ven': 'Ven',\n",
    "    'rome': 'Rom', 'rom': 'Rom',\n",
    "    'apulia': 'Apu', 'apu': 'Apu',\n",
    "    'naples': 'Nap', 'nap': 'Nap',\n",
    "    'albania': 'Alb', 'alb': 'Alb',\n",
    "    'greece': 'Gre', 'gre': 'Gre',\n",
    "    'bulgaria': 'Bul', 'bul': 'Bul',\n",
    "    'constantinople': 'Con', 'con': 'Con',\n",
    "    'ankara': 'Ank', 'ank': 'Ank',\n",
    "    'armenia': 'Arm', 'arm': 'Arm',\n",
    "    'smyrna': 'Smy', 'smy': 'Smy',\n",
    "    'syria': 'Syr', 'syr': 'Syr',\n",
    "    'north africa': 'Naf', 'naf': 'Naf',\n",
    "    'tunis': 'Tun', 'tun': 'Tun'\n",
    "}\n",
    "\n",
    "# This is a helper set for the new tokenizer\n",
    "# We find all unique \"words\" in our map's keys\n",
    "# e.g., \"north\", \"atlantic\", \"ocean\", \"nao\", \"irish\", \"sea\", \"iri\", etc.\n",
    "TEXT_LOOKUP_WORDS = set()\n",
    "for key in NAME_TO_ABBREVIATION.keys():\n",
    "    TEXT_LOOKUP_WORDS.update(key.split())\n",
    "\n",
    "\n",
    "# --- 2. GAME STATE LOADING (Same as before) ---\n",
    "def create_game_state_dictionary(game_state_dir):\n",
    "    filename_pattern = re.compile(r\"DiplomacyGame(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "    game_states = defaultdict(lambda: defaultdict(dict))\n",
    "    print(f\"Starting to build game state dictionary from: {game_state_dir}\")\n",
    "    file_count = 0\n",
    "    if not os.path.exists(game_state_dir):\n",
    "        print(f\"Error: Directory not found: {game_state_dir}\")\n",
    "        return None\n",
    "    for filename in os.listdir(game_state_dir):\n",
    "        match = filename_pattern.match(filename)\n",
    "        if match:\n",
    "            game_id = int(match.group(1))\n",
    "            year = match.group(2)\n",
    "            season = match.group(3).lower() # <-- Store key as lowercase\n",
    "            file_path = os.path.join(game_state_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    game_states[game_id][year][season] = data\n",
    "                    file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load or parse {filename}. Error: {e}\")\n",
    "    print(f\"Successfully loaded and indexed {file_count} game state files.\")\n",
    "    return game_states\n",
    "\n",
    "# --- 3. NEW: TEXT-TO-STATE FEATURE EXTRACTION ---\n",
    "\n",
    "def find_mentioned_territories_abbreviations(message_text, lookup_map, lookup_words):\n",
    "    \"\"\"\n",
    "    Finds all unique territory ABBREVIATIONS (e.g., 'Par') mentioned\n",
    "    in a raw text message (e.g., \"I'm going to Paris\").\n",
    "    \"\"\"\n",
    "    # Tokenize the message by finding all words\n",
    "    words = re.findall(r'\\b\\w+\\b', message_text.lower())\n",
    "    \n",
    "    # Filter for words that are part of our map to speed things up\n",
    "    potential_words = [w for w in words if w in lookup_words]\n",
    "    \n",
    "    if not potential_words:\n",
    "        return set()\n",
    "\n",
    "    text = \" \".join(potential_words)\n",
    "    mentioned_abbreviations = set()\n",
    "    \n",
    "    # Check for multi-word keys first (e.g., \"north atlantic ocean\")\n",
    "    for name, abbr in lookup_map.items():\n",
    "        if \" \" in name and name in text:\n",
    "            mentioned_abbreviations.add(abbr)\n",
    "            \n",
    "    # Check for single-word keys (e.g., \"paris\" or \"par\")\n",
    "    for word in potential_words:\n",
    "        if word in lookup_map:\n",
    "            mentioned_abbreviations.add(lookup_map[word])\n",
    "            \n",
    "    return mentioned_abbreviations\n",
    "\n",
    "def extract_features_from_state(message_data, game_state):\n",
    "    \"\"\"\n",
    "    The master feature function. Creates all game-state and\n",
    "    text-to-state contradiction features.\n",
    "    \"\"\"\n",
    "    new_features = {}\n",
    "    \n",
    "    sender = message_data['speaker'].lower()\n",
    "    receiver = message_data['receiver'].lower()\n",
    "    message_text = message_data['text']\n",
    "    \n",
    "    # --- Get state data ---\n",
    "    territories_data = game_state.get('territories', {})\n",
    "    orders_data = game_state.get('orders', {})\n",
    "    sc_text = game_state.get('sc', '')\n",
    "    \n",
    "    # === 1. & 2. SC and Unit Counts (Same as before) ===\n",
    "    sc_pattern = re.compile(r\"\\s*(\\w+)\\s+(\\d+)\\s*\")\n",
    "    all_scores = {}\n",
    "    for match in sc_pattern.finditer(sc_text):\n",
    "        country = match.group(1).lower()\n",
    "        score = int(match.group(2))\n",
    "        all_scores[country] = score\n",
    "    new_features['sender_sc'] = all_scores.get(sender, 0)\n",
    "    new_features['receiver_sc'] = all_scores.get(receiver, 0)\n",
    "\n",
    "    sender_units_orders = orders_data.get(sender.upper(), {})\n",
    "    receiver_units_orders = orders_data.get(receiver.upper(), {})\n",
    "    new_features['sender_unit_count'] = len(sender_units_orders)\n",
    "    new_features['receiver_unit_count'] = len(receiver_units_orders)\n",
    "    \n",
    "    # === 3. Score Delta (Same as before) ===\n",
    "    new_features['score_delta'] = message_data['score_delta']\n",
    "    \n",
    "    # === 4. Adjacency (Same as before) ===\n",
    "    # This requires a global DIPLOMACY_MAP, which we'll define.\n",
    "    # We'll just hard-code this feature for now to avoid the map.\n",
    "    # is_adjacent = are_players_adjacent(sender, receiver, territories_data, DIPLOMACY_MAP)\n",
    "    # new_features['are_adjacent'] = 1 if is_adjacent else 0\n",
    "    \n",
    "    # === 5. NEW: Text-to-State & Contradiction Features ===\n",
    "    \n",
    "    # Find all territories mentioned in the message\n",
    "    mentioned_abbreviations = find_mentioned_territories_abbreviations(\n",
    "        message_text, NAME_TO_ABBREVIATION, TEXT_LOOKUP_WORDS\n",
    "    )\n",
    "    new_features['territory_mention_count'] = len(mentioned_abbreviations)\n",
    "\n",
    "    # Find all territories owned by sender and receiver\n",
    "    sender_owned_territories = set()\n",
    "    receiver_owned_territories = set()\n",
    "    for territory, owner in territories_data.items():\n",
    "        if owner.lower() == sender:\n",
    "            sender_owned_territories.add(territory.upper()) # Abbreviations are case-insensitive\n",
    "        elif owner.lower() == receiver:\n",
    "            receiver_owned_territories.add(territory.upper())\n",
    "            \n",
    "    # Find all territories involved in sender's orders\n",
    "    origin_territories = set(sender_units_orders.keys()) # e.g., {'Arm', 'Bul', 'Ank'}\n",
    "    target_territories = set()\n",
    "    for unit, order in sender_units_orders.items():\n",
    "        if 'to' in order:\n",
    "            target_territories.add(order['to'].upper()) # e.g., {'Sev', 'Rum'}\n",
    "\n",
    "    # Create the new features (as 1s or 0s)\n",
    "    new_features['mentions_own_territory'] = 0\n",
    "    new_features['mentions_receiver_territory'] = 0\n",
    "    new_features['mentions_origin_unit'] = 0\n",
    "    new_features['mentions_target_territory'] = 0\n",
    "    \n",
    "    for abbr in mentioned_abbreviations:\n",
    "        abbr_upper = abbr.upper()\n",
    "        if abbr_upper in sender_owned_territories:\n",
    "            new_features['mentions_own_territory'] = 1\n",
    "        if abbr_upper in receiver_owned_territories:\n",
    "            new_features['mentions_receiver_territory'] = 1\n",
    "        if abbr_upper in origin_territories:\n",
    "            new_features['mentions_origin_unit'] = 1\n",
    "        if abbr_upper in target_territories:\n",
    "            new_features['mentions_target_territory'] = 1\n",
    "            \n",
    "    return new_features\n",
    "\n",
    "# --- 4. DATA LOADING ---\n",
    "\n",
    "def load_and_flatten_data(filepath, game_state_lookup):\n",
    "    X_text_list = []\n",
    "    y_list = []\n",
    "    X_new_features_list = [] \n",
    "\n",
    "    with jsonlines.open(filepath, 'r') as reader:\n",
    "        for game in reader:\n",
    "            game_id = game['game_id']\n",
    "            years = game['years']\n",
    "            seasons = game['seasons']\n",
    "            speakers = game['speakers']\n",
    "            \n",
    "            for i in range(len(game['messages'])):\n",
    "                sender_label = game['sender_labels'][i]\n",
    "                if sender_label == \"NOANNOTATION\":\n",
    "                    continue\n",
    "                \n",
    "                label = 1 if sender_label == False else 0\n",
    "                message_text = game['messages'][i]\n",
    "                year = years[i]\n",
    "                season = seasons[i].lower() # <-- Use lowercase for lookup\n",
    "                \n",
    "                try:\n",
    "                    current_game_state = game_state_lookup[game_id][year][season]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "                message_data = {\n",
    "                    'text': message_text,\n",
    "                    'speaker': speakers[i],\n",
    "                    'receiver': game['receivers'][i],\n",
    "                    'score_delta': int(game['game_score_delta'][i])\n",
    "                }\n",
    "                \n",
    "                new_features = extract_features_from_state(message_data, current_game_state)\n",
    "                \n",
    "                X_text_list.append(message_text)\n",
    "                y_list.append(label)\n",
    "                X_new_features_list.append(new_features)\n",
    "\n",
    "    print(f\"Loaded and processed {len(X_text_list)} samples from {filepath}.\")\n",
    "    return X_text_list, X_new_features_list, np.array(y_list)\n",
    "\n",
    "# --- 5. MODEL EVALUATION ---\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)'], zero_division=0))\n",
    "\n",
    "# --- 6. NEW: NEURAL NETWORK PIPELINE ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Set Paths ---\n",
    "    GAME_STATE_DIRECTORY = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\") \n",
    "\n",
    "    TRAIN_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\") \n",
    "    VAL_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"validation.jsonl\") \n",
    "    TEST_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"test.jsonl\") \n",
    "    \n",
    "    VOCAB_SIZE = 10000\n",
    "    MAX_LEN = 120\n",
    "    EMBEDDING_DIM = 64\n",
    "    LSTM_UNITS = 64\n",
    "\n",
    "    # --- Load Game State Dictionary ---\n",
    "    game_state_lookup = create_game_state_dictionary(GAME_STATE_DIRECTORY)\n",
    "    if not game_state_lookup:\n",
    "        print(\"Game state lookup dictionary is empty. Exiting.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # --- Load all data ---\n",
    "    print(\"Loading all datasets...\")\n",
    "    X_train_text, X_train_features_list, y_train = load_and_flatten_data(TRAIN_FILE, game_state_lookup)\n",
    "    X_val_text, X_val_features_list, y_val = load_and_flatten_data(VAL_FILE, game_state_lookup)\n",
    "    X_test_text, X_test_features_list, y_test = load_and_flatten_data(TEST_FILE, game_state_lookup)\n",
    "\n",
    "    if len(X_train_text) == 0:\n",
    "        print(\"CRITICAL ERROR: No training data was loaded.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 1. PREPARE TEXT INPUT (Tokenizer + Padding) ---\n",
    "    print(\"\\nPreprocessing Text Data...\")\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train_text) # Fit ONLY on training text\n",
    "    \n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val_text)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "    \n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    print(f\"Text data shape: {X_train_pad.shape}\")\n",
    "\n",
    "    # --- 2. PREPARE GAME STATE INPUT (DictVectorizer + Scaling) ---\n",
    "    print(\"Preprocessing Game State Data...\")\n",
    "    feature_vectorizer = DictVectorizer(sparse=False)  # Use dense arrays instead of sparse\n",
    "    X_train_features_numeric = feature_vectorizer.fit_transform(X_train_features_list)\n",
    "    X_val_features_numeric = feature_vectorizer.transform(X_val_features_list)\n",
    "    X_test_features_numeric = feature_vectorizer.transform(X_test_features_list)\n",
    "    \n",
    "    scaler = StandardScaler()  # Now works with dense arrays\n",
    "    X_train_features_scaled = scaler.fit_transform(X_train_features_numeric)\n",
    "    X_val_features_scaled = scaler.transform(X_val_features_numeric)\n",
    "    X_test_features_scaled = scaler.transform(X_test_features_numeric)\n",
    "    \n",
    "    num_game_features = X_train_features_scaled.shape[1]\n",
    "    print(f\"Game state feature names: {feature_vectorizer.feature_names_}\")\n",
    "    print(f\"Game state data shape: {X_train_features_scaled.shape}\")\n",
    "\n",
    "    # --- 3. DEFINE THE DUAL-INPUT MODEL ---\n",
    "    print(\"\\nBuilding Neural Network Model...\")\n",
    "    \n",
    "    # Input 1: Text Sequences\n",
    "    text_input_layer = Input(shape=(MAX_LEN,), name='text_input')\n",
    "    text_model = Embedding(input_dim=VOCAB_SIZE+1, output_dim=EMBEDDING_DIM, input_length=MAX_LEN)(text_input_layer)\n",
    "    text_model = SpatialDropout1D(0.2)(text_model) # Dropout for embeddings\n",
    "    text_model = LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2, name='lstm_layer')(text_model)\n",
    "    \n",
    "    # Input 2: Game State Features\n",
    "    game_state_input_layer = Input(shape=(num_game_features,), name='game_state_input')\n",
    "    \n",
    "    # Combine (Concatenate)\n",
    "    combined = concatenate([text_model, game_state_input_layer], name='concatenate')\n",
    "    \n",
    "    # Classifier Head\n",
    "    output = Dense(64, activation='relu', name='dense_1')(combined)\n",
    "    output = Dropout(0.5, name='dropout')(output)\n",
    "    output = Dense(1, activation='sigmoid', name='output_layer')(output)\n",
    "    \n",
    "    model = Model(inputs=[text_input_layer, game_state_input_layer], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # --- 4. HANDLE CLASS IMBALANCE ---\n",
    "    # This is CRITICAL for getting a good F1 score\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(f\"Class Weights (for balancing): {class_weight_dict}\")\n",
    "\n",
    "    # --- 5. TRAIN THE MODEL ---\n",
    "    print(\"\\nTraining the model...\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=3,         # Stop after 3 epochs with no improvement\n",
    "        restore_best_weights=True # Keep the best model, not the last\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_train_pad, X_train_features_scaled], # Pass inputs as a list\n",
    "        y_train,\n",
    "        epochs=15,\n",
    "        batch_size=64,\n",
    "        validation_data=([X_val_pad, X_val_features_scaled], y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight_dict # Apply the weights\n",
    "    )\n",
    "\n",
    "    # --- 6. EVALUATE THE FINAL MODEL ---\n",
    "    print(\"\\nEvaluating final model on test set...\")\n",
    "    y_pred_probs = model.predict([X_test_pad, X_test_features_scaled])\n",
    "    evaluate_model(\"Dual-Input Neural Network\", y_test, y_pred_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
