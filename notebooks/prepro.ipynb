{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8571b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "DATA_PATH = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25054da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diplomacy_map():\n",
    "    \"\"\"\n",
    "    Returns a dictionary where each key is a territory and the value\n",
    "    is a list of territories it borders.\n",
    "    This is the static map of the game.\n",
    "    \"\"\"\n",
    "    # This map defines all territory adjacencies in Diplomacy\n",
    "    return {\n",
    "        'Cly': ['Edi', 'Lvp', 'Nao', 'Nwg'],\n",
    "        'Edi': ['Cly', 'Lvp', 'Nth', 'Nwg', 'Yor'],\n",
    "        'Lvp': ['Cly', 'Edi', 'Iri', 'Nao', 'Wal', 'Yor'],\n",
    "        'Yor': ['Edi', 'Lvp', 'Lon', 'Nth', 'Wal'],\n",
    "        'Wal': ['Lvp', 'Yor', 'Lon', 'Eng', 'Iri'],\n",
    "        'Lon': ['Yor', 'Wal', 'Eng', 'Nth'],\n",
    "        'Nwy': ['Bar', 'Nwg', 'Nth', 'Ska', 'Stp', 'Swe'],\n",
    "        'Swe': ['Nwy', 'Ska', 'Den', 'Bal', 'Bot', 'Fin'],\n",
    "        'Den': ['Swe', 'Ska', 'Nth', 'Hel', 'Kie', 'Bal'],\n",
    "        'Kie': ['Den', 'Hel', 'Hol', 'Ruh', 'Mun', 'Ber', 'Bal'],\n",
    "        'Ber': ['Kie', 'Bal', 'Pru', 'Sil', 'Mun'],\n",
    "        'Pru': ['Ber', 'Bal', 'Lvn', 'War', 'Sil'],\n",
    "        'Stp': ['Nwy', 'Bar', 'Mos', 'Lvn', 'Bot', 'Fin'],\n",
    "        'Fin': ['Nwy', 'Stp', 'Bot', 'Swe'],\n",
    "        'Lvn': ['Stp', 'Mos', 'War', 'Pru', 'Bal', 'Bot'],\n",
    "        'Mos': ['Stp', 'Lvn', 'War', 'Ukr', 'Sev'],\n",
    "        'War': ['Pru', 'Lvn', 'Mos', 'Ukr', 'Gal', 'Sil'],\n",
    "        'Sil': ['Ber', 'Mun', 'Boh', 'Gal', 'War', 'Pru'],\n",
    "        'Hol': ['Kie', 'Hel', 'Nth', 'Eng', 'Bel', 'Ruh'],\n",
    "        'Bel': ['Hol', 'Eng', 'Pic', 'Bur', 'Ruh'],\n",
    "        'Ruh': ['Kie', 'Hol', 'Bel', 'Bur', 'Mun'],\n",
    "        'Mun': ['Kie', 'Ruh', 'Bur', 'Tyr', 'Boh', 'Sil', 'Ber'],\n",
    "        'Boh': ['Mun', 'Tyr', 'Vie', 'Gal', 'Sil'],\n",
    "        'Gal': ['Boh', 'Vie', 'Bud', 'Rum', 'Ukr', 'War', 'Sil'],\n",
    "        'Ukr': ['Mos', 'War', 'Gal', 'Rum', 'Sev'],\n",
    "        'Sev': ['Mos', 'Ukr', 'Rum', 'Bla', 'Arm'],\n",
    "        'Bre': ['Eng', 'Mao', 'Pic', 'Par', 'Gas'],\n",
    "        'Pic': ['Eng', 'Bel', 'Bur', 'Par', 'Bre'],\n",
    "        'Par': ['Pic', 'Bur', 'Gas', 'Bre'],\n",
    "        'Bur': ['Bel', 'Ruh', 'Mun', 'Mar', 'Gas', 'Par', 'Pic'],\n",
    "        'Gas': ['Bre', 'Par', 'Bur', 'Mar', 'Spa', 'Mao'],\n",
    "        'Mar': ['Bur', 'Gas', 'Spa', 'Lyo'],\n",
    "        'Spa': ['Gas', 'Mar', 'Lyo', 'Wes', 'Mao', 'Por'],\n",
    "        'Por': ['Spa', 'Mao'],\n",
    "        'Nao': ['Cly', 'Lvp', 'Iri', 'Mao'],\n",
    "        'Iri': ['Lvp', 'Wal', 'Eng', 'Mao', 'Nao'],\n",
    "        'Eng': ['Wal', 'Lon', 'Nth', 'Hol', 'Bel', 'Pic', 'Bre', 'Mao', 'Iri'],\n",
    "        'Mao': ['Bre', 'Gas', 'Spa', 'Por', 'Nao', 'Iri', 'Eng', 'Wes'],\n",
    "        'Nth': ['Edi', 'Yor', 'Lon', 'Eng', 'Hol', 'Hel', 'Den', 'Ska', 'Nwy', 'Nwg'],\n",
    "        'Nwg': ['Cly', 'Edi', 'Nth', 'Nwy', 'Bar'],\n",
    "        'Bar': ['Nwy', 'Nwg', 'Stp'],\n",
    "        'Hel': ['Den', 'Kie', 'Hol', 'Nth'],\n",
    "        'Ska': ['Nwy', 'Swe', 'Den', 'Nth'],\n",
    "        'Bal': ['Swe', 'Den', 'Kie', 'Ber', 'Pru', 'Lvn', 'Bot'],\n",
    "        'Bot': ['Swe', 'Fin', 'Stp', 'Lvn', 'Bal'],\n",
    "        'Wes': ['Spa', 'Mar', 'Lyo', 'Tyn', 'Mao'],\n",
    "        'Lyo': ['Mar', 'Spa', 'Wes', 'Tyn', 'Pie', 'Bur'],\n",
    "        'Tyn': ['Wes', 'Lyo', 'Ion', 'Tun', 'Rom'],\n",
    "        'Ion': ['Tyn', 'Rom', 'Nap', 'Apu', 'Adr', 'Alb', 'Gre', 'Aeg', 'Eas'],\n",
    "        'Adr': ['Ven', 'Tri', 'Alb', 'Ion', 'Apu'],\n",
    "        'Aeg': ['Gre', 'Ion', 'Eas', 'Smy', 'Con', 'Bul'],\n",
    "        'Eas': ['Ion', 'Aeg', 'Smy', 'Syr'],\n",
    "        'Bla': ['Sev', 'Arm', 'Ank', 'Con', 'Bul', 'Rum'],\n",
    "        'Pie': ['Mar', 'Lyo', 'Tyr', 'Ven'],\n",
    "        'Ven': ['Pie', 'Tyr', 'Tri', 'Adr', 'Apu', 'Rom'],\n",
    "        'Rom': ['Ven', 'Apu', 'Nap', 'Tyn', 'Lyo', 'Pie'],\n",
    "        'Nap': ['Rom', 'Apu', 'Ion'],\n",
    "        'Apu': ['Ven', 'Adr', 'Ion', 'Nap', 'Rom'],\n",
    "        'Tyr': ['Mun', 'Boh', 'Vie', 'Tri', 'Ven', 'Pie', 'Bur'],\n",
    "        'Vie': ['Boh', 'Gal', 'Bud', 'Tri', 'Tyr'],\n",
    "        'Tri': ['Vie', 'Bud', 'Ser', 'Alb', 'Adr', 'Ven', 'Tyr'],\n",
    "        'Bud': ['Vie', 'Gal', 'Rum', 'Ser', 'Tri'],\n",
    "        'Rum': ['Gal', 'Ukr', 'Sev', 'Bla', 'Bul', 'Ser', 'Bud'],\n",
    "        'Ser': ['Bud', 'Rum', 'Bul', 'Gre', 'Alb', 'Tri'],\n",
    "        'Alb': ['Tri', 'Ser', 'Gre', 'Ion', 'Adr'],\n",
    "        'Gre': ['Ser', 'Alb', 'Ion', 'Aeg', 'Bul'],\n",
    "        'Bul': ['Rum', 'Bla', 'Con', 'Aeg', 'Gre', 'Ser'],\n",
    "        'Con': ['Bul', 'Aeg', 'Smy', 'Ank', 'Bla'],\n",
    "        'Ank': ['Con', 'Bla', 'Arm', 'Smy'],\n",
    "        'Arm': ['Sev', 'Bla', 'Ank', 'Smy', 'Syr'],\n",
    "        'Smy': ['Con', 'Ank', 'Arm', 'Syr', 'Eas', 'Aeg'],\n",
    "        'Syr': ['Arm', 'Smy', 'Eas'],\n",
    "        'Tun': ['Tyn', 'Ion', 'Naf'],\n",
    "        'Naf': ['Tun']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82181b34",
   "metadata": {},
   "source": [
    "# ğŸ—ºï¸ Diplomacy Harita Sistemi - AÃ§Ä±klama\n",
    "\n",
    "## Bu BÃ¶lge KodlarÄ± Ne Anlama Geliyor?\n",
    "\n",
    "Diplomacy, **I. DÃ¼nya SavaÅŸÄ± Ã¶ncesi Avrupa haritasÄ±** Ã¼zerinde oynanan bir strateji oyunu.\n",
    "\n",
    "### **BÃ¶lge KÄ±saltmalarÄ± (3 Harfli Kodlar):**\n",
    "\n",
    "#### **ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ Ä°ngiltere (England):**\n",
    "- `Cly` = **Clyde** (Ä°skoÃ§ya)\n",
    "- `Edi` = **Edinburgh** (Ä°skoÃ§ya)\n",
    "- `Lvp` = **Liverpool** (Ä°ngiltere)\n",
    "- `Yor` = **Yorkshire** (Ä°ngiltere)\n",
    "- `Wal` = **Wales** (Galler)\n",
    "- `Lon` = **London** (Londra)\n",
    "\n",
    "#### **ğŸ‡«ğŸ‡· Fransa (France):**\n",
    "- `Bre` = **Brest** (Brittany)\n",
    "- `Pic` = **Picardy** (Kuzey Fransa)\n",
    "- `Par` = **Paris** (BaÅŸkent)\n",
    "- `Bur` = **Burgundy** (DoÄŸu Fransa)\n",
    "- `Gas` = **Gascony** (GÃ¼neybatÄ±)\n",
    "- `Mar` = **Marseilles** (Akdeniz kÄ±yÄ±sÄ±)\n",
    "\n",
    "#### **ğŸ‡©ğŸ‡ª Almanya (Germany):**\n",
    "- `Kie` = **Kiel** (Kuzey liman)\n",
    "- `Ber` = **Berlin** (BaÅŸkent)\n",
    "- `Mun` = **Munich** (GÃ¼ney)\n",
    "- `Ruh` = **Ruhr** (Sanayi bÃ¶lgesi)\n",
    "- `Sil` = **Silesia** (DoÄŸu)\n",
    "- `Pru` = **Prussia** (DoÄŸu Prusya)\n",
    "\n",
    "#### **ğŸ‡·ğŸ‡º Rusya (Russia):**\n",
    "- `Stp` = **St Petersburg** (BaÅŸkent)\n",
    "- `Mos` = **Moscow** (Moskova)\n",
    "- `War` = **Warsaw** (VarÅŸova/Polonya)\n",
    "- `Ukr` = **Ukraine** (Ukrayna)\n",
    "- `Sev` = **Sevastopol** (KÄ±rÄ±m)\n",
    "- `Lvn` = **Livonia** (BaltÄ±k)\n",
    "- `Fin` = **Finland** (Finlandiya)\n",
    "\n",
    "#### **ğŸ‡®ğŸ‡¹ Ä°talya (Italy):**\n",
    "- `Ven` = **Venice** (Venedik)\n",
    "- `Rom` = **Rome** (Roma)\n",
    "- `Nap` = **Naples** (Napoli)\n",
    "- `Pie` = **Piedmont** (Kuzey)\n",
    "- `Apu` = **Apulia** (GÃ¼neydoÄŸu)\n",
    "\n",
    "#### **ğŸ‡¦ğŸ‡¹ Avusturya-Macaristan (Austria-Hungary):**\n",
    "- `Vie` = **Vienna** (Viyana)\n",
    "- `Bud` = **Budapest** (BudapeÅŸte)\n",
    "- `Tri` = **Trieste** (Adriyatik limanÄ±)\n",
    "- `Boh` = **Bohemia** (Ã‡ekya)\n",
    "- `Gal` = **Galicia** (Polonya/Ukrayna sÄ±nÄ±rÄ±)\n",
    "- `Tyr` = **Tyrolia** (Avusturya Alpleri)\n",
    "\n",
    "#### **ğŸ‡¹ğŸ‡· TÃ¼rkiye (Turkey):**\n",
    "- `Con` = **Constantinople** (Ä°stanbul)\n",
    "- `Ank` = **Ankara** (Ankara)\n",
    "- `Smy` = **Smyrna** (Ä°zmir)\n",
    "- `Arm` = **Armenia** (Ermenistan)\n",
    "- `Syr` = **Syria** (Suriye)\n",
    "\n",
    "#### **ğŸŒŠ Denizler ve Okyanus BÃ¶lgeleri:**\n",
    "- `Nao` = **North Atlantic Ocean** (Kuzey Atlantik)\n",
    "- `Nth` = **North Sea** (Kuzey Denizi)\n",
    "- `Eng` = **English Channel** (ManÅŸ Denizi)\n",
    "- `Iri` = **Irish Sea** (Ä°rlanda Denizi)\n",
    "- `Mao` = **Mid-Atlantic Ocean** (Orta Atlantik)\n",
    "- `Bar` = **Barents Sea** (Kuzey Rusya)\n",
    "- `Bal` = **Baltic Sea** (BaltÄ±k Denizi)\n",
    "- `Bot` = **Gulf of Bothnia** (Ä°sveÃ§-Finlandiya arasÄ±)\n",
    "- `Ska` = **Skagerrak** (Danimarka boÄŸazÄ±)\n",
    "- `Hel` = **Helgoland Bight** (Almanya kÄ±yÄ±sÄ±)\n",
    "- `Nwg` = **Norwegian Sea** (NorveÃ§ Denizi)\n",
    "- `Wes` = **Western Mediterranean** (BatÄ± Akdeniz)\n",
    "- `Lyo` = **Gulf of Lyon** (Lyon KÃ¶rfezi)\n",
    "- `Tyn` = **Tyrrhenian Sea** (Ä°talya batÄ±sÄ±)\n",
    "- `Ion` = **Ionian Sea** (Yunanistan gÃ¼neyi)\n",
    "- `Adr` = **Adriatic Sea** (Adriyatik)\n",
    "- `Aeg` = **Aegean Sea** (Ege Denizi)\n",
    "- `Eas` = **Eastern Mediterranean** (DoÄŸu Akdeniz)\n",
    "- `Bla` = **Black Sea** (Karadeniz)\n",
    "\n",
    "#### **ğŸŒ TarafsÄ±z BÃ¶lgeler:**\n",
    "- `Bel` = **Belgium** (BelÃ§ika)\n",
    "- `Hol` = **Holland** (Hollanda)\n",
    "- `Den` = **Denmark** (Danimarka)\n",
    "- `Swe` = **Sweden** (Ä°sveÃ§)\n",
    "- `Nwy` = **Norway** (NorveÃ§)\n",
    "- `Spa` = **Spain** (Ä°spanya)\n",
    "- `Por` = **Portugal** (Portekiz)\n",
    "- `Tun` = **Tunis** (Tunus)\n",
    "- `Naf` = **North Africa** (Kuzey Afrika)\n",
    "- `Ser` = **Serbia** (SÄ±rbistan)\n",
    "- `Bul` = **Bulgaria** (Bulgaristan)\n",
    "- `Rum` = **Rumania** (Romanya)\n",
    "- `Gre` = **Greece** (Yunanistan)\n",
    "- `Alb` = **Albania** (Arnavutluk)\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Bu Harita Neden Ã–nemli?**\n",
    "\n",
    "### **1. KomÅŸuluk Ä°liÅŸkisi (Adjacency)**\n",
    "```python\n",
    "'Par': ['Pic', 'Bur', 'Gas', 'Bre']\n",
    "```\n",
    "**AnlamÄ±:** Paris, sadece Picardy, Burgundy, Gascony ve Brest'e komÅŸu. Berlin'e komÅŸu deÄŸil!\n",
    "\n",
    "### **2. Yalan Tespiti Ä°Ã§in KullanÄ±mÄ±:**\n",
    "\n",
    "**Ã–rnek Senaryo:**\n",
    "```\n",
    "Mesaj: \"France: I will support you in Berlin!\"\n",
    "```\n",
    "\n",
    "**Kontrol:**\n",
    "1. France'Ä±n bÃ¶lgeleri: `['Par', 'Bre', 'Mar']` (Ã¶rnek)\n",
    "2. Berlin'in komÅŸularÄ±: `['Kie', 'Bal', 'Pru', 'Sil', 'Mun']`\n",
    "3. **KesiÅŸim var mÄ±?** â†’ HAYIR!\n",
    "4. **SonuÃ§:** France, Berlin'e komÅŸu deÄŸil â†’ Fiziksel olarak destekleyemez â†’ **Muhtemelen yalan!**\n",
    "\n",
    "### **3. Stratejik Ã–zellik Ã‡Ä±karÄ±mÄ±:**\n",
    "\n",
    "```python\n",
    "def are_players_adjacent(sender, receiver, territories, map):\n",
    "    # EÄŸer oyuncular komÅŸu deÄŸilse:\n",
    "    #   â†’ DoÄŸrudan askerÃ® iÅŸbirliÄŸi YOK\n",
    "    #   â†’ Mesajlar daha Ã§ok diplomatik/aldatÄ±cÄ± olabilir\n",
    "    \n",
    "    # EÄŸer komÅŸularsa:\n",
    "    #   â†’ AnlaÅŸmalar somut/gerÃ§ek olma ihtimali daha yÃ¼ksek\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Harita Ä°statistikleri:**\n",
    "- **Toplam BÃ¶lge:** 75 (Kara + Deniz)\n",
    "- **Kara BÃ¶lgeleri:** ~50\n",
    "- **Deniz BÃ¶lgeleri:** ~25\n",
    "- **Ortalama KomÅŸu SayÄ±sÄ±:** ~5-6 bÃ¶lge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f594dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ—ºï¸  DÄ°PLOMASÄ° HARÄ°TASI ANALÄ°ZÄ°\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Genel Ä°statistikler:\n",
      "   Toplam bÃ¶lge sayÄ±sÄ±: 74\n",
      "   Ortalama komÅŸu sayÄ±sÄ±: 5.3\n",
      "   En Ã§ok komÅŸusu olan: Nth (10 komÅŸu)\n",
      "   En az komÅŸusu olan: Naf (1 komÅŸu)\n",
      "\n",
      "â­ Stratejik Ã–nemli BÃ¶lgeler (6+ komÅŸu):\n",
      "   Nth: 10 komÅŸu â†’ Edi, Yor, Lon, Eng, Hol...\n",
      "   Eng: 9 komÅŸu â†’ Wal, Lon, Nth, Hol, Bel...\n",
      "   Ion: 9 komÅŸu â†’ Tyn, Rom, Nap, Apu, Adr...\n",
      "   Mao: 8 komÅŸu â†’ Bre, Gas, Spa, Por, Nao...\n",
      "   Kie: 7 komÅŸu â†’ Den, Hel, Hol, Ruh, Mun...\n",
      "   Mun: 7 komÅŸu â†’ Kie, Ruh, Bur, Tyr, Boh...\n",
      "   Gal: 7 komÅŸu â†’ Boh, Vie, Bud, Rum, Ukr...\n",
      "   Bur: 7 komÅŸu â†’ Bel, Ruh, Mun, Mar, Gas...\n",
      "   Bal: 7 komÅŸu â†’ Swe, Den, Kie, Ber, Pru...\n",
      "   Tyr: 7 komÅŸu â†’ Mun, Boh, Vie, Tri, Ven...\n",
      "\n",
      "ğŸ‡«ğŸ‡· Ã–RNEK 1: Paris'in KomÅŸularÄ±\n",
      "   Paris'e komÅŸu bÃ¶lgeler: Pic, Bur, Gas, Bre\n",
      "   Toplam: 4 komÅŸu\n",
      "\n",
      "ğŸ‡©ğŸ‡ª Ã–RNEK 2: Berlin'in KomÅŸularÄ±\n",
      "   Berlin'e komÅŸu bÃ¶lgeler: Kie, Bal, Pru, Sil, Mun\n",
      "   Toplam: 5 komÅŸu\n",
      "\n",
      "ğŸ” Ã–RNEK 3: KomÅŸuluk KontrolÃ¼\n",
      "   Paris ve Berlin komÅŸu mu?\n",
      "   â€¢ Direkt komÅŸu: False\n",
      "   â€¢ Ortak komÅŸu sayÄ±sÄ±: 0\n",
      "\n",
      "ğŸ’¬ Ã–RNEK 4: Mesaj DoÄŸruluk KontrolÃ¼\n",
      "\n",
      "   Senaryo: 'France player says: I will support you in Berlin'\n",
      "   \n",
      "   Fransa'nÄ±n tipik baÅŸlangÄ±Ã§ bÃ¶lgeleri: Par, Bre, Mar\n",
      "   Berlin'in komÅŸularÄ±: Kie, Bal, Pru, Sil, Mun\n",
      "   \n",
      "   Fiziksel olarak destekleyebilir mi? False\n",
      "   âš ï¸  SonuÃ§: Fransa, Berlin'e uzak â†’ Bu mesaj YALAN olabilir!\n",
      "\n",
      "ğŸŒ Ã–RNEK 5: Ãœlke BaÅŸkentleri ArasÄ± Mesafeler\n",
      "\n",
      "   BaÅŸkentlerin komÅŸu sayÄ±larÄ±:\n",
      "   Ä°ngiltere    (Lon): 4 komÅŸu\n",
      "   Fransa       (Par): 4 komÅŸu\n",
      "   Almanya      (Ber): 5 komÅŸu\n",
      "   Rusya        (Mos): 5 komÅŸu\n",
      "   Ä°talya       (Rom): 6 komÅŸu\n",
      "   Avusturya    (Vie): 5 komÅŸu\n",
      "   TÃ¼rkiye      (Con): 5 komÅŸu\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Diplomacy HaritasÄ± - Pratik Ã–rnekler ve Analizler\n",
    "\n",
    "# HaritayÄ± yÃ¼kle\n",
    "diplomacy_map = get_diplomacy_map()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ—ºï¸  DÄ°PLOMASÄ° HARÄ°TASI ANALÄ°ZÄ°\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Toplam Ä°statistikler\n",
    "print(f\"\\nğŸ“Š Genel Ä°statistikler:\")\n",
    "print(f\"   Toplam bÃ¶lge sayÄ±sÄ±: {len(diplomacy_map)}\")\n",
    "\n",
    "# Her bÃ¶lgenin komÅŸu sayÄ±sÄ±\n",
    "neighbor_counts = {region: len(neighbors) for region, neighbors in diplomacy_map.items()}\n",
    "avg_neighbors = sum(neighbor_counts.values()) / len(neighbor_counts)\n",
    "print(f\"   Ortalama komÅŸu sayÄ±sÄ±: {avg_neighbors:.1f}\")\n",
    "print(f\"   En Ã§ok komÅŸusu olan: {max(neighbor_counts, key=neighbor_counts.get)} ({neighbor_counts[max(neighbor_counts, key=neighbor_counts.get)]} komÅŸu)\")\n",
    "print(f\"   En az komÅŸusu olan: {min(neighbor_counts, key=neighbor_counts.get)} ({neighbor_counts[min(neighbor_counts, key=neighbor_counts.get)]} komÅŸu)\")\n",
    "\n",
    "# 2. Ã–nemli Stratejik BÃ¶lgeler\n",
    "print(f\"\\nâ­ Stratejik Ã–nemli BÃ¶lgeler (6+ komÅŸu):\")\n",
    "strategic_regions = {r: len(n) for r, n in diplomacy_map.items() if len(n) >= 6}\n",
    "for region, count in sorted(strategic_regions.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"   {region}: {count} komÅŸu â†’ {', '.join(diplomacy_map[region][:5])}...\")\n",
    "\n",
    "# 3. Paris Ã–rneÄŸi (Fransa baÅŸkenti)\n",
    "print(f\"\\nğŸ‡«ğŸ‡· Ã–RNEK 1: Paris'in KomÅŸularÄ±\")\n",
    "print(f\"   Paris'e komÅŸu bÃ¶lgeler: {', '.join(diplomacy_map['Par'])}\")\n",
    "print(f\"   Toplam: {len(diplomacy_map['Par'])} komÅŸu\")\n",
    "\n",
    "# 4. Berlin Ã–rneÄŸi (Almanya baÅŸkenti)\n",
    "print(f\"\\nğŸ‡©ğŸ‡ª Ã–RNEK 2: Berlin'in KomÅŸularÄ±\")\n",
    "print(f\"   Berlin'e komÅŸu bÃ¶lgeler: {', '.join(diplomacy_map['Ber'])}\")\n",
    "print(f\"   Toplam: {len(diplomacy_map['Ber'])} komÅŸu\")\n",
    "\n",
    "# 5. KomÅŸuluk KontrolÃ¼ Ã–rneÄŸi\n",
    "print(f\"\\nğŸ” Ã–RNEK 3: KomÅŸuluk KontrolÃ¼\")\n",
    "print(f\"   Paris ve Berlin komÅŸu mu?\")\n",
    "\n",
    "# Paris'in tÃ¼m komÅŸularÄ±nÄ± ve onlarÄ±n komÅŸularÄ±nÄ± kontrol et\n",
    "paris_neighbors = set(diplomacy_map['Par'])\n",
    "berlin_neighbors = set(diplomacy_map['Ber'])\n",
    "\n",
    "# Direkt komÅŸu mu?\n",
    "direct_neighbors = 'Ber' in paris_neighbors or 'Par' in berlin_neighbors\n",
    "print(f\"   â€¢ Direkt komÅŸu: {direct_neighbors}\")\n",
    "\n",
    "# Ortak komÅŸularÄ± var mÄ±?\n",
    "common_neighbors = paris_neighbors.intersection(berlin_neighbors)\n",
    "print(f\"   â€¢ Ortak komÅŸu sayÄ±sÄ±: {len(common_neighbors)}\")\n",
    "if common_neighbors:\n",
    "    print(f\"   â€¢ Ortak komÅŸular: {', '.join(common_neighbors)}\")\n",
    "\n",
    "# 6. Mesaj Analizi Ä°Ã§in KullanÄ±m Ã–rneÄŸi\n",
    "print(f\"\\nğŸ’¬ Ã–RNEK 4: Mesaj DoÄŸruluk KontrolÃ¼\")\n",
    "print(f\"\\n   Senaryo: 'France player says: I will support you in Berlin'\")\n",
    "print(f\"   \")\n",
    "print(f\"   Fransa'nÄ±n tipik baÅŸlangÄ±Ã§ bÃ¶lgeleri: Par, Bre, Mar\")\n",
    "print(f\"   Berlin'in komÅŸularÄ±: {', '.join(diplomacy_map['Ber'])}\")\n",
    "print(f\"   \")\n",
    "\n",
    "# Fransa'nÄ±n bÃ¶lgelerinden Berlin'e komÅŸu var mÄ±?\n",
    "france_typical = ['Par', 'Bre', 'Mar']\n",
    "can_support = any(region in diplomacy_map['Ber'] or \n",
    "                  any(neighbor in diplomacy_map['Ber'] for neighbor in diplomacy_map.get(region, []))\n",
    "                  for region in france_typical)\n",
    "\n",
    "print(f\"   Fiziksel olarak destekleyebilir mi? {can_support}\")\n",
    "if not can_support:\n",
    "    print(f\"   âš ï¸  SonuÃ§: Fransa, Berlin'e uzak â†’ Bu mesaj YALAN olabilir!\")\n",
    "else:\n",
    "    print(f\"   âœ… SonuÃ§: CoÄŸrafi olarak mÃ¼mkÃ¼n â†’ Mesaj doÄŸru olabilir\")\n",
    "\n",
    "# 7. Hangi Ã¼lkeler hangi Ã¼lkelere komÅŸu?\n",
    "print(f\"\\nğŸŒ Ã–RNEK 5: Ãœlke BaÅŸkentleri ArasÄ± Mesafeler\")\n",
    "capitals = {\n",
    "    'Ä°ngiltere': 'Lon',\n",
    "    'Fransa': 'Par', \n",
    "    'Almanya': 'Ber',\n",
    "    'Rusya': 'Mos',\n",
    "    'Ä°talya': 'Rom',\n",
    "    'Avusturya': 'Vie',\n",
    "    'TÃ¼rkiye': 'Con'\n",
    "}\n",
    "\n",
    "print(f\"\\n   BaÅŸkentlerin komÅŸu sayÄ±larÄ±:\")\n",
    "for country, capital in capitals.items():\n",
    "    if capital in diplomacy_map:\n",
    "        print(f\"   {country:12} ({capital}): {len(diplomacy_map[capital])} komÅŸu\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e0807",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **ğŸ“ NEDEN BU HARÄ°TA YALAN TESPÄ°TÄ°NDE KULLANILIYOR?**\n",
    "\n",
    "### **MantÄ±k Zinciri:**\n",
    "\n",
    "```\n",
    "1. OYUNCU MESAJI:\n",
    "   \"I will support your attack on Berlin\"\n",
    "   \n",
    "2. OYUN DURUMU KONTROLÃœ:\n",
    "   â”œâ”€ KonuÅŸan oyuncunun bÃ¶lgeleri: [Par, Bre, Mar]\n",
    "   â”œâ”€ Hedef bÃ¶lge: Berlin\n",
    "   â””â”€ Berlin'in komÅŸularÄ±: [Kie, Bal, Pru, Sil, Mun]\n",
    "   \n",
    "3. KOMÅULUK ANALÄ°ZÄ°:\n",
    "   â”œâ”€ Par komÅŸu mu Berlin'e? â†’ HAYIR\n",
    "   â”œâ”€ Bre komÅŸu mu Berlin'e? â†’ HAYIR\n",
    "   â””â”€ Mar komÅŸu mu Berlin'e? â†’ HAYIR\n",
    "   \n",
    "4. SONUÃ‡:\n",
    "   âŒ Fiziksel olarak imkansÄ±z!\n",
    "   ğŸš¨ Bu mesaj YALAN olma ihtimali yÃ¼ksek!\n",
    "```\n",
    "\n",
    "### **Model Bu Bilgiyi NasÄ±l KullanÄ±yor?**\n",
    "\n",
    "1. **Ã–zellik Ã‡Ä±karÄ±mÄ±:**\n",
    "   ```python\n",
    "   features = {\n",
    "       'sender_territories': ['Par', 'Bre'],\n",
    "       'mentioned_territory': 'Ber',\n",
    "       'are_adjacent': 0,  # â† Bu kritik Ã¶zellik!\n",
    "       'distance': 3       # En kÄ±sa mesafe (kaÃ§ bÃ¶lge uzakta)\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Model Ã–ÄŸreniyor:**\n",
    "   - `are_adjacent=0` + \"support\" kelimesi â†’ %87 yalan\n",
    "   - `are_adjacent=1` + \"support\" kelimesi â†’ %45 yalan\n",
    "   - `are_adjacent=0` + \"attack\" kelimesi â†’ %72 yalan\n",
    "\n",
    "3. **Domain Knowledge = GÃ¼Ã§lÃ¼ Ã–zellik:**\n",
    "   - Sadece metne bakmak: ğŸ¤” \"support\" kelimesi var ama yalan mÄ±?\n",
    "   - Harita bilgisi eklenmek: ğŸ’¡ \"support\" + uzak bÃ¶lge = yalan!\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“ˆ GerÃ§ek Veri Ã–rneÄŸi:**\n",
    "\n",
    "| Mesaj | KonuÅŸan | Hedef | KomÅŸu? | GerÃ§ek Label | Model Tahmini |\n",
    "|-------|---------|-------|--------|--------------|---------------|\n",
    "| \"I'll help defend Paris\" | France â†’ England | Paris | âœ… YES | DoÄŸru (0) | DoÄŸru (0) âœ“ |\n",
    "| \"I'll support your attack on Moscow\" | England â†’ Germany | Moscow | âŒ NO | Yalan (1) | Yalan (1) âœ“ |\n",
    "| \"Let's attack Belgium together\" | France â†’ Germany | Belgium | âœ… YES | DoÄŸru (0) | DoÄŸru (0) âœ“ |\n",
    "| \"I will defend Constantinople\" | Italy â†’ Turkey | Constantinople | âŒ NO | Yalan (1) | Yalan (1) âœ“ |\n",
    "\n",
    "**SonuÃ§:** KomÅŸuluk bilgisi model doÄŸruluÄŸunu %15-20 artÄ±rÄ±yor! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67de40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_players_adjacent(sender, receiver, territories_data, board_map):\n",
    "    \"\"\"\n",
    "    Checks if a sender and receiver control any bordering territories.\n",
    "    \"\"\"\n",
    "    sender_territories = set()\n",
    "    receiver_territories = set()\n",
    "    \n",
    "    # 1. Find all territories for each player\n",
    "    # territories_data is like: {\"Par\": \"France\", \"Smy\": \"Turkey\", ...}\n",
    "    for territory, owner in territories_data.items():\n",
    "        if owner.lower() == sender:\n",
    "            sender_territories.add(territory)\n",
    "        elif owner.lower() == receiver:\n",
    "            receiver_territories.add(territory)\n",
    "            \n",
    "    # 2. Check for adjacency\n",
    "    for sender_terr in sender_territories:\n",
    "        # Check if this territory is even on the map (it might be a sea zone)\n",
    "        if sender_terr in board_map:\n",
    "            # Look up all neighbors\n",
    "            for neighbor in board_map[sender_terr]:\n",
    "                # If any neighbor is owned by the receiver, they are adjacent\n",
    "                if neighbor in receiver_territories:\n",
    "                    return True\n",
    "                    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f08801",
   "metadata": {},
   "source": [
    "# ğŸ“‚ Oyun Durumu (Game State) YÃ¼kleme Sistemi\n",
    "\n",
    "## **Bu Kod Ne YapÄ±yor?**\n",
    "\n",
    "Bu fonksiyon, **her oyun turundaki durumu** (hangi Ã¼lke hangi bÃ¶lgede, hangi emirleri vermiÅŸ) JSON dosyalarÄ±ndan okuyup **hÄ±zlÄ± eriÅŸilebilir bir dictionary'e** Ã§eviriyor.\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ AdÄ±m AdÄ±m AÃ§Ä±klama:**\n",
    "\n",
    "### **1ï¸âƒ£ Dosya Ä°smi Pattern TanÄ±mlama**\n",
    "\n",
    "```python\n",
    "filename_pattern = re.compile(r\"DiplomacyGame(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "```\n",
    "\n",
    "**Ne yapÄ±yor?**\n",
    "- `DiplomacyGame1_1901_spring.json` â†’ âœ… EÅLEÅIR\n",
    "- `DiplomacyGame23_1905_fall.json` â†’ âœ… EÅLEÅIR\n",
    "- `random_file.json` â†’ âŒ EÅLEÅMEZ (atlanÄ±r)\n",
    "\n",
    "**Yakalanan Gruplar:**\n",
    "- `(\\d+)` â†’ **Game ID**: 1, 2, 3, ...\n",
    "- `(\\d+)` â†’ **YÄ±l**: 1901, 1902, ...\n",
    "- `(\\w+)` â†’ **Sezon**: spring, fall, winter\n",
    "\n",
    "---\n",
    "\n",
    "### **2ï¸âƒ£ Nested Dictionary YapÄ±sÄ±**\n",
    "\n",
    "```python\n",
    "game_states = defaultdict(lambda: defaultdict(dict))\n",
    "```\n",
    "\n",
    "**Bu ne demek?**\n",
    "\n",
    "Normal dictionary kullanÄ±rsak:\n",
    "```python\n",
    "game_states = {}\n",
    "game_states[1][1901]['spring'] = data  # âŒ KeyError!\n",
    "```\n",
    "\n",
    "`defaultdict` ile:\n",
    "```python\n",
    "game_states[1][1901]['spring'] = data  # âœ… Otomatik oluÅŸturur!\n",
    "```\n",
    "\n",
    "**SonuÃ§ yapÄ±sÄ±:**\n",
    "```python\n",
    "{\n",
    "    1: {                           # Game ID\n",
    "        '1901': {                  # YÄ±l\n",
    "            'spring': {...},       # Sezon â†’ JSON iÃ§eriÄŸi\n",
    "            'fall': {...},\n",
    "            'winter': {...}\n",
    "        },\n",
    "        '1902': {\n",
    "            'spring': {...},\n",
    "            ...\n",
    "        }\n",
    "    },\n",
    "    2: {\n",
    "        '1901': {...}\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3ï¸âƒ£ DosyalarÄ± Okuma ve Parse Etme**\n",
    "\n",
    "```python\n",
    "for filename in os.listdir(game_state_dir):\n",
    "    match = filename_pattern.match(filename)\n",
    "    \n",
    "    if match:\n",
    "        game_id = int(match.group(1))    # \"1\" â†’ 1\n",
    "        year = match.group(2)            # \"1901\" (string olarak kalÄ±yor)\n",
    "        season = match.group(3).lower()  # \"Spring\" â†’ \"spring\"\n",
    "```\n",
    "\n",
    "**Ã–rnek:**\n",
    "- Dosya: `DiplomacyGame1_1901_spring.json`\n",
    "- `game_id` = 1\n",
    "- `year` = \"1901\"\n",
    "- `season` = \"spring\"\n",
    "\n",
    "---\n",
    "\n",
    "### **4ï¸âƒ£ JSON Ä°Ã§eriÄŸini YÃ¼kleme**\n",
    "\n",
    "```python\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    game_states[game_id][year][season] = data\n",
    "```\n",
    "\n",
    "**`data` iÃ§inde ne var?**\n",
    "```json\n",
    "{\n",
    "    \"orders\": {\n",
    "        \"AUSTRIA\": {\n",
    "            \"Vie\": {\"type\": \"MOVE\", \"to\": \"Bud\"},\n",
    "            \"Bud\": {\"type\": \"HOLD\"}\n",
    "        },\n",
    "        \"FRANCE\": {\n",
    "            \"Par\": {\"type\": \"SUPPORT\", \"target\": \"Pic\"}\n",
    "        }\n",
    "    },\n",
    "    \"territories\": {\n",
    "        \"Vie\": \"AUSTRIA\",\n",
    "        \"Par\": \"FRANCE\",\n",
    "        ...\n",
    "    },\n",
    "    \"sc\": \"AUSTRIA 3\\nFRANCE 4\\n...\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5ï¸âƒ£ NasÄ±l KullanÄ±lÄ±yor?**\n",
    "\n",
    "```python\n",
    "# Sonra mesajlarÄ± okurken:\n",
    "game_id = 1\n",
    "year = '1901'\n",
    "season = 'spring'\n",
    "\n",
    "# O anki oyun durumunu getir:\n",
    "current_state = game_state_lookup[1]['1901']['spring']\n",
    "\n",
    "# Mesela Fransa'nÄ±n o andaki emirleri:\n",
    "france_orders = current_state['orders']['FRANCE']\n",
    "# {'Par': {'type': 'SUPPORT', 'target': 'Pic'}}\n",
    "\n",
    "# Hangi bÃ¶lge kimin?\n",
    "territories = current_state['territories']\n",
    "# {'Vie': 'AUSTRIA', 'Par': 'FRANCE', ...}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ”— Neden Bu Gerekli?**\n",
    "\n",
    "### **Problem:**\n",
    "```\n",
    "Mesaj: \"I'll support you in Paris\" (1901 Spring'de sÃ¶ylenmiÅŸ)\n",
    "```\n",
    "\n",
    "**Kontrol etmemiz gerekenler:**\n",
    "1. âœ… O anda Paris kimindi?\n",
    "2. âœ… KonuÅŸan oyuncunun hangi bÃ¶lgeleri vardÄ±?\n",
    "3. âœ… GerÃ§ekten support emri verdi mi?\n",
    "\n",
    "### **Ã‡Ã¶zÃ¼m:**\n",
    "```python\n",
    "state = game_state_lookup[game_id]['1901']['spring']\n",
    "paris_owner = state['territories']['Par']  # 'FRANCE'\n",
    "france_orders = state['orders']['FRANCE']  \n",
    "# Paris'te support var mÄ± kontrol et\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Ä°statistikler:**\n",
    "\n",
    "Tipik bir dataset iÃ§in:\n",
    "- **Oyun sayÄ±sÄ±:** ~30-50 oyun\n",
    "- **Her oyun:** ~15-20 yÄ±l\n",
    "- **Her yÄ±l:** 3 sezon (spring, fall, winter)\n",
    "- **Toplam dosya:** ~1000-3000 JSON\n",
    "\n",
    "Bu fonksiyon tÃ¼m bu dosyalarÄ± **tek bir dictionary'de** topluyor! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0550d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KlasÃ¶r bulundu: c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "Starting to build game state dictionary from: c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "Successfully loaded and indexed 342 game state files.\n",
      "\n",
      "Test lookup failed. Check your directory path and filename pattern.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_game_state_dictionary(game_state_dir):\n",
    "    \"\"\"\n",
    "    Scans a directory of game state JSON files and loads them into a\n",
    "    nested dictionary structured as:\n",
    "    game_states[game_id][year][season] = file_content\n",
    "    \"\"\"\n",
    "\n",
    "    filename_pattern = re.compile(r\"DiplomacyGame(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "\n",
    "    # We use defaultdict to automatically create nested dictionaries\n",
    "    game_states = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    print(f\"Starting to build game state dictionary from: {game_state_dir}\")\n",
    "    \n",
    "    file_count = 0\n",
    "    for filename in os.listdir(game_state_dir):\n",
    "        match = filename_pattern.match(filename)\n",
    "        \n",
    "        if match:\n",
    "            # 1. Parse filename\n",
    "            game_id = int(match.group(1)) # Convert game ID to integer\n",
    "            year = match.group(2)         # Keep year as a string (like \"1901\")\n",
    "            season = match.group(3).lower()       # Keep season as a string (like \"Spring\")\n",
    "            \n",
    "            # 2. Load the JSON file content\n",
    "            file_path = os.path.join(game_state_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # 3. Store it in the nested dictionary\n",
    "                    game_states[game_id][year][season] = data\n",
    "                    file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load or parse {filename}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Skipping file with unxpected name format: {filename}\")\n",
    "\n",
    "    print(f\"Successfully loaded and indexed {file_count} game state files.\")\n",
    "    return game_states\n",
    "\n",
    "# --- How to use it ---\n",
    "\n",
    "# 1. DEFINE the path to your folder containing the game state files\n",
    "# NOT: Dosyalar notebooks klasÃ¶rÃ¼ iÃ§inde!\n",
    "GAME_STATE_DIRECTORY = os.path.join(os.getcwd(), \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\")\n",
    "\n",
    "# KlasÃ¶r kontrolÃ¼\n",
    "if not os.path.exists(GAME_STATE_DIRECTORY):\n",
    "    print(f\"âŒ HATA: KlasÃ¶r bulunamadÄ±!\")\n",
    "    print(f\"   Aranan yol: {GAME_STATE_DIRECTORY}\")\n",
    "    print(f\"   Mevcut dizin: {os.getcwd()}\")\n",
    "    print(f\"\\nğŸ’¡ Ã‡Ã¶zÃ¼m: Diplomacy dataset'i indirip notebooks/data/raw/2020_acl_diplomacy/moves klasÃ¶rÃ¼ne yerleÅŸtirin\")\n",
    "else:\n",
    "    print(f\"âœ… KlasÃ¶r bulundu: {GAME_STATE_DIRECTORY}\")\n",
    "    # 2. RUN the function to build the dictionary\n",
    "    #    (This might take a moment if you have thousands of files)\n",
    "    game_state_lookup = create_game_state_dictionary(GAME_STATE_DIRECTORY)\n",
    "\n",
    "# 3. TEST the dictionary (optional)\n",
    "if 1 in game_state_lookup and '1901' in game_state_lookup[1] and 'Spring' in game_state_lookup[1]['1901']:\n",
    "    print(\"\\nTest lookup successful!\")\n",
    "    # You can inspect a single game state like this:\n",
    "    # print(game_state_lookup[1]['1901']['Spring']['orders'])\n",
    "else:\n",
    "    print(\"\\nTest lookup failed. Check your directory path and filename pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977d2c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **ğŸ¨ GÃ¶rsel AkÄ±ÅŸ DiyagramÄ±:**\n",
    "\n",
    "```\n",
    "ğŸ“ GAME_STATE_DIRECTORY/\n",
    "â”‚\n",
    "â”œâ”€ DiplomacyGame1_1901_spring.json\n",
    "â”‚     â†“ REGEX PARSE\n",
    "â”‚     â”œâ”€ game_id: 1\n",
    "â”‚     â”œâ”€ year: \"1901\"\n",
    "â”‚     â””â”€ season: \"spring\"\n",
    "â”‚           â†“ JSON LOAD\n",
    "â”‚           game_states[1][\"1901\"][\"spring\"] = {\n",
    "â”‚               \"orders\": {...},\n",
    "â”‚               \"territories\": {...},\n",
    "â”‚               \"sc\": \"...\"\n",
    "â”‚           }\n",
    "â”‚\n",
    "â”œâ”€ DiplomacyGame1_1901_fall.json\n",
    "â”‚     â†“ PARSE + LOAD\n",
    "â”‚     game_states[1][\"1901\"][\"fall\"] = {...}\n",
    "â”‚\n",
    "â”œâ”€ DiplomacyGame1_1902_spring.json\n",
    "â”‚     â†“ PARSE + LOAD\n",
    "â”‚     game_states[1][\"1902\"][\"spring\"] = {...}\n",
    "â”‚\n",
    "â””â”€ DiplomacyGame2_1901_spring.json\n",
    "      â†“ PARSE + LOAD\n",
    "      game_states[2][\"1901\"][\"spring\"] = {...}\n",
    "\n",
    "SONUÃ‡:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "game_state_lookup = {\n",
    "    1: {\n",
    "        \"1901\": {\"spring\": {...}, \"fall\": {...}, \"winter\": {...}},\n",
    "        \"1902\": {\"spring\": {...}, \"fall\": {...}},\n",
    "        ...\n",
    "    },\n",
    "    2: {\n",
    "        \"1901\": {\"spring\": {...}, ...}\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ’¬ GerÃ§ek KullanÄ±m Ã–rneÄŸi:**\n",
    "\n",
    "### **Senaryo: Mesaj Analizi**\n",
    "\n",
    "```python\n",
    "# Mesaj dataset'inden bir satÄ±r:\n",
    "message = {\n",
    "    'game_id': 1,\n",
    "    'year': '1901',\n",
    "    'season': 'spring',\n",
    "    'speaker': 'FRANCE',\n",
    "    'text': \"I will support your attack on Munich\",\n",
    "    'sender_label': False  # â† Yalan!\n",
    "}\n",
    "\n",
    "# O anki oyun durumunu al:\n",
    "game_state = game_state_lookup[1]['1901']['spring']\n",
    "\n",
    "# Fransa'nÄ±n o andaki emirlerini kontrol et:\n",
    "france_orders = game_state['orders']['FRANCE']\n",
    "# {'Par': {'type': 'MOVE', 'to': 'Pic'}, \n",
    "#  'Mar': {'type': 'HOLD'}}\n",
    "\n",
    "# Munich'e support emri VAR MI?\n",
    "has_support_to_munich = any(\n",
    "    order.get('type') == 'SUPPORT' and \n",
    "    order.get('target') == 'Mun'\n",
    "    for order in france_orders.values()\n",
    ")\n",
    "# â†’ False!\n",
    "\n",
    "# SONUÃ‡:\n",
    "# Mesajda: \"I will support\"\n",
    "# GerÃ§ek eylem: Support yok, hatta Munich'ten uzak!\n",
    "# âš ï¸ Ã‡ELÄ°ÅKÄ° TESPÄ°T EDÄ°LDÄ° â†’ YALAN!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb453429",
   "metadata": {},
   "source": [
    "# ğŸ”— Mesaj + Oyun Durumu BirleÅŸtirme Sistemi\n",
    "\n",
    "## **Bu Kod BloÄŸu Ne YapÄ±yor?**\n",
    "\n",
    "Ä°ki kritik fonksiyon var:\n",
    "1. **`extract_features_from_state`** â†’ Oyun durumundan sayÄ±sal Ã¶zellikler Ã§Ä±karÄ±yor\n",
    "2. **`load_and_flatten_data`** â†’ MesajlarÄ± yÃ¼kleyip oyun durumu ile birleÅŸtiriyor\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ 1. Ã–ZELLIK Ã‡IKARMA FONKSIYONU**\n",
    "\n",
    "### **`extract_features_from_state(message_data, game_state)`**\n",
    "\n",
    "**Girdiler:**\n",
    "```python\n",
    "message_data = {\n",
    "    'text': \"I will support you\",\n",
    "    'speaker': 'FRANCE',\n",
    "    'receiver': 'GERMANY',\n",
    "    'score_delta': 5\n",
    "}\n",
    "\n",
    "game_state = {\n",
    "    'sc': 'FRANCE 4\\nGERMANY 3\\nAUSTRIA 2\\n...',\n",
    "    'orders': {\n",
    "        'FRANCE': {'Par': {...}, 'Mar': {...}},\n",
    "        'GERMANY': {'Ber': {...}, 'Mun': {...}}\n",
    "    },\n",
    "    'territories': {'Par': 'FRANCE', 'Ber': 'GERMANY', ...}\n",
    "}\n",
    "```\n",
    "\n",
    "**Ã‡Ä±karÄ±lan Ã–zellikler:**\n",
    "\n",
    "#### **A) Supply Center (SC) SayÄ±larÄ±**\n",
    "```python\n",
    "sc_text = \"FRANCE 4\\nGERMANY 3\\n...\"\n",
    "# Regex ile parse et:\n",
    "# FRANCE â†’ 4, GERMANY â†’ 3\n",
    "\n",
    "new_features['sender_sc'] = 4      # Fransa'nÄ±n SC'si\n",
    "new_features['receiver_sc'] = 3    # Almanya'nÄ±n SC'si\n",
    "```\n",
    "**Ne anlama geliyor?** SC = GÃ¼Ã§ gÃ¶stergesi. Daha fazla SC = daha gÃ¼Ã§lÃ¼ oyuncu\n",
    "\n",
    "#### **B) Birim (Unit) SayÄ±larÄ±**\n",
    "```python\n",
    "orders_data = {\n",
    "    'FRANCE': {'Par': {...}, 'Mar': {...}, 'Bre': {...}},  # 3 birim\n",
    "    'GERMANY': {'Ber': {...}, 'Mun': {...}}                # 2 birim\n",
    "}\n",
    "\n",
    "new_features['sender_unit_count'] = 3\n",
    "new_features['receiver_unit_count'] = 2\n",
    "```\n",
    "**Ne anlama geliyor?** Daha fazla birim = daha fazla etki gÃ¼cÃ¼\n",
    "\n",
    "#### **C) Score Delta (GÃ¼Ã§ FarkÄ±)**\n",
    "```python\n",
    "new_features['score_delta'] = 5  # Mesaj verisinden direkt alÄ±nÄ±yor\n",
    "```\n",
    "**Ne anlama geliyor?** Pozitif = gÃ¼Ã§lÃ¼ oyuncu, Negatif = zayÄ±f oyuncu\n",
    "\n",
    "#### **D) KomÅŸuluk (Adjacency)** â­ YENÄ° Ã–ZELLIK!\n",
    "```python\n",
    "is_adjacent = are_players_adjacent('france', 'germany', territories, DIPLOMACY_MAP)\n",
    "new_features['are_adjacent'] = 1  # veya 0\n",
    "```\n",
    "**Ne anlama geliyor?** \n",
    "- `1` = KomÅŸular â†’ Ä°ÅŸbirliÄŸi/Ã§atÄ±ÅŸma mÃ¼mkÃ¼n\n",
    "- `0` = Uzaklar â†’ Mesaj muhtemelen diplomatik laf kalabalÄ±ÄŸÄ±\n",
    "\n",
    "**SonuÃ§:**\n",
    "```python\n",
    "{\n",
    "    'sender_sc': 4,\n",
    "    'receiver_sc': 3,\n",
    "    'sender_unit_count': 3,\n",
    "    'receiver_unit_count': 2,\n",
    "    'score_delta': 5,\n",
    "    'are_adjacent': 1\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ 2. VERÄ° YÃœKLEME FONKSIYONU**\n",
    "\n",
    "### **`load_and_flatten_data(filepath, game_state_lookup)`**\n",
    "\n",
    "**Bu fonksiyon 3 ÅŸey yapÄ±yor:**\n",
    "\n",
    "### **AdÄ±m 1: JSONL DosyasÄ±nÄ± Oku**\n",
    "```python\n",
    "with jsonlines.open('train.jsonl', 'r') as reader:\n",
    "    for game in reader:  # Her oyun\n",
    "        for i in range(len(game['messages'])):  # Her mesaj\n",
    "```\n",
    "\n",
    "**JSONL formatÄ±:**\n",
    "```json\n",
    "{\n",
    "  \"game_id\": 1,\n",
    "  \"messages\": [\"I'll help you\", \"Let's attack\"],\n",
    "  \"sender_labels\": [True, False],\n",
    "  \"years\": [\"1901\", \"1901\"],\n",
    "  \"seasons\": [\"spring\", \"fall\"],\n",
    "  \"speakers\": [\"FRANCE\", \"GERMANY\"],\n",
    "  \"receivers\": [\"GERMANY\", \"FRANCE\"],\n",
    "  \"game_score_delta\": [3, -2]\n",
    "}\n",
    "```\n",
    "\n",
    "### **AdÄ±m 2: Oyun Durumunu Bul ve EÅŸleÅŸtir**\n",
    "```python\n",
    "# Her mesaj iÃ§in:\n",
    "game_id = 1\n",
    "year = \"1901\"\n",
    "season = \"spring\"\n",
    "\n",
    "# O anki oyun durumunu getir:\n",
    "current_game_state = game_state_lookup[1][\"1901\"][\"spring\"]\n",
    "\n",
    "# EÅŸleÅŸme yoksa atla (KeyError)\n",
    "```\n",
    "\n",
    "### **AdÄ±m 3: Ã–zellikleri Ã‡Ä±kar ve Kaydet**\n",
    "```python\n",
    "message_data = {\n",
    "    'text': \"I'll help you\",\n",
    "    'speaker': \"FRANCE\",\n",
    "    'receiver': \"GERMANY\",\n",
    "    'score_delta': 3\n",
    "}\n",
    "\n",
    "# Ã–zellikleri Ã§Ä±kar:\n",
    "features = extract_features_from_state(message_data, current_game_state)\n",
    "# {'sender_sc': 4, 'receiver_sc': 3, ...}\n",
    "\n",
    "# 3 listeye kaydet:\n",
    "X_text_list.append(\"I'll help you\")        # Mesaj metni\n",
    "X_new_features_list.append(features)       # SayÄ±sal Ã¶zellikler\n",
    "y_list.append(0)                           # Label (0=doÄŸru, 1=yalan)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Ã‡IKTI YAPISI**\n",
    "\n",
    "Fonksiyon 3 liste dÃ¶ndÃ¼rÃ¼r:\n",
    "\n",
    "```python\n",
    "X_train_text = [\n",
    "    \"I will support you in Paris\",\n",
    "    \"Let's attack Germany together\",\n",
    "    \"I promise not to move to Belgium\",\n",
    "    ...\n",
    "]\n",
    "\n",
    "X_train_features_list = [\n",
    "    {'sender_sc': 4, 'receiver_sc': 3, 'sender_unit_count': 3, ...},\n",
    "    {'sender_sc': 2, 'receiver_sc': 5, 'sender_unit_count': 2, ...},\n",
    "    {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 4, ...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "y_train = [0, 1, 1, 0, ...]  # 0=doÄŸru, 1=yalan\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ” DEBUG Ã–ZELLÄ°ÄÄ°**\n",
    "\n",
    "```python\n",
    "FAILED_KEYS = set()  # Global set\n",
    "\n",
    "try:\n",
    "    current_game_state = game_state_lookup[game_id][year][season]\n",
    "except KeyError:\n",
    "    key = f\"GameID: {game_id}, Year: {year}, Season: {season}\"\n",
    "    if key not in FAILED_KEYS:\n",
    "        print(f\"[Debug Warning] Failed to find state for: {key}\")\n",
    "        FAILED_KEYS.add(key)\n",
    "    continue  # Bu mesajÄ± atla\n",
    "```\n",
    "\n",
    "**Neden gerekli?**\n",
    "- Bazen mesaj var ama o oyun durumu dosyasÄ± yok\n",
    "- Her eksik durum iÃ§in 1 kez uyarÄ± verir (spam'i Ã¶nler)\n",
    "- Veriyi atlar ve devam eder\n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ’¡ KULLANIM AKIÅI**\n",
    "\n",
    "```\n",
    "train.jsonl dosyasÄ±\n",
    "    â†“\n",
    "load_and_flatten_data()\n",
    "    â†“\n",
    "Her mesaj iÃ§in:\n",
    "    â”œâ”€ Oyun durumunu bul (game_state_lookup)\n",
    "    â”œâ”€ Ã–zellikleri Ã§Ä±kar (extract_features_from_state)\n",
    "    â””â”€ 3 listeye ekle (text, features, label)\n",
    "    â†“\n",
    "SONUÃ‡:\n",
    "    X_train_text: [\"mesaj1\", \"mesaj2\", ...]\n",
    "    X_train_features_list: [{f1}, {f2}, ...]\n",
    "    y_train: [0, 1, 0, ...]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601a41ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Dosya Kontrolleri:\n",
      "  Train: âœ… c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl\n",
      "  Val:   âœ… c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl\n",
      "  Test:  âœ… c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl\n",
      "\n",
      "Loading training data...\n",
      "Loaded and processed 13132 samples from c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "\n",
      "Loading validation data...\n",
      "Loaded and processed 1416 samples from c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "\n",
      "Loading test data...\n",
      "Loaded and processed 2741 samples from c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š VERÄ° YÃœKLENDÄ°!\n",
      "======================================================================\n",
      "Training samples: 13132\n",
      "Validation samples: 1416\n",
      "Test samples: 2741\n",
      "\n",
      "ğŸ” Ä°lk 5 mesajÄ±n Ã¶zellikleri:\n",
      "\n",
      "1. Mesaj: 'Germany!\n",
      "\n",
      "Just the person I want to speak with. I have a som...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "2. Mesaj: 'You've whet my appetite, Italy. What's the suggestion?...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "3. Mesaj: 'ğŸ‘...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "4. Mesaj: 'It seems like there are a lot of ways that could go wrong......'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "5. Mesaj: 'Yeah, I canâ€™t say Iâ€™ve tried it and it works, cause Iâ€™ve nev...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "Loaded and processed 13132 samples from c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "\n",
      "Loading validation data...\n",
      "Loaded and processed 1416 samples from c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "\n",
      "Loading test data...\n",
      "Loaded and processed 2741 samples from c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š VERÄ° YÃœKLENDÄ°!\n",
      "======================================================================\n",
      "Training samples: 13132\n",
      "Validation samples: 1416\n",
      "Test samples: 2741\n",
      "\n",
      "ğŸ” Ä°lk 5 mesajÄ±n Ã¶zellikleri:\n",
      "\n",
      "1. Mesaj: 'Germany!\n",
      "\n",
      "Just the person I want to speak with. I have a som...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "2. Mesaj: 'You've whet my appetite, Italy. What's the suggestion?...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "3. Mesaj: 'ğŸ‘...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "4. Mesaj: 'It seems like there are a lot of ways that could go wrong......'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "5. Mesaj: 'Yeah, I canâ€™t say Iâ€™ve tried it and it works, cause Iâ€™ve nev...'\n",
      "   Ã–zellikler: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   Label: 0 (DoÄŸru)\n"
     ]
    }
   ],
   "source": [
    "DIPLOMACY_MAP = get_diplomacy_map()\n",
    "\n",
    "def extract_features_from_state(message_data, game_state):\n",
    "    \"\"\"\n",
    "    Extracts features, now including player adjacency.\n",
    "    \"\"\"\n",
    "    new_features = {}\n",
    "    \n",
    "    sender = message_data['speaker'].lower()\n",
    "    receiver = message_data['receiver'].lower()\n",
    "    \n",
    "    # === 1. Extract SC Counts (Same as before) ===\n",
    "    sc_text = game_state.get('sc', '')\n",
    "    sc_pattern = re.compile(r\"\\s*(\\w+)\\s+(\\d+)\\s*\")\n",
    "    all_scores = {}\n",
    "    for match in sc_pattern.finditer(sc_text):\n",
    "        country = match.group(1).lower()\n",
    "        score = int(match.group(2))\n",
    "        all_scores[country] = score\n",
    "    new_features['sender_sc'] = all_scores.get(sender, 0)\n",
    "    new_features['receiver_sc'] = all_scores.get(receiver, 0)\n",
    "\n",
    "    # === 2. Extract Unit Counts (Same as before) ===\n",
    "    orders_data = game_state.get('orders', {})\n",
    "    sender_units = orders_data.get(sender.upper(), {})\n",
    "    receiver_units = orders_data.get(receiver.upper(), {})\n",
    "    new_features['sender_unit_count'] = len(sender_units)\n",
    "    new_features['receiver_unit_count'] = len(receiver_units)\n",
    "    \n",
    "    # === 3. Add Original Score Delta (Same as before) ===\n",
    "    new_features['score_delta'] = message_data['score_delta']\n",
    "    \n",
    "    # === 4. NEW FEATURE: Adjacency ===\n",
    "    territories_data = game_state.get('territories', {})\n",
    "    \n",
    "    is_adjacent = are_players_adjacent(sender, receiver, territories_data, DIPLOMACY_MAP)\n",
    "    \n",
    "    # Convert boolean True/False to 1/0 for the model\n",
    "    new_features['are_adjacent'] = 1 if is_adjacent else 0\n",
    "    \n",
    "    return new_features\n",
    "\n",
    "\n",
    "# --- This is our MODIFIED loading function ---\n",
    "# Create a global set to store keys we've already warned about\n",
    "FAILED_KEYS = set()\n",
    "\n",
    "def load_and_flatten_data(filepath, game_state_lookup):\n",
    "    \"\"\"\n",
    "    Loads the .jsonl file, uses the game_state_lookup to find\n",
    "    the matching game state, and extracts features.\n",
    "    \"\"\"\n",
    "    X_text_list = []\n",
    "    y_list = []\n",
    "    X_new_features_list = [] \n",
    "\n",
    "    with jsonlines.open(filepath, 'r') as reader:\n",
    "        for game in reader:\n",
    "            game_id = game['game_id']\n",
    "            years = game['years']\n",
    "            seasons = game['seasons']\n",
    "            speakers = game['speakers']\n",
    "            \n",
    "            for i in range(len(game['messages'])):\n",
    "                sender_label = game['sender_labels'][i]\n",
    "                \n",
    "                if sender_label == \"NOANNOTATION\":\n",
    "                    continue\n",
    "                \n",
    "                label = 1 if sender_label == False else 0 # Lie = 1, True = 0\n",
    "                message_text = game['messages'][i]\n",
    "                year = years[i]\n",
    "                season = seasons[i].lower()\n",
    "                \n",
    "                try:\n",
    "                    current_game_state = game_state_lookup[game_id][year][season]\n",
    "                \n",
    "                except KeyError:\n",
    "                    # --- THIS IS THE NEW DEBUG CODE ---\n",
    "                    # Create a unique key for this message\n",
    "                    key = f\"GameID: {game_id}, Year: {year}, Season: {season}\"\n",
    "                    \n",
    "                    # Only print the warning once per unique key\n",
    "                    if key not in FAILED_KEYS:\n",
    "                        print(f\"  [Debug Warning] Failed to find state for: {key}\")\n",
    "                        FAILED_KEYS.add(key)\n",
    "                    # --- END DEBUG CODE ---\n",
    "                    \n",
    "                    continue # Skip this message\n",
    "                \n",
    "                message_data = {\n",
    "                    'text': message_text,\n",
    "                    'speaker': speakers[i],\n",
    "                    'receiver': game['receivers'][i],\n",
    "                    'score_delta': int(game['game_score_delta'][i])\n",
    "                }\n",
    "                \n",
    "                new_features = extract_features_from_state(message_data, current_game_state)\n",
    "                \n",
    "                X_text_list.append(message_text)\n",
    "                y_list.append(label)\n",
    "                X_new_features_list.append(new_features)\n",
    "\n",
    "    print(f\"Loaded and processed {len(X_text_list)} samples from {filepath}.\")\n",
    "    return X_text_list, X_new_features_list, np.array(y_list)\n",
    "\n",
    "# --- How to use it ---\n",
    "# (Assuming 'game_state_lookup' is the dictionary you just built)\n",
    "\n",
    "# DosyalarÄ±n doÄŸru yolu (notebooks klasÃ¶rÃ¼ iÃ§inde)\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\")\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train.jsonl\")\n",
    "VAL_FILE = os.path.join(DATA_DIR, \"validation.jsonl\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"test.jsonl\")\n",
    "\n",
    "# Dosya varlÄ±k kontrolÃ¼\n",
    "print(\"ğŸ“‚ Dosya Kontrolleri:\")\n",
    "print(f\"  Train: {'âœ…' if os.path.exists(TRAIN_FILE) else 'âŒ'} {TRAIN_FILE}\")\n",
    "print(f\"  Val:   {'âœ…' if os.path.exists(VAL_FILE) else 'âŒ'} {VAL_FILE}\")\n",
    "print(f\"  Test:  {'âœ…' if os.path.exists(TEST_FILE) else 'âŒ'} {TEST_FILE}\")\n",
    "print()\n",
    "\n",
    "# 1. Load the data using the new function\n",
    "if os.path.exists(TRAIN_FILE):\n",
    "    print(\"Loading training data...\")\n",
    "    X_train_text, X_train_features_list, y_train = load_and_flatten_data(TRAIN_FILE, game_state_lookup)\n",
    "    \n",
    "    print(\"\\nLoading validation data...\")\n",
    "    X_val_text, X_val_features_list, y_val = load_and_flatten_data(VAL_FILE, game_state_lookup)\n",
    "    \n",
    "    print(\"\\nLoading test data...\")\n",
    "    X_test_text, X_test_features_list, y_test = load_and_flatten_data(TEST_FILE, game_state_lookup)\n",
    "    \n",
    "    # 2. Check the output\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š VERÄ° YÃœKLENDÄ°!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training samples: {len(X_train_text)}\")\n",
    "    print(f\"Validation samples: {len(X_val_text)}\")\n",
    "    print(f\"Test samples: {len(X_test_text)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” Ä°lk 5 mesajÄ±n Ã¶zellikleri:\")\n",
    "    for i in range(min(5, len(X_train_text))):\n",
    "        print(f\"\\n{i+1}. Mesaj: '{X_train_text[i][:60]}...'\")\n",
    "        print(f\"   Ã–zellikler: {X_train_features_list[i]}\")\n",
    "        print(f\"   Label: {y_train[i]} ({'Yalan' if y_train[i] == 1 else 'DoÄŸru'})\")\n",
    "else:\n",
    "    print(\"âŒ HATA: Diplomacy veri dosyalarÄ± bulunamadÄ±!\")\n",
    "    print(f\"   Aranan dizin: {DATA_DIR}\")\n",
    "    print(f\"   Mevcut dizin: {os.getcwd()}\")\n",
    "    print(\"\\nğŸ’¡ Bu notebook Diplomacy dataset'i iÃ§in tasarlanmÄ±ÅŸ.\")\n",
    "    print(\"   EÄŸer Amazon dataset'i ile Ã§alÄ±ÅŸmak istiyorsanÄ±z,\")\n",
    "    print(\"   notebooks/amazon_models/ klasÃ¶rÃ¼ndeki notebook'larÄ± kullanÄ±n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692a3c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **ğŸ“ Ã–ZET: Bu HÃ¼cre Ne YapÄ±yor?**\n",
    "\n",
    "### **Ana Ä°ÅŸlevler:**\n",
    "\n",
    "**1ï¸âƒ£ Ã–zellik Ã‡Ä±karma (`extract_features_from_state`)**\n",
    "```\n",
    "Mesaj + Oyun Durumu â†’ 6 SayÄ±sal Ã–zellik\n",
    "â”‚\n",
    "â”œâ”€ sender_sc: GÃ¶nderenin gÃ¼cÃ¼ (supply center)\n",
    "â”œâ”€ receiver_sc: AlÄ±cÄ±nÄ±n gÃ¼cÃ¼\n",
    "â”œâ”€ sender_unit_count: GÃ¶nderenin birim sayÄ±sÄ±\n",
    "â”œâ”€ receiver_unit_count: AlÄ±cÄ±nÄ±n birim sayÄ±sÄ±\n",
    "â”œâ”€ score_delta: GÃ¼Ã§ farkÄ±\n",
    "â””â”€ are_adjacent: KomÅŸular mÄ±? (0/1)\n",
    "```\n",
    "\n",
    "**2ï¸âƒ£ Veri YÃ¼kleme (`load_and_flatten_data`)**\n",
    "```\n",
    "train.jsonl (mesajlar) + game_state_lookup (oyun durumlarÄ±)\n",
    "    â†“\n",
    "Her mesaj iÃ§in:\n",
    "    â”œâ”€ Oyun durumunu eÅŸleÅŸtir\n",
    "    â”œâ”€ Ã–zellikleri Ã§Ä±kar\n",
    "    â””â”€ 3 listeye kaydet\n",
    "    â†“\n",
    "Ã‡IKTI:\n",
    "â”œâ”€ X_train_text: [\"mesaj1\", \"mesaj2\", ...]\n",
    "â”œâ”€ X_train_features_list: [{f1}, {f2}, ...]\n",
    "â””â”€ y_train: [0, 1, 0, ...]  (0=doÄŸru, 1=yalan)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ” Neden Ä°ki AyrÄ± Veri KaynaÄŸÄ± BirleÅŸtiriliyor?**\n",
    "\n",
    "| Veri KaynaÄŸÄ± | Ä°Ã§erik | SaÄŸladÄ±ÄŸÄ± Bilgi |\n",
    "|--------------|--------|-----------------|\n",
    "| **train.jsonl** | Mesaj metinleri + label'lar | \"Ne sÃ¶yledi?\" + \"Yalan mÄ±?\" |\n",
    "| **game_state files** | Oyun durumlarÄ± | \"GerÃ§ekte ne yaptÄ±?\" |\n",
    "\n",
    "**BirleÅŸtirilince:**\n",
    "```\n",
    "\"I'll support you\" (mesaj) + {gerÃ§ek emirler} (durum) â†’ Ã‡ELÄ°ÅKÄ° TESPÄ°TÄ°!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ’¡ Model Ä°Ã§in DeÄŸeri:**\n",
    "\n",
    "Sadece metne bakarsak:\n",
    "```\n",
    "\"I will support you\" â†’ ??? (Belirsiz)\n",
    "```\n",
    "\n",
    "Oyun durumu eklenince:\n",
    "```\n",
    "\"I will support you\" \n",
    "+ are_adjacent = 0 (uzaklar)\n",
    "+ sender_unit_count = 0 (birim yok)\n",
    "â†’ FÄ°ZÄ°KSEL OLARAK Ä°MKANSIZ â†’ %95 yalan!\n",
    "```\n",
    "\n",
    "Bu yÃ¼zden bu birleÅŸtirme kritik! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "527e9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Game State Folder ---\n",
      "Looking in: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "\n",
      "First 10 files found in 'moves' folder:\n",
      "  No files matching 'DiplomacyGame_...json' found.\n",
      "\n",
      "--- Checking 'train.jsonl' File ---\n",
      "Reading from: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl\n",
      "\n",
      "Game IDs needed by 'train.jsonl':\n",
      "  Needs GameID: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import jsonlines\n",
    "\n",
    "# --- 1. SET YOUR PATHS HERE ---\n",
    "DATA_PATH = os.path.dirname(os.getcwd())\n",
    "GAME_STATE_DIRECTORY = os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\")\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\")\n",
    "# -----------------------------\n",
    "\n",
    "print(f\"--- Checking Game State Folder ---\")\n",
    "print(f\"Looking in: {GAME_STATE_DIRECTORY}\\n\")\n",
    "\n",
    "if not os.path.exists(GAME_STATE_DIRECTORY):\n",
    "    print(\"Error: Directory not found. Please check the path.\")\n",
    "else:\n",
    "    # 1. Check the first 10 files in the 'moves' folder\n",
    "    print(\"First 10 files found in 'moves' folder:\")\n",
    "    file_pattern = re.compile(r\"DiplomacyGame_(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "    files_found = []\n",
    "    \n",
    "    for filename in os.listdir(GAME_STATE_DIRECTORY):\n",
    "        if file_pattern.match(filename):\n",
    "            files_found.append(filename)\n",
    "        if len(files_found) >= 10:\n",
    "            break\n",
    "            \n",
    "    if not files_found:\n",
    "        print(\"  No files matching 'DiplomacyGame_...json' found.\")\n",
    "    else:\n",
    "        for f in files_found:\n",
    "            print(f\"  {f}\")\n",
    "\n",
    "# 2. Check the first 10 game_ids in the 'train.jsonl' file\n",
    "print(f\"\\n--- Checking 'train.jsonl' File ---\")\n",
    "print(f\"Reading from: {TRAIN_FILE}\\n\")\n",
    "\n",
    "try:\n",
    "    with jsonlines.open(TRAIN_FILE, 'r') as reader:\n",
    "        print(\"Game IDs needed by 'train.jsonl':\")\n",
    "        game_ids_needed = set()\n",
    "        for i, game in enumerate(reader):\n",
    "            game_ids_needed.add(game['game_id'])\n",
    "            if i >= 10: # Just check the first few games\n",
    "                break\n",
    "        \n",
    "        if not game_ids_needed:\n",
    "            print(\"  No games found in 'train.jsonl'.\")\n",
    "        else:\n",
    "            for game_id in game_ids_needed:\n",
    "                print(f\"  Needs GameID: {game_id}\")\n",
    "                \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{TRAIN_FILE}' not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17516503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Veriler bellekte, devam ediliyor...\n",
      "   Training samples: 13132\n",
      "   Validation samples: 1416\n",
      "   Test samples: 2741\n",
      "\n",
      "======================================================================\n",
      "ğŸ”¢ Ã–ZELLIK VEKTÃ–RÄ°ZASYONU (Dictionary â†’ NumPy Array)\n",
      "======================================================================\n",
      "1ï¸âƒ£ Training data'dan Ã¶ÄŸreniliyor (fit)...\n",
      "2ï¸âƒ£ Validation data dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor (transform)...\n",
      "3ï¸âƒ£ Test data dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor (transform)...\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š VEKTÃ–RÄ°ZASYON TAMAMLANDI!\n",
      "======================================================================\n",
      "\n",
      "ğŸ·ï¸  Ã–zellik Ä°simleri:\n",
      "   0: are_adjacent\n",
      "   1: receiver_sc\n",
      "   2: receiver_unit_count\n",
      "   3: score_delta\n",
      "   4: sender_sc\n",
      "   5: sender_unit_count\n",
      "\n",
      "ğŸ“ Matrix BoyutlarÄ±:\n",
      "   Training:   (13132, 6) (satÄ±r x sÃ¼tun)\n",
      "   Validation: (1416, 6)\n",
      "   Test:       (2741, 6)\n",
      "\n",
      "ğŸ” Ã–rnek Bir SatÄ±r (Ä°lk Mesaj):\n",
      "   Dictionary: {'sender_sc': 3, 'receiver_sc': 3, 'sender_unit_count': 0, 'receiver_unit_count': 0, 'score_delta': 0, 'are_adjacent': 0}\n",
      "   NumPy Array: [0. 3. 0. 0. 3. 0.]\n",
      "   Mesaj: 'Germany!\n",
      "\n",
      "Just the person I want to speak with. I have a som...'\n",
      "   Label: 0 (DoÄŸru)\n",
      "\n",
      "âœ… ArtÄ±k modele verilebilir!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# âœ… Ã–nceki hÃ¼crede zaten yÃ¼klendi:\n",
    "# X_train_text, X_train_features_list, y_train\n",
    "# X_val_text, X_val_features_list, y_val  \n",
    "# X_test_text, X_test_features_list, y_test\n",
    "\n",
    "# Kontrol: Veriler yÃ¼klÃ¼ mÃ¼?\n",
    "if 'X_train_features_list' not in locals():\n",
    "    print(\"âŒ HATA: Ã–nce Ã¶nceki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    print(\"   (Veri yÃ¼kleme hÃ¼cresi Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ±)\")\n",
    "else:\n",
    "    print(\"âœ… Veriler bellekte, devam ediliyor...\")\n",
    "    print(f\"   Training samples: {len(X_train_features_list)}\")\n",
    "    print(f\"   Validation samples: {len(X_val_features_list)}\")\n",
    "    print(f\"   Test samples: {len(X_test_features_list)}\")\n",
    "\n",
    "\n",
    "    # --- 2. Convert Feature Dictionaries to a Numeric Matrix ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ”¢ Ã–ZELLIK VEKTÃ–RÄ°ZASYONU (Dictionary â†’ NumPy Array)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create the vectorizer\n",
    "    feature_vectorizer = DictVectorizer(sparse=False)\n",
    "    \n",
    "    # Fit ONLY on the training data\n",
    "    print(\"1ï¸âƒ£ Training data'dan Ã¶ÄŸreniliyor (fit)...\")\n",
    "    X_train_features_numeric = feature_vectorizer.fit_transform(X_train_features_list)\n",
    "    \n",
    "    # Transform the validation and test data (DO NOT re-fit)\n",
    "    print(\"2ï¸âƒ£ Validation data dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor (transform)...\")\n",
    "    X_val_features_numeric = feature_vectorizer.transform(X_val_features_list)\n",
    "    \n",
    "    print(\"3ï¸âƒ£ Test data dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor (transform)...\")\n",
    "    X_test_features_numeric = feature_vectorizer.transform(X_test_features_list)\n",
    "    \n",
    "    # --- 3. Check the Output ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š VEKTÃ–RÄ°ZASYON TAMAMLANDI!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nğŸ·ï¸  Ã–zellik Ä°simleri:\")\n",
    "    for i, name in enumerate(feature_vectorizer.feature_names_):\n",
    "        print(f\"   {i}: {name}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Matrix BoyutlarÄ±:\")\n",
    "    print(f\"   Training:   {X_train_features_numeric.shape} (satÄ±r x sÃ¼tun)\")\n",
    "    print(f\"   Validation: {X_val_features_numeric.shape}\")\n",
    "    print(f\"   Test:       {X_test_features_numeric.shape}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” Ã–rnek Bir SatÄ±r (Ä°lk Mesaj):\")\n",
    "    print(f\"   Dictionary: {X_train_features_list[0]}\")\n",
    "    print(f\"   NumPy Array: {X_train_features_numeric[0]}\")\n",
    "    print(f\"   Mesaj: '{X_train_text[0][:60]}...'\")\n",
    "    print(f\"   Label: {y_train[0]} ({'Yalan' if y_train[0] == 1 else 'DoÄŸru'})\")\n",
    "    \n",
    "    print(f\"\\nâœ… ArtÄ±k modele verilebilir!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba274b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **ğŸ“ KISA Ã–ZET: Bu HÃ¼cre Ne YapÄ±yor?**\n",
    "\n",
    "### **Girdi:**\n",
    "```python\n",
    "X_train_features_list = [\n",
    "    {'sender_sc': 4, 'are_adjacent': 1, ...},  # Dictionary\n",
    "    {'sender_sc': 2, 'are_adjacent': 0, ...},\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### **Ã‡Ä±ktÄ±:**\n",
    "```python\n",
    "X_train_features_numeric = [\n",
    "    [1, 3, 2, 5, 4, 3],  # NumPy array\n",
    "    [0, 5, 3, -2, 2, 2],\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### **Neden Gerekli?**\n",
    "- Machine Learning modelleri **dictionary** anlayamaz\n",
    "- Sadece **sayÄ±sal matrixler** ile Ã§alÄ±ÅŸÄ±r\n",
    "- `DictVectorizer` kÃ¶prÃ¼ gÃ¶revi gÃ¶rÃ¼r\n",
    "\n",
    "### **Kritik Nokta:**\n",
    "- Training'de: `fit_transform()` â†’ Ã–ÄŸren ve dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "- Val/Test'de: `transform()` â†’ Sadece dÃ¶nÃ¼ÅŸtÃ¼r (aynÄ± mapping)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c4aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a3efe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ¯ LEXICON-BASED MODEL\n",
      "======================================================================\n",
      "TASK=SENDER, POWER=y\n",
      "\n",
      "ğŸ“– Lexicon yÃ¼kleniyor...\n",
      "   Aranan yol: c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\utils\\2015_Diplomacy_lexicon.json\n",
      "   âš ï¸  Lexicon dosyasÄ± bulunamadÄ±, manuel oluÅŸturuluyor...\n",
      "   âœ… Manuel lexicon oluÅŸturuldu (5 kategori)\n",
      "   ğŸ“š Toplam feature kategorisi: 7\n",
      "\n",
      "ğŸ“‚ Dataset yÃ¼kleniyor...\n",
      "   Train: âœ… c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl\n",
      "   Test:  âœ… c:\\Users\\aadil\\Desktop\\YAP470\\amazon-fake-review-detector\\notebooks\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl\n",
      "   âœ… Datasets yÃ¼klendi\n",
      "\n",
      "Aggregating messages...\n",
      "Converting data to lexicon features...\n",
      "Splitting features and labels...\n",
      "Scaling features...\n",
      "Feature matrix shape: (13132, 9)\n",
      "Training samples: 13132, Test samples: 2741\n",
      "Training Logistic Regression model...\n",
      "Evaluating model on test set...\n",
      "\n",
      "=============================================\n",
      "PERFORMANCE TEST: Logistic Regression (Lexicon, SENDER)\n",
      "=============================================\n",
      "F1 Score (Lie Class): 0.2269\n",
      "F1 Score (Macro Avg): 0.5230\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted 0 (Truth)  |  Predicted 1 (Lie)\n",
      "Actual 0:    1819                 | 682                 \n",
      "Actual 1:    122                  | 118                 \n",
      "\n",
      "Full Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (Truth)       0.94      0.73      0.82      2501\n",
      "  Class 1 (Lie)       0.15      0.49      0.23       240\n",
      "\n",
      "       accuracy                           0.71      2741\n",
      "      macro avg       0.54      0.61      0.52      2741\n",
      "   weighted avg       0.87      0.71      0.77      2741\n",
      "\n",
      "Splitting features and labels...\n",
      "Scaling features...\n",
      "Feature matrix shape: (13132, 9)\n",
      "Training samples: 13132, Test samples: 2741\n",
      "Training Logistic Regression model...\n",
      "Evaluating model on test set...\n",
      "\n",
      "=============================================\n",
      "PERFORMANCE TEST: Logistic Regression (Lexicon, SENDER)\n",
      "=============================================\n",
      "F1 Score (Lie Class): 0.2269\n",
      "F1 Score (Macro Avg): 0.5230\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted 0 (Truth)  |  Predicted 1 (Lie)\n",
      "Actual 0:    1819                 | 682                 \n",
      "Actual 1:    122                  | 118                 \n",
      "\n",
      "Full Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (Truth)       0.94      0.73      0.82      2501\n",
      "  Class 1 (Lie)       0.15      0.49      0.23       240\n",
      "\n",
      "       accuracy                           0.71      2741\n",
      "      macro avg       0.54      0.61      0.52      2741\n",
      "   weighted avg       0.87      0.71      0.77      2741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "# import spacy  <- REMOVED\n",
    "# from spacy.lang.en import English <- REMOVED\n",
    "import re # <- ADDED\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Global Settings & Initializer ---\n",
    "# We will set TASK and POWER from command-line args, just like the original\n",
    "# nlp = None # Will be initialized in main <- REMOVED\n",
    "\n",
    "# --- 2. Helper Functions (from harbringers.py) ---\n",
    "\n",
    "def regex_tokenizer(text): # <- RENAMED and RE-WRITTEN\n",
    "    \"\"\"\n",
    "    Tokenizes text using regular expressions.\n",
    "    This avoids the need for spacy.\n",
    "    It finds all sequences of word characters (letters, numbers, underscore),\n",
    "    which also effectively removes punctuation.\n",
    "    \"\"\"\n",
    "    # \\b matches a word boundary\n",
    "    # \\w+ matches one or more word characters (alphanumeric + underscore)\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "def aggregate(dataset):\n",
    "    \"\"\"Flattens dialogs into a list of single messages\"\"\"\n",
    "    messages = []\n",
    "    rec = []\n",
    "    send = []\n",
    "    power = []\n",
    "    for dialogs in dataset:\n",
    "        messages.extend(dialogs['messages'])\n",
    "        rec.extend(dialogs['receiver_labels'])\n",
    "        send.extend(dialogs['sender_labels'])\n",
    "        power.extend(dialogs['game_score_delta'])\n",
    "    \n",
    "    merged = []\n",
    "    for i, item in enumerate(messages):\n",
    "        merged.append({\n",
    "            'message': item,\n",
    "            'sender_annotation': send[i],\n",
    "            'receiver_annotation': rec[i],\n",
    "            'score_delta': int(power[i])\n",
    "        })\n",
    "    return merged\n",
    "\n",
    "def convert_to_features(dataset, feature_dict, TASK, POWER):\n",
    "    \"\"\"\n",
    "    This is the core of harbringers.py. It converts a message\n",
    "    into a vector of lexicon-based features.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for message in dataset:\n",
    "        \n",
    "        annotation_key = 'sender_annotation' if TASK == \"SENDER\" else 'receiver_annotation'\n",
    "        \n",
    "        # Skip messages with no label\n",
    "        if message[annotation_key] == \"NOANNOTATION\":\n",
    "            continue\n",
    "\n",
    "        features = []\n",
    "        \n",
    "        # 1. Lexicon Features\n",
    "        tokenized_message = regex_tokenizer(message['message'].lower()) # <- USE NEW FUNCTION\n",
    "        \n",
    "        for feature_category in feature_dict.keys():\n",
    "            total_count = 0\n",
    "            for word in feature_dict[feature_category]:\n",
    "                # Count all occurrences of this word\n",
    "                total_count += tokenized_message.count(word)\n",
    "            features.append(total_count)\n",
    "        \n",
    "        # 2. Power Features\n",
    "        if POWER == \"y\":\n",
    "            features.append(1 if message['score_delta'] > 4 else 0)\n",
    "            features.append(1 if message['score_delta'] < -4 else 0)\n",
    "        \n",
    "        # 3. Label (Appended last)\n",
    "        # We'll use our convention: Lie = 1, True = 0\n",
    "        if message[annotation_key] == False:\n",
    "            features.append(1) # Lie\n",
    "        else:\n",
    "            features.append(0) # True\n",
    "            \n",
    "        processed_data.append(features)\n",
    "        \n",
    "    return processed_data\n",
    "\n",
    "def split_xy(data):\n",
    "    \"\"\"Splits a list of lists into X (features) and y (labels)\"\"\"\n",
    "    X, y = [], []\n",
    "    for line in data:\n",
    "        X.append(line[:-1])  # All but the last item\n",
    "        y.append(line[-1])   # The last item\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    \"\"\"Prints a detailed performance report\"\"\"\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    # Use pos_label=1 to get F1 for the \"Lie\" class\n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    # Set zero_division=0 to avoid errors if a class is never predicted\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)'], zero_division=0))\n",
    "\n",
    "# --- 3. MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Parse Command-Line Arguments (like harbringers.py) ---\n",
    "    # This script defaults to 'sender' (s) and 'power' (y)\n",
    "    # You can run: python test_lexicon_model.py s n  (to turn off power)\n",
    "    # Or:          python test_lexicon_model.py r y  (to predict receiver labels)\n",
    "    \n",
    "    TASK = \"SENDER\"\n",
    "    POWER = \"y\"\n",
    "    \n",
    "    if len(sys.argv) >= 2:\n",
    "        if sys.argv[1] == 's':\n",
    "            TASK = \"SENDER\"\n",
    "        elif sys.argv[1] == 'r':\n",
    "            TASK = \"RECEIVER\"\n",
    "    if len(sys.argv) >= 3:\n",
    "        if sys.argv[2] == 'n':\n",
    "            POWER = 'n'\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ¯ LEXICON-BASED MODEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TASK={TASK}, POWER={POWER}\\n\")\n",
    "    \n",
    "    # --- DOÄRU YOLLAR (notebooks klasÃ¶rÃ¼nde) ---\n",
    "    DATA_DIR = os.path.join(os.getcwd(), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\")\n",
    "    LEXICON_PATH = os.path.join(os.getcwd(), \"data\", \"raw\", \"2020_acl_diplomacy\", \"utils\", \"2015_Diplomacy_lexicon.json\")\n",
    "    \n",
    "    # 1. Load Lexicon (veya manuel oluÅŸtur)\n",
    "    print(f\"ğŸ“– Lexicon yÃ¼kleniyor...\")\n",
    "    print(f\"   Aranan yol: {LEXICON_PATH}\")\n",
    "    \n",
    "    if os.path.exists(LEXICON_PATH):\n",
    "        with open(LEXICON_PATH, 'r') as f:\n",
    "            feature_dict = json.loads(f.readline())\n",
    "        print(f\"   âœ… Lexicon yÃ¼klendi\")\n",
    "    else:\n",
    "        # Lexicon yoksa manuel oluÅŸtur\n",
    "        print(f\"   âš ï¸  Lexicon dosyasÄ± bulunamadÄ±, manuel oluÅŸturuluyor...\")\n",
    "        feature_dict = {\n",
    "            'positive_words': ['trust', 'friend', 'ally', 'help', 'support', 'cooperate', 'agree', 'work'],\n",
    "            'negative_words': ['attack', 'betray', 'stab', 'invade', 'destroy', 'enemy', 'war'],\n",
    "            'certainty_words': ['definitely', 'certainly', 'absolutely', 'sure', 'guarantee'],\n",
    "            'tentative_words': ['maybe', 'perhaps', 'might', 'possibly', 'probably'],\n",
    "            'politeness': ['please', 'thank', 'sorry', 'apologize']\n",
    "        }\n",
    "        print(f\"   âœ… Manuel lexicon oluÅŸturuldu ({len(feature_dict)} kategori)\")\n",
    "    \n",
    "    # Custom features ekle\n",
    "    feature_dict['but'] = ['but']\n",
    "    feature_dict['countries'] = ['austria', 'england', 'france', 'germany', 'italy', 'russia', 'turkey']\n",
    "    print(f\"   ğŸ“š Toplam feature kategorisi: {len(feature_dict)}\\n\")\n",
    "\n",
    "    # 2. Load Datasets\n",
    "    print(f\"ğŸ“‚ Dataset yÃ¼kleniyor...\")\n",
    "    TRAIN_FILE = os.path.join(DATA_DIR, 'train.jsonl')\n",
    "    TEST_FILE = os.path.join(DATA_DIR, 'test.jsonl')\n",
    "    \n",
    "    print(f\"   Train: {'âœ…' if os.path.exists(TRAIN_FILE) else 'âŒ'} {TRAIN_FILE}\")\n",
    "    print(f\"   Test:  {'âœ…' if os.path.exists(TEST_FILE) else 'âŒ'} {TEST_FILE}\")\n",
    "    \n",
    "    if not os.path.exists(TRAIN_FILE):\n",
    "        print(f\"\\nâŒ HATA: Diplomacy dataset dosyalarÄ± bulunamadÄ±!\")\n",
    "        print(f\"   Mevcut dizin: {os.getcwd()}\")\n",
    "        print(f\"\\nğŸ’¡ Bu model Diplomacy dataset'i iÃ§in tasarlanmÄ±ÅŸ.\")\n",
    "        print(f\"   Amazon dataset iÃ§in notebooks/amazon_models/ kullanÄ±n.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        with jsonlines.open(TRAIN_FILE, 'r') as reader:\n",
    "            train_data_raw = list(reader)\n",
    "        with jsonlines.open(TEST_FILE, 'r') as reader:\n",
    "            test_data_raw = list(reader)\n",
    "        print(f\"   âœ… Datasets yÃ¼klendi\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ YÃ¼kleme hatasÄ±: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. Process Data (Aggregate -> Convert to Features -> Split)\n",
    "    print(\"Aggregating messages...\")\n",
    "    train_agg = aggregate(train_data_raw)\n",
    "    test_agg = aggregate(test_data_raw)\n",
    "    \n",
    "    print(\"Converting data to lexicon features...\")\n",
    "    train_converted = convert_to_features(train_agg, feature_dict, TASK, POWER)\n",
    "    test_converted = convert_to_features(test_agg, feature_dict, TASK, POWER)\n",
    "    \n",
    "    print(\"Splitting features and labels...\")\n",
    "    X_train, y_train = split_xy(train_converted)\n",
    "    X_test, y_test = split_xy(test_converted)\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(\"CRITICAL ERROR: No data was processed.\")\n",
    "        print(\"This might happen if 'NOANNOTATION' is the only label.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 4. Scale Features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Training samples: {len(y_train)}, Test samples: {len(y_test)}\")\n",
    "\n",
    "    # 5. Train Model\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    # class_weight='balanced' is CRITICAL for this imbalanced dataset\n",
    "    logmodel = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    logmodel.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 6. Evaluate\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    y_pred = logmodel.predict(X_test_scaled)\n",
    "    evaluate_model(f\"Logistic Regression (Lexicon, {TASK})\", y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e08d84",
   "metadata": {},
   "source": [
    "NEURAL NETWORK TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae4cf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Starting to build game state dictionary from: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "Successfully loaded and indexed 342 game state files.\n",
      "Loading all datasets...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n",
      "Game state feature names: ['mentions_origin_unit', 'mentions_own_territory', 'mentions_receiver_territory', 'mentions_target_territory', 'receiver_sc', 'receiver_unit_count', 'score_delta', 'sender_sc', 'sender_unit_count', 'territory_mention_count']\n",
      "Game state data shape: (13132, 10)\n",
      "\n",
      "Building Neural Network Model...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n",
      "Game state feature names: ['mentions_origin_unit', 'mentions_own_territory', 'mentions_receiver_territory', 'mentions_target_territory', 'receiver_sc', 'receiver_unit_count', 'score_delta', 'sender_sc', 'sender_unit_count', 'territory_mention_count']\n",
      "Game state data shape: (13132, 10)\n",
      "\n",
      "Building Neural Network Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_input          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_1         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">640,064</span> â”‚ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d_1 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚ spatial_dropout1â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ game_state_input    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lstm_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ game_state_inputâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,800</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output_layer        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ text_input          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_1         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)   â”‚    \u001b[38;5;34m640,064\u001b[0m â”‚ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d_1 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚     \u001b[38;5;34m33,024\u001b[0m â”‚ spatial_dropout1â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ game_state_input    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lstm_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ game_state_inputâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m4,800\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output_layer        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         â”‚         \u001b[38;5;34m65\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">677,953</span> (2.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m677,953\u001b[0m (2.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">677,953</span> (2.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m677,953\u001b[0m (2.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights (for balancing): {0: np.float64(0.5235627142971055), 1: np.float64(11.109983079526227)}\n",
      "\n",
      "Training the model...\n",
      "Epoch 1/15\n",
      "Epoch 1/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 0.6430 - loss: 0.6821 - val_accuracy: 0.5395 - val_loss: 0.7376\n",
      "Epoch 2/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 0.6430 - loss: 0.6821 - val_accuracy: 0.5395 - val_loss: 0.7376\n",
      "Epoch 2/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - accuracy: 0.6550 - loss: 0.6664 - val_accuracy: 0.7874 - val_loss: 0.6766\n",
      "Epoch 3/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - accuracy: 0.6550 - loss: 0.6664 - val_accuracy: 0.7874 - val_loss: 0.6766\n",
      "Epoch 3/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6643 - loss: 0.6721 - val_accuracy: 0.6384 - val_loss: 0.6981\n",
      "Epoch 4/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6643 - loss: 0.6721 - val_accuracy: 0.6384 - val_loss: 0.6981\n",
      "Epoch 4/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - accuracy: 0.6525 - loss: 0.6678 - val_accuracy: 0.6695 - val_loss: 0.6836\n",
      "Epoch 5/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - accuracy: 0.6525 - loss: 0.6678 - val_accuracy: 0.6695 - val_loss: 0.6836\n",
      "Epoch 5/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6457 - loss: 0.6628 - val_accuracy: 0.7225 - val_loss: 0.6412\n",
      "Epoch 6/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.6457 - loss: 0.6628 - val_accuracy: 0.7225 - val_loss: 0.6412\n",
      "Epoch 6/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.6954 - loss: 0.6572 - val_accuracy: 0.6370 - val_loss: 0.7080\n",
      "Epoch 7/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - accuracy: 0.6954 - loss: 0.6572 - val_accuracy: 0.6370 - val_loss: 0.7080\n",
      "Epoch 7/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6538 - loss: 0.6553 - val_accuracy: 0.7105 - val_loss: 0.6564\n",
      "Epoch 8/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6538 - loss: 0.6553 - val_accuracy: 0.7105 - val_loss: 0.6564\n",
      "Epoch 8/15\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6892 - loss: 0.6558 - val_accuracy: 0.7140 - val_loss: 0.6526\n",
      "\u001b[1m206/206\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - accuracy: 0.6892 - loss: 0.6558 - val_accuracy: 0.7140 - val_loss: 0.6526\n",
      "\n",
      "Evaluating final model on test set...\n",
      "\n",
      "Evaluating final model on test set...\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "\n",
      "=============================================\n",
      "PERFORMANCE TEST: Dual-Input Neural Network\n",
      "=============================================\n",
      "F1 Score (Lie Class): 0.1993\n",
      "F1 Score (Macro Avg): 0.4917\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted 0 (Truth)  |  Predicted 1 (Lie)\n",
      "Actual 0:    1693                 | 808                 \n",
      "Actual 1:    124                  | 116                 \n",
      "\n",
      "Full Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (Truth)       0.93      0.68      0.78      2501\n",
      "  Class 1 (Lie)       0.13      0.48      0.20       240\n",
      "\n",
      "       accuracy                           0.66      2741\n",
      "      macro avg       0.53      0.58      0.49      2741\n",
      "   weighted avg       0.86      0.66      0.73      2741\n",
      "\n",
      "\n",
      "=============================================\n",
      "PERFORMANCE TEST: Dual-Input Neural Network\n",
      "=============================================\n",
      "F1 Score (Lie Class): 0.1993\n",
      "F1 Score (Macro Avg): 0.4917\n",
      "\n",
      "Confusion Matrix:\n",
      "           Predicted 0 (Truth)  |  Predicted 1 (Lie)\n",
      "Actual 0:    1693                 | 808                 \n",
      "Actual 1:    124                  | 116                 \n",
      "\n",
      "Full Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Class 0 (Truth)       0.93      0.68      0.78      2501\n",
      "  Class 1 (Lie)       0.13      0.48      0.20       240\n",
      "\n",
      "       accuracy                           0.66      2741\n",
      "      macro avg       0.53      0.58      0.49      2741\n",
      "   weighted avg       0.86      0.66      0.73      2741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure TensorFlow is installed in the notebook environment\n",
    "%pip install tensorflow --quiet\n",
    "\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. NEW: EXPERT KNOWLEDGE MAPPING ---\n",
    "\n",
    "# This map is our \"secret weapon\". It connects text (\"paris\") to game data (\"Par\").\n",
    "NAME_TO_ABBREVIATION = {\n",
    "    'north atlantic ocean': 'Nao', 'nao': 'Nao',\n",
    "    'irish sea': 'Iri', 'iri': 'Iri',\n",
    "    'english channel': 'Eng', 'eng': 'Eng', 'channel': 'Eng',\n",
    "    'mid-atlantic ocean': 'Mao', 'mao': 'Mao', 'mid atlantic': 'Mao',\n",
    "    'north sea': 'Nth', 'nth': 'Nth',\n",
    "    'norwegian sea': 'Nwg', 'nwg': 'Nwg',\n",
    "    'barents sea': 'Bar', 'bar': 'Bar',\n",
    "    'helgoland bight': 'Hel', 'hel': 'Hel',\n",
    "    'skagerrak': 'Ska', 'ska': 'Ska',\n",
    "    'baltic sea': 'Bal', 'bal': 'Bal',\n",
    "    'gulf of bothnia': 'Bot', 'bot': 'Bot',\n",
    "    'western mediterranean': 'Wes', 'wes': 'Wes',\n",
    "    'gulf of lyon': 'Lyo', 'lyo': 'Lyo',\n",
    "    'tyrrhenian sea': 'Tyn', 'tyn': 'Tyn',\n",
    "    'ionian sea': 'Ion', 'ion': 'Ion',\n",
    "    'adriatic sea': 'Adr', 'adr': 'Adr',\n",
    "    'aegean sea': 'Aeg', 'aeg': 'Aeg',\n",
    "    'eastern mediterranean': 'Eas', 'eas': 'Eas',\n",
    "    'black sea': 'Bla', 'bla': 'Bla',\n",
    "    \n",
    "    'clyde': 'Cly', 'cly': 'Cly',\n",
    "    'edinburgh': 'Edi', 'edi': 'Edi',\n",
    "    'liverpool': 'Lvp', 'lvp': 'Lvp',\n",
    "    'yorkshire': 'Yor', 'yor': 'Yor',\n",
    "    'wales': 'Wal', 'wal': 'Wal',\n",
    "    'london': 'Lon', 'lon': 'Lon',\n",
    "    'belgium': 'Bel', 'bel': 'Bel',\n",
    "    'holland': 'Hol', 'hol': 'Hol',\n",
    "    'denmark': 'Den', 'den': 'Den',\n",
    "    'sweden': 'Swe', 'swe': 'Swe',\n",
    "    'norway': 'Nwy', 'nwy': 'Nwy',\n",
    "    'finland': 'Fin', 'fin': 'Fin',\n",
    "    'st petersburg': 'Stp', 'stp': 'Stp',\n",
    "    'livonia': 'Lvn', 'lvn': 'Lvn',\n",
    "    'prussia': 'Pru', 'pru': 'Pru',\n",
    "    'kiel': 'Kie', 'kie': 'Kie',\n",
    "    'berlin': 'Ber', 'ber': 'Ber',\n",
    "    'silesia': 'Sil', 'sil': 'Sil',\n",
    "    'poland': 'War', 'war': 'War', 'warsaw': 'War',\n",
    "    'moscow': 'Mos', 'mos': 'Mos',\n",
    "    'ukraine': 'Ukr', 'ukr': 'Ukr',\n",
    "    'sevastopol': 'Sev', 'sev': 'Sev',\n",
    "    'ruhr': 'Ruh', 'ruh': 'Ruh',\n",
    "    'munich': 'Mun', 'mun': 'Mun',\n",
    "    'bohemia': 'Boh', 'boh': 'Boh',\n",
    "    'galicia': 'Gal', 'gal': 'Gal',\n",
    "    'brest': 'Bre', 'bre': 'Bre',\n",
    "    'picardy': 'Pic', 'pic': 'Pic',\n",
    "    'paris': 'Par', 'par': 'Par',\n",
    "    'burgundy': 'Bur', 'bur': 'Bur',\n",
    "    'gascony': 'Gas', 'gas': 'Gas',\n",
    "    'marseilles': 'Mar', 'mar': 'Mar',\n",
    "    'spain': 'Spa', 'spa': 'Spa',\n",
    "    'portugal': 'Por', 'por': 'Por',\n",
    "    'piedmont': 'Pie', 'pie': 'Pie',\n",
    "    'tyrolia': 'Tyr', 'tyr': 'Tyr',\n",
    "    'vienna': 'Vie', 'vie': 'Vie',\n",
    "    'trieste': 'Tri', 'tri': 'Tri',\n",
    "    'budapest': 'Bud', 'bud': 'Bud',\n",
    "    'rumania': 'Rum', 'rum': 'Rum',\n",
    "    'serbia': 'Ser', 'ser': 'Ser',\n",
    "    'venice': 'Ven', 'ven': 'Ven',\n",
    "    'rome': 'Rom', 'rom': 'Rom',\n",
    "    'apulia': 'Apu', 'apu': 'Apu',\n",
    "    'naples': 'Nap', 'nap': 'Nap',\n",
    "    'albania': 'Alb', 'alb': 'Alb',\n",
    "    'greece': 'Gre', 'gre': 'Gre',\n",
    "    'bulgaria': 'Bul', 'bul': 'Bul',\n",
    "    'constantinople': 'Con', 'con': 'Con',\n",
    "    'ankara': 'Ank', 'ank': 'Ank',\n",
    "    'armenia': 'Arm', 'arm': 'Arm',\n",
    "    'smyrna': 'Smy', 'smy': 'Smy',\n",
    "    'syria': 'Syr', 'syr': 'Syr',\n",
    "    'north africa': 'Naf', 'naf': 'Naf',\n",
    "    'tunis': 'Tun', 'tun': 'Tun'\n",
    "}\n",
    "\n",
    "# This is a helper set for the new tokenizer\n",
    "# We find all unique \"words\" in our map's keys\n",
    "# e.g., \"north\", \"atlantic\", \"ocean\", \"nao\", \"irish\", \"sea\", \"iri\", etc.\n",
    "TEXT_LOOKUP_WORDS = set()\n",
    "for key in NAME_TO_ABBREVIATION.keys():\n",
    "    TEXT_LOOKUP_WORDS.update(key.split())\n",
    "\n",
    "\n",
    "# --- 2. GAME STATE LOADING (Same as before) ---\n",
    "def create_game_state_dictionary(game_state_dir):\n",
    "    filename_pattern = re.compile(r\"DiplomacyGame(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "    game_states = defaultdict(lambda: defaultdict(dict))\n",
    "    print(f\"Starting to build game state dictionary from: {game_state_dir}\")\n",
    "    file_count = 0\n",
    "    if not os.path.exists(game_state_dir):\n",
    "        print(f\"Error: Directory not found: {game_state_dir}\")\n",
    "        return None\n",
    "    for filename in os.listdir(game_state_dir):\n",
    "        match = filename_pattern.match(filename)\n",
    "        if match:\n",
    "            game_id = int(match.group(1))\n",
    "            year = match.group(2)\n",
    "            season = match.group(3).lower() # <-- Store key as lowercase\n",
    "            file_path = os.path.join(game_state_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    game_states[game_id][year][season] = data\n",
    "                    file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load or parse {filename}. Error: {e}\")\n",
    "    print(f\"Successfully loaded and indexed {file_count} game state files.\")\n",
    "    return game_states\n",
    "\n",
    "# --- 3. NEW: TEXT-TO-STATE FEATURE EXTRACTION ---\n",
    "\n",
    "def find_mentioned_territories_abbreviations(message_text, lookup_map, lookup_words):\n",
    "    \"\"\"\n",
    "    Finds all unique territory ABBREVIATIONS (e.g., 'Par') mentioned\n",
    "    in a raw text message (e.g., \"I'm going to Paris\").\n",
    "    \"\"\"\n",
    "    # Tokenize the message by finding all words\n",
    "    words = re.findall(r'\\b\\w+\\b', message_text.lower())\n",
    "    \n",
    "    # Filter for words that are part of our map to speed things up\n",
    "    potential_words = [w for w in words if w in lookup_words]\n",
    "    \n",
    "    if not potential_words:\n",
    "        return set()\n",
    "\n",
    "    text = \" \".join(potential_words)\n",
    "    mentioned_abbreviations = set()\n",
    "    \n",
    "    # Check for multi-word keys first (e.g., \"north atlantic ocean\")\n",
    "    for name, abbr in lookup_map.items():\n",
    "        if \" \" in name and name in text:\n",
    "            mentioned_abbreviations.add(abbr)\n",
    "            \n",
    "    # Check for single-word keys (e.g., \"paris\" or \"par\")\n",
    "    for word in potential_words:\n",
    "        if word in lookup_map:\n",
    "            mentioned_abbreviations.add(lookup_map[word])\n",
    "            \n",
    "    return mentioned_abbreviations\n",
    "\n",
    "def extract_features_from_state(message_data, game_state):\n",
    "    \"\"\"\n",
    "    The master feature function. Creates all game-state and\n",
    "    text-to-state contradiction features.\n",
    "    \"\"\"\n",
    "    new_features = {}\n",
    "    \n",
    "    sender = message_data['speaker'].lower()\n",
    "    receiver = message_data['receiver'].lower()\n",
    "    message_text = message_data['text']\n",
    "    \n",
    "    # --- Get state data ---\n",
    "    territories_data = game_state.get('territories', {})\n",
    "    orders_data = game_state.get('orders', {})\n",
    "    sc_text = game_state.get('sc', '')\n",
    "    \n",
    "    # === 1. & 2. SC and Unit Counts (Same as before) ===\n",
    "    sc_pattern = re.compile(r\"\\s*(\\w+)\\s+(\\d+)\\s*\")\n",
    "    all_scores = {}\n",
    "    for match in sc_pattern.finditer(sc_text):\n",
    "        country = match.group(1).lower()\n",
    "        score = int(match.group(2))\n",
    "        all_scores[country] = score\n",
    "    new_features['sender_sc'] = all_scores.get(sender, 0)\n",
    "    new_features['receiver_sc'] = all_scores.get(receiver, 0)\n",
    "\n",
    "    sender_units_orders = orders_data.get(sender.upper(), {})\n",
    "    receiver_units_orders = orders_data.get(receiver.upper(), {})\n",
    "    new_features['sender_unit_count'] = len(sender_units_orders)\n",
    "    new_features['receiver_unit_count'] = len(receiver_units_orders)\n",
    "    \n",
    "    # === 3. Score Delta (Same as before) ===\n",
    "    new_features['score_delta'] = message_data['score_delta']\n",
    "    \n",
    "    # === 4. Adjacency (Same as before) ===\n",
    "    # This requires a global DIPLOMACY_MAP, which we'll define.\n",
    "    # We'll just hard-code this feature for now to avoid the map.\n",
    "    # is_adjacent = are_players_adjacent(sender, receiver, territories_data, DIPLOMACY_MAP)\n",
    "    # new_features['are_adjacent'] = 1 if is_adjacent else 0\n",
    "    \n",
    "    # === 5. NEW: Text-to-State & Contradiction Features ===\n",
    "    \n",
    "    # Find all territories mentioned in the message\n",
    "    mentioned_abbreviations = find_mentioned_territories_abbreviations(\n",
    "        message_text, NAME_TO_ABBREVIATION, TEXT_LOOKUP_WORDS\n",
    "    )\n",
    "    new_features['territory_mention_count'] = len(mentioned_abbreviations)\n",
    "\n",
    "    # Find all territories owned by sender and receiver\n",
    "    sender_owned_territories = set()\n",
    "    receiver_owned_territories = set()\n",
    "    for territory, owner in territories_data.items():\n",
    "        if owner.lower() == sender:\n",
    "            sender_owned_territories.add(territory.upper()) # Abbreviations are case-insensitive\n",
    "        elif owner.lower() == receiver:\n",
    "            receiver_owned_territories.add(territory.upper())\n",
    "            \n",
    "    # Find all territories involved in sender's orders\n",
    "    origin_territories = set(sender_units_orders.keys()) # e.g., {'Arm', 'Bul', 'Ank'}\n",
    "    target_territories = set()\n",
    "    for unit, order in sender_units_orders.items():\n",
    "        if 'to' in order:\n",
    "            target_territories.add(order['to'].upper()) # e.g., {'Sev', 'Rum'}\n",
    "\n",
    "    # Create the new features (as 1s or 0s)\n",
    "    new_features['mentions_own_territory'] = 0\n",
    "    new_features['mentions_receiver_territory'] = 0\n",
    "    new_features['mentions_origin_unit'] = 0\n",
    "    new_features['mentions_target_territory'] = 0\n",
    "    \n",
    "    for abbr in mentioned_abbreviations:\n",
    "        abbr_upper = abbr.upper()\n",
    "        if abbr_upper in sender_owned_territories:\n",
    "            new_features['mentions_own_territory'] = 1\n",
    "        if abbr_upper in receiver_owned_territories:\n",
    "            new_features['mentions_receiver_territory'] = 1\n",
    "        if abbr_upper in origin_territories:\n",
    "            new_features['mentions_origin_unit'] = 1\n",
    "        if abbr_upper in target_territories:\n",
    "            new_features['mentions_target_territory'] = 1\n",
    "            \n",
    "    return new_features\n",
    "\n",
    "# --- 4. DATA LOADING ---\n",
    "\n",
    "def load_and_flatten_data(filepath, game_state_lookup):\n",
    "    X_text_list = []\n",
    "    y_list = []\n",
    "    X_new_features_list = [] \n",
    "\n",
    "    with jsonlines.open(filepath, 'r') as reader:\n",
    "        for game in reader:\n",
    "            game_id = game['game_id']\n",
    "            years = game['years']\n",
    "            seasons = game['seasons']\n",
    "            speakers = game['speakers']\n",
    "            \n",
    "            for i in range(len(game['messages'])):\n",
    "                sender_label = game['sender_labels'][i]\n",
    "                if sender_label == \"NOANNOTATION\":\n",
    "                    continue\n",
    "                \n",
    "                label = 1 if sender_label == False else 0\n",
    "                message_text = game['messages'][i]\n",
    "                year = years[i]\n",
    "                season = seasons[i].lower() # <-- Use lowercase for lookup\n",
    "                \n",
    "                try:\n",
    "                    current_game_state = game_state_lookup[game_id][year][season]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "                message_data = {\n",
    "                    'text': message_text,\n",
    "                    'speaker': speakers[i],\n",
    "                    'receiver': game['receivers'][i],\n",
    "                    'score_delta': int(game['game_score_delta'][i])\n",
    "                }\n",
    "                \n",
    "                new_features = extract_features_from_state(message_data, current_game_state)\n",
    "                \n",
    "                X_text_list.append(message_text)\n",
    "                y_list.append(label)\n",
    "                X_new_features_list.append(new_features)\n",
    "\n",
    "    print(f\"Loaded and processed {len(X_text_list)} samples from {filepath}.\")\n",
    "    return X_text_list, X_new_features_list, np.array(y_list)\n",
    "\n",
    "# --- 5. MODEL EVALUATION ---\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred_probs): # y_pred_probs is an array of floats\n",
    "    \"\"\"Prints a detailed performance report\"\"\"\n",
    "    \n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Convert continuous probabilities (e.g., 0.73) into binary\n",
    "    # class labels (e.g., 1) using a 0.5 threshold.\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Now, y_true and y_pred are both binary (0s and 1s)\n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred) # Use the binary y_pred\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)'], zero_division=0))\n",
    "\n",
    "# --- 6. NEW: NEURAL NETWORK PIPELINE ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Set Paths ---\n",
    "    GAME_STATE_DIRECTORY = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\") \n",
    "\n",
    "    TRAIN_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\") \n",
    "    VAL_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"validation.jsonl\") \n",
    "    TEST_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"test.jsonl\") \n",
    "    \n",
    "    VOCAB_SIZE = 10000\n",
    "    MAX_LEN = 120\n",
    "    EMBEDDING_DIM = 64\n",
    "    LSTM_UNITS = 64\n",
    "\n",
    "    # --- Load Game State Dictionary ---\n",
    "    game_state_lookup = create_game_state_dictionary(GAME_STATE_DIRECTORY)\n",
    "    if not game_state_lookup:\n",
    "        print(\"Game state lookup dictionary is empty. Exiting.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # --- Load all data ---\n",
    "    print(\"Loading all datasets...\")\n",
    "    X_train_text, X_train_features_list, y_train = load_and_flatten_data(TRAIN_FILE, game_state_lookup)\n",
    "    X_val_text, X_val_features_list, y_val = load_and_flatten_data(VAL_FILE, game_state_lookup)\n",
    "    X_test_text, X_test_features_list, y_test = load_and_flatten_data(TEST_FILE, game_state_lookup)\n",
    "\n",
    "    if len(X_train_text) == 0:\n",
    "        print(\"CRITICAL ERROR: No training data was loaded.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 1. PREPARE TEXT INPUT (Tokenizer + Padding) ---\n",
    "    print(\"\\nPreprocessing Text Data...\")\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train_text) # Fit ONLY on training text\n",
    "    \n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val_text)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "    \n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    print(f\"Text data shape: {X_train_pad.shape}\")\n",
    "\n",
    "    # --- 2. PREPARE GAME STATE INPUT (DictVectorizer + Scaling) ---\n",
    "    print(\"Preprocessing Game State Data...\")\n",
    "    feature_vectorizer = DictVectorizer(sparse=False)  # Use dense arrays instead of sparse\n",
    "    X_train_features_numeric = feature_vectorizer.fit_transform(X_train_features_list)\n",
    "    X_val_features_numeric = feature_vectorizer.transform(X_val_features_list)\n",
    "    X_test_features_numeric = feature_vectorizer.transform(X_test_features_list)\n",
    "    \n",
    "    scaler = StandardScaler()  # Now works with dense arrays\n",
    "    X_train_features_scaled = scaler.fit_transform(X_train_features_numeric)\n",
    "    X_val_features_scaled = scaler.transform(X_val_features_numeric)\n",
    "    X_test_features_scaled = scaler.transform(X_test_features_numeric)\n",
    "    \n",
    "    num_game_features = X_train_features_scaled.shape[1]\n",
    "    print(f\"Game state feature names: {feature_vectorizer.feature_names_}\")\n",
    "    print(f\"Game state data shape: {X_train_features_scaled.shape}\")\n",
    "\n",
    "    # --- 3. DEFINE THE DUAL-INPUT MODEL ---\n",
    "    print(\"\\nBuilding Neural Network Model...\")\n",
    "    \n",
    "    # Input 1: Text Sequences\n",
    "    text_input_layer = Input(shape=(MAX_LEN,), name='text_input')\n",
    "    text_model = Embedding(input_dim=VOCAB_SIZE+1, output_dim=EMBEDDING_DIM, input_length=MAX_LEN)(text_input_layer)\n",
    "    text_model = SpatialDropout1D(0.2)(text_model) # Dropout for embeddings\n",
    "    text_model = LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2, name='lstm_layer')(text_model)\n",
    "    \n",
    "    # Input 2: Game State Features\n",
    "    game_state_input_layer = Input(shape=(num_game_features,), name='game_state_input')\n",
    "    \n",
    "    # Combine (Concatenate)\n",
    "    combined = concatenate([text_model, game_state_input_layer], name='concatenate')\n",
    "    \n",
    "    # Classifier Head\n",
    "    output = Dense(64, activation='relu', name='dense_1')(combined)\n",
    "    output = Dropout(0.5, name='dropout')(output)\n",
    "    output = Dense(1, activation='sigmoid', name='output_layer')(output)\n",
    "    \n",
    "    model = Model(inputs=[text_input_layer, game_state_input_layer], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # --- 4. HANDLE CLASS IMBALANCE ---\n",
    "    # This is CRITICAL for getting a good F1 score\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(f\"Class Weights (for balancing): {class_weight_dict}\")\n",
    "\n",
    "    # --- 5. TRAIN THE MODEL ---\n",
    "    print(\"\\nTraining the model...\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=3,         # Stop after 3 epochs with no improvement\n",
    "        restore_best_weights=True # Keep the best model, not the last\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_train_pad, X_train_features_scaled], # Pass inputs as a list\n",
    "        y_train,\n",
    "        epochs=15,\n",
    "        batch_size=64,\n",
    "        validation_data=([X_val_pad, X_val_features_scaled], y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight_dict # Apply the weights\n",
    "    )\n",
    "\n",
    "    # --- 6. EVALUATE THE FINAL MODEL ---\n",
    "    print(\"\\nEvaluating final model on test set...\")\n",
    "    y_pred_probs = model.predict([X_test_pad, X_test_features_scaled])\n",
    "    evaluate_model(\"Dual-Input Neural Network\", y_test, y_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b5e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build game state dictionary from: c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\moves\n",
      "Successfully loaded and indexed 342 game state files.\n",
      "Loading all datasets...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Loaded and processed 13132 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\train.jsonl.\n",
      "Loaded and processed 1416 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\validation.jsonl.\n",
      "Loaded and processed 2741 samples from c:\\work environment\\Projects\\amazon-spam-review\\data\\raw\\2020_acl_diplomacy\\data\\test.jsonl.\n",
      "\n",
      "Preprocessing Text Data...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n",
      "Text data shape: (13132, 120)\n",
      "Preprocessing Game State Data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 380\u001b[0m\n\u001b[0;32m    377\u001b[0m X_test_features_numeric \u001b[38;5;241m=\u001b[39m feature_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test_features_list)\n\u001b[0;32m    379\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler() \u001b[38;5;66;03m# NNs work best with scaled data\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m X_train_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train_features_numeric)\n\u001b[0;32m    381\u001b[0m X_val_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_val_features_numeric)\n\u001b[0;32m    382\u001b[0m X_test_features_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test_features_numeric)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:959\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n\u001b[1;32m--> 959\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    960\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot center sparse matrices: pass `with_mean=False` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    961\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead. See docstring for motivation and alternatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     sparse_constructor \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    964\u001b[0m         sparse\u001b[38;5;241m.\u001b[39mcsr_matrix \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39mcsc_matrix\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[0;32m    968\u001b[0m         \u001b[38;5;66;03m# First pass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. NEW: EXPERT KNOWLEDGE MAPPING ---\n",
    "\n",
    "# This map is our \"secret weapon\". It connects text (\"paris\") to game data (\"Par\").\n",
    "NAME_TO_ABBREVIATION = {\n",
    "    'north atlantic ocean': 'Nao', 'nao': 'Nao',\n",
    "    'irish sea': 'Iri', 'iri': 'Iri',\n",
    "    'english channel': 'Eng', 'eng': 'Eng', 'channel': 'Eng',\n",
    "    'mid-atlantic ocean': 'Mao', 'mao': 'Mao', 'mid atlantic': 'Mao',\n",
    "    'north sea': 'Nth', 'nth': 'Nth',\n",
    "    'norwegian sea': 'Nwg', 'nwg': 'Nwg',\n",
    "    'barents sea': 'Bar', 'bar': 'Bar',\n",
    "    'helgoland bight': 'Hel', 'hel': 'Hel',\n",
    "    'skagerrak': 'Ska', 'ska': 'Ska',\n",
    "    'baltic sea': 'Bal', 'bal': 'Bal',\n",
    "    'gulf of bothnia': 'Bot', 'bot': 'Bot',\n",
    "    'western mediterranean': 'Wes', 'wes': 'Wes',\n",
    "    'gulf of lyon': 'Lyo', 'lyo': 'Lyo',\n",
    "    'tyrrhenian sea': 'Tyn', 'tyn': 'Tyn',\n",
    "    'ionian sea': 'Ion', 'ion': 'Ion',\n",
    "    'adriatic sea': 'Adr', 'adr': 'Adr',\n",
    "    'aegean sea': 'Aeg', 'aeg': 'Aeg',\n",
    "    'eastern mediterranean': 'Eas', 'eas': 'Eas',\n",
    "    'black sea': 'Bla', 'bla': 'Bla',\n",
    "    \n",
    "    'clyde': 'Cly', 'cly': 'Cly',\n",
    "    'edinburgh': 'Edi', 'edi': 'Edi',\n",
    "    'liverpool': 'Lvp', 'lvp': 'Lvp',\n",
    "    'yorkshire': 'Yor', 'yor': 'Yor',\n",
    "    'wales': 'Wal', 'wal': 'Wal',\n",
    "    'london': 'Lon', 'lon': 'Lon',\n",
    "    'belgium': 'Bel', 'bel': 'Bel',\n",
    "    'holland': 'Hol', 'hol': 'Hol',\n",
    "    'denmark': 'Den', 'den': 'Den',\n",
    "    'sweden': 'Swe', 'swe': 'Swe',\n",
    "    'norway': 'Nwy', 'nwy': 'Nwy',\n",
    "    'finland': 'Fin', 'fin': 'Fin',\n",
    "    'st petersburg': 'Stp', 'stp': 'Stp',\n",
    "    'livonia': 'Lvn', 'lvn': 'Lvn',\n",
    "    'prussia': 'Pru', 'pru': 'Pru',\n",
    "    'kiel': 'Kie', 'kie': 'Kie',\n",
    "    'berlin': 'Ber', 'ber': 'Ber',\n",
    "    'silesia': 'Sil', 'sil': 'Sil',\n",
    "    'poland': 'War', 'war': 'War', 'warsaw': 'War',\n",
    "    'moscow': 'Mos', 'mos': 'Mos',\n",
    "    'ukraine': 'Ukr', 'ukr': 'Ukr',\n",
    "    'sevastopol': 'Sev', 'sev': 'Sev',\n",
    "    'ruhr': 'Ruh', 'ruh': 'Ruh',\n",
    "    'munich': 'Mun', 'mun': 'Mun',\n",
    "    'bohemia': 'Boh', 'boh': 'Boh',\n",
    "    'galicia': 'Gal', 'gal': 'Gal',\n",
    "    'brest': 'Bre', 'bre': 'Bre',\n",
    "    'picardy': 'Pic', 'pic': 'Pic',\n",
    "    'paris': 'Par', 'par': 'Par',\n",
    "    'burgundy': 'Bur', 'bur': 'Bur',\n",
    "    'gascony': 'Gas', 'gas': 'Gas',\n",
    "    'marseilles': 'Mar', 'mar': 'Mar',\n",
    "    'spain': 'Spa', 'spa': 'Spa',\n",
    "    'portugal': 'Por', 'por': 'Por',\n",
    "    'piedmont': 'Pie', 'pie': 'Pie',\n",
    "    'tyrolia': 'Tyr', 'tyr': 'Tyr',\n",
    "    'vienna': 'Vie', 'vie': 'Vie',\n",
    "    'trieste': 'Tri', 'tri': 'Tri',\n",
    "    'budapest': 'Bud', 'bud': 'Bud',\n",
    "    'rumania': 'Rum', 'rum': 'Rum',\n",
    "    'serbia': 'Ser', 'ser': 'Ser',\n",
    "    'venice': 'Ven', 'ven': 'Ven',\n",
    "    'rome': 'Rom', 'rom': 'Rom',\n",
    "    'apulia': 'Apu', 'apu': 'Apu',\n",
    "    'naples': 'Nap', 'nap': 'Nap',\n",
    "    'albania': 'Alb', 'alb': 'Alb',\n",
    "    'greece': 'Gre', 'gre': 'Gre',\n",
    "    'bulgaria': 'Bul', 'bul': 'Bul',\n",
    "    'constantinople': 'Con', 'con': 'Con',\n",
    "    'ankara': 'Ank', 'ank': 'Ank',\n",
    "    'armenia': 'Arm', 'arm': 'Arm',\n",
    "    'smyrna': 'Smy', 'smy': 'Smy',\n",
    "    'syria': 'Syr', 'syr': 'Syr',\n",
    "    'north africa': 'Naf', 'naf': 'Naf',\n",
    "    'tunis': 'Tun', 'tun': 'Tun'\n",
    "}\n",
    "\n",
    "# This is a helper set for the new tokenizer\n",
    "# We find all unique \"words\" in our map's keys\n",
    "# e.g., \"north\", \"atlantic\", \"ocean\", \"nao\", \"irish\", \"sea\", \"iri\", etc.\n",
    "TEXT_LOOKUP_WORDS = set()\n",
    "for key in NAME_TO_ABBREVIATION.keys():\n",
    "    TEXT_LOOKUP_WORDS.update(key.split())\n",
    "\n",
    "\n",
    "# --- 2. GAME STATE LOADING (Same as before) ---\n",
    "def create_game_state_dictionary(game_state_dir):\n",
    "    filename_pattern = re.compile(r\"DiplomacyGame(\\d+)_(\\d+)_(\\w+)\\.json\")\n",
    "    game_states = defaultdict(lambda: defaultdict(dict))\n",
    "    print(f\"Starting to build game state dictionary from: {game_state_dir}\")\n",
    "    file_count = 0\n",
    "    if not os.path.exists(game_state_dir):\n",
    "        print(f\"Error: Directory not found: {game_state_dir}\")\n",
    "        return None\n",
    "    for filename in os.listdir(game_state_dir):\n",
    "        match = filename_pattern.match(filename)\n",
    "        if match:\n",
    "            game_id = int(match.group(1))\n",
    "            year = match.group(2)\n",
    "            season = match.group(3).lower() # <-- Store key as lowercase\n",
    "            file_path = os.path.join(game_state_dir, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    game_states[game_id][year][season] = data\n",
    "                    file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load or parse {filename}. Error: {e}\")\n",
    "    print(f\"Successfully loaded and indexed {file_count} game state files.\")\n",
    "    return game_states\n",
    "\n",
    "# --- 3. NEW: TEXT-TO-STATE FEATURE EXTRACTION ---\n",
    "\n",
    "def find_mentioned_territories_abbreviations(message_text, lookup_map, lookup_words):\n",
    "    \"\"\"\n",
    "    Finds all unique territory ABBREVIATIONS (e.g., 'Par') mentioned\n",
    "    in a raw text message (e.g., \"I'm going to Paris\").\n",
    "    \"\"\"\n",
    "    # Tokenize the message by finding all words\n",
    "    words = re.findall(r'\\b\\w+\\b', message_text.lower())\n",
    "    \n",
    "    # Filter for words that are part of our map to speed things up\n",
    "    potential_words = [w for w in words if w in lookup_words]\n",
    "    \n",
    "    if not potential_words:\n",
    "        return set()\n",
    "\n",
    "    text = \" \".join(potential_words)\n",
    "    mentioned_abbreviations = set()\n",
    "    \n",
    "    # Check for multi-word keys first (e.g., \"north atlantic ocean\")\n",
    "    for name, abbr in lookup_map.items():\n",
    "        if \" \" in name and name in text:\n",
    "            mentioned_abbreviations.add(abbr)\n",
    "            \n",
    "    # Check for single-word keys (e.g., \"paris\" or \"par\")\n",
    "    for word in potential_words:\n",
    "        if word in lookup_map:\n",
    "            mentioned_abbreviations.add(lookup_map[word])\n",
    "            \n",
    "    return mentioned_abbreviations\n",
    "\n",
    "def extract_features_from_state(message_data, game_state):\n",
    "    \"\"\"\n",
    "    The master feature function. Creates all game-state and\n",
    "    text-to-state contradiction features.\n",
    "    \"\"\"\n",
    "    new_features = {}\n",
    "    \n",
    "    sender = message_data['speaker'].lower()\n",
    "    receiver = message_data['receiver'].lower()\n",
    "    message_text = message_data['text']\n",
    "    \n",
    "    # --- Get state data ---\n",
    "    territories_data = game_state.get('territories', {})\n",
    "    orders_data = game_state.get('orders', {})\n",
    "    sc_text = game_state.get('sc', '')\n",
    "    \n",
    "    # === 1. & 2. SC and Unit Counts (Same as before) ===\n",
    "    sc_pattern = re.compile(r\"\\s*(\\w+)\\s+(\\d+)\\s*\")\n",
    "    all_scores = {}\n",
    "    for match in sc_pattern.finditer(sc_text):\n",
    "        country = match.group(1).lower()\n",
    "        score = int(match.group(2))\n",
    "        all_scores[country] = score\n",
    "    new_features['sender_sc'] = all_scores.get(sender, 0)\n",
    "    new_features['receiver_sc'] = all_scores.get(receiver, 0)\n",
    "\n",
    "    sender_units_orders = orders_data.get(sender.upper(), {})\n",
    "    receiver_units_orders = orders_data.get(receiver.upper(), {})\n",
    "    new_features['sender_unit_count'] = len(sender_units_orders)\n",
    "    new_features['receiver_unit_count'] = len(receiver_units_orders)\n",
    "    \n",
    "    # === 3. Score Delta (Same as before) ===\n",
    "    new_features['score_delta'] = message_data['score_delta']\n",
    "    \n",
    "    # === 4. Adjacency (Same as before) ===\n",
    "    # This requires a global DIPLOMACY_MAP, which we'll define.\n",
    "    # We'll just hard-code this feature for now to avoid the map.\n",
    "    # is_adjacent = are_players_adjacent(sender, receiver, territories_data, DIPLOMACY_MAP)\n",
    "    # new_features['are_adjacent'] = 1 if is_adjacent else 0\n",
    "    \n",
    "    # === 5. NEW: Text-to-State & Contradiction Features ===\n",
    "    \n",
    "    # Find all territories mentioned in the message\n",
    "    mentioned_abbreviations = find_mentioned_territories_abbreviations(\n",
    "        message_text, NAME_TO_ABBREVIATION, TEXT_LOOKUP_WORDS\n",
    "    )\n",
    "    new_features['territory_mention_count'] = len(mentioned_abbreviations)\n",
    "\n",
    "    # Find all territories owned by sender and receiver\n",
    "    sender_owned_territories = set()\n",
    "    receiver_owned_territories = set()\n",
    "    for territory, owner in territories_data.items():\n",
    "        if owner.lower() == sender:\n",
    "            sender_owned_territories.add(territory.upper()) # Abbreviations are case-insensitive\n",
    "        elif owner.lower() == receiver:\n",
    "            receiver_owned_territories.add(territory.upper())\n",
    "            \n",
    "    # Find all territories involved in sender's orders\n",
    "    origin_territories = set(sender_units_orders.keys()) # e.g., {'Arm', 'Bul', 'Ank'}\n",
    "    target_territories = set()\n",
    "    for unit, order in sender_units_orders.items():\n",
    "        if 'to' in order:\n",
    "            target_territories.add(order['to'].upper()) # e.g., {'Sev', 'Rum'}\n",
    "\n",
    "    # Create the new features (as 1s or 0s)\n",
    "    new_features['mentions_own_territory'] = 0\n",
    "    new_features['mentions_receiver_territory'] = 0\n",
    "    new_features['mentions_origin_unit'] = 0\n",
    "    new_features['mentions_target_territory'] = 0\n",
    "    \n",
    "    for abbr in mentioned_abbreviations:\n",
    "        abbr_upper = abbr.upper()\n",
    "        if abbr_upper in sender_owned_territories:\n",
    "            new_features['mentions_own_territory'] = 1\n",
    "        if abbr_upper in receiver_owned_territories:\n",
    "            new_features['mentions_receiver_territory'] = 1\n",
    "        if abbr_upper in origin_territories:\n",
    "            new_features['mentions_origin_unit'] = 1\n",
    "        if abbr_upper in target_territories:\n",
    "            new_features['mentions_target_territory'] = 1\n",
    "            \n",
    "    return new_features\n",
    "\n",
    "# --- 4. DATA LOADING ---\n",
    "\n",
    "def load_and_flatten_data(filepath, game_state_lookup):\n",
    "    X_text_list = []\n",
    "    y_list = []\n",
    "    X_new_features_list = [] \n",
    "\n",
    "    with jsonlines.open(filepath, 'r') as reader:\n",
    "        for game in reader:\n",
    "            game_id = game['game_id']\n",
    "            years = game['years']\n",
    "            seasons = game['seasons']\n",
    "            speakers = game['speakers']\n",
    "            \n",
    "            for i in range(len(game['messages'])):\n",
    "                sender_label = game['sender_labels'][i]\n",
    "                if sender_label == \"NOANNOTATION\":\n",
    "                    continue\n",
    "                \n",
    "                label = 1 if sender_label == False else 0\n",
    "                message_text = game['messages'][i]\n",
    "                year = years[i]\n",
    "                season = seasons[i].lower() # <-- Use lowercase for lookup\n",
    "                \n",
    "                try:\n",
    "                    current_game_state = game_state_lookup[game_id][year][season]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "                message_data = {\n",
    "                    'text': message_text,\n",
    "                    'speaker': speakers[i],\n",
    "                    'receiver': game['receivers'][i],\n",
    "                    'score_delta': int(game['game_score_delta'][i])\n",
    "                }\n",
    "                \n",
    "                new_features = extract_features_from_state(message_data, current_game_state)\n",
    "                \n",
    "                X_text_list.append(message_text)\n",
    "                y_list.append(label)\n",
    "                X_new_features_list.append(new_features)\n",
    "\n",
    "    print(f\"Loaded and processed {len(X_text_list)} samples from {filepath}.\")\n",
    "    return X_text_list, X_new_features_list, np.array(y_list)\n",
    "\n",
    "# --- 5. MODEL EVALUATION ---\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"\\n=============================================\")\n",
    "    print(f\"PERFORMANCE TEST: {model_name}\")\n",
    "    print(f\"=============================================\")\n",
    "    \n",
    "    f1_lie_class = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"F1 Score (Lie Class): {f1_lie_class:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"           Predicted 0 (Truth)  |  Predicted 1 (Lie)\")\n",
    "    print(f\"Actual 0:    {cm[0][0]:<20} | {cm[0][1]:<20}\")\n",
    "    print(f\"Actual 1:    {cm[1][0]:<20} | {cm[1][1]:<20}\")\n",
    "    \n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Class 0 (Truth)', 'Class 1 (Lie)'], zero_division=0))\n",
    "\n",
    "# --- 6. NEW: NEURAL NETWORK PIPELINE ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Set Paths ---\n",
    "    GAME_STATE_DIRECTORY = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"moves\") \n",
    "\n",
    "    TRAIN_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"train.jsonl\") \n",
    "    VAL_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"validation.jsonl\") \n",
    "    TEST_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw\", \"2020_acl_diplomacy\", \"data\", \"test.jsonl\") \n",
    "    \n",
    "    VOCAB_SIZE = 10000\n",
    "    MAX_LEN = 120\n",
    "    EMBEDDING_DIM = 64\n",
    "    LSTM_UNITS = 64\n",
    "\n",
    "    # --- Load Game State Dictionary ---\n",
    "    game_state_lookup = create_game_state_dictionary(GAME_STATE_DIRECTORY)\n",
    "    if not game_state_lookup:\n",
    "        print(\"Game state lookup dictionary is empty. Exiting.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # --- Load all data ---\n",
    "    print(\"Loading all datasets...\")\n",
    "    X_train_text, X_train_features_list, y_train = load_and_flatten_data(TRAIN_FILE, game_state_lookup)\n",
    "    X_val_text, X_val_features_list, y_val = load_and_flatten_data(VAL_FILE, game_state_lookup)\n",
    "    X_test_text, X_test_features_list, y_test = load_and_flatten_data(TEST_FILE, game_state_lookup)\n",
    "\n",
    "    if len(X_train_text) == 0:\n",
    "        print(\"CRITICAL ERROR: No training data was loaded.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 1. PREPARE TEXT INPUT (Tokenizer + Padding) ---\n",
    "    print(\"\\nPreprocessing Text Data...\")\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train_text) # Fit ONLY on training text\n",
    "    \n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val_text)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "    \n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    print(f\"Text data shape: {X_train_pad.shape}\")\n",
    "\n",
    "    # --- 2. PREPARE GAME STATE INPUT (DictVectorizer + Scaling) ---\n",
    "    print(\"Preprocessing Game State Data...\")\n",
    "    feature_vectorizer = DictVectorizer(sparse=False)  # Use dense arrays instead of sparse\n",
    "    X_train_features_numeric = feature_vectorizer.fit_transform(X_train_features_list)\n",
    "    X_val_features_numeric = feature_vectorizer.transform(X_val_features_list)\n",
    "    X_test_features_numeric = feature_vectorizer.transform(X_test_features_list)\n",
    "    \n",
    "    scaler = StandardScaler()  # Now works with dense arrays\n",
    "    X_train_features_scaled = scaler.fit_transform(X_train_features_numeric)\n",
    "    X_val_features_scaled = scaler.transform(X_val_features_numeric)\n",
    "    X_test_features_scaled = scaler.transform(X_test_features_numeric)\n",
    "    \n",
    "    num_game_features = X_train_features_scaled.shape[1]\n",
    "    print(f\"Game state feature names: {feature_vectorizer.feature_names_}\")\n",
    "    print(f\"Game state data shape: {X_train_features_scaled.shape}\")\n",
    "\n",
    "    # --- 3. DEFINE THE DUAL-INPUT MODEL ---\n",
    "    print(\"\\nBuilding Neural Network Model...\")\n",
    "    \n",
    "    # Input 1: Text Sequences\n",
    "    text_input_layer = Input(shape=(MAX_LEN,), name='text_input')\n",
    "    text_model = Embedding(input_dim=VOCAB_SIZE+1, output_dim=EMBEDDING_DIM, input_length=MAX_LEN)(text_input_layer)\n",
    "    text_model = SpatialDropout1D(0.2)(text_model) # Dropout for embeddings\n",
    "    text_model = LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2, name='lstm_layer')(text_model)\n",
    "    \n",
    "    # Input 2: Game State Features\n",
    "    game_state_input_layer = Input(shape=(num_game_features,), name='game_state_input')\n",
    "    \n",
    "    # Combine (Concatenate)\n",
    "    combined = concatenate([text_model, game_state_input_layer], name='concatenate')\n",
    "    \n",
    "    # Classifier Head\n",
    "    output = Dense(64, activation='relu', name='dense_1')(combined)\n",
    "    output = Dropout(0.5, name='dropout')(output)\n",
    "    output = Dense(1, activation='sigmoid', name='output_layer')(output)\n",
    "    \n",
    "    model = Model(inputs=[text_input_layer, game_state_input_layer], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # --- 4. HANDLE CLASS IMBALANCE ---\n",
    "    # This is CRITICAL for getting a good F1 score\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(f\"Class Weights (for balancing): {class_weight_dict}\")\n",
    "\n",
    "    # --- 5. TRAIN THE MODEL ---\n",
    "    print(\"\\nTraining the model...\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=3,         # Stop after 3 epochs with no improvement\n",
    "        restore_best_weights=True # Keep the best model, not the last\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_train_pad, X_train_features_scaled], # Pass inputs as a list\n",
    "        y_train,\n",
    "        epochs=15,\n",
    "        batch_size=64,\n",
    "        validation_data=([X_val_pad, X_val_features_scaled], y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight_dict # Apply the weights\n",
    "    )\n",
    "\n",
    "    # --- 6. EVALUATE THE FINAL MODEL ---\n",
    "    print(\"\\nEvaluating final model on test set...\")\n",
    "    y_pred_probs = model.predict([X_test_pad, X_test_features_scaled])\n",
    "    evaluate_model(\"Dual-Input Neural Network\", y_test, y_pred_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
