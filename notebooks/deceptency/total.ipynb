{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8431db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Model Klas\u00f6r\u00fc: c:\\work environment\\Projects\\amazon-spam-review\\models\\deceptency\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "base_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_path = os.path.join(base_path, \"data\", \"processed\", \"diplomacy\")\n",
    "model_dir = os.path.join(base_path, \"models\", \"deceptency\")\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(data_path, \"train_processed.parquet\"))\n",
    "test_df = pd.read_parquet(os.path.join(data_path, \"test_processed.parquet\"))\n",
    "\n",
    "# Prepare X_test with all features for sklearn pipelines\n",
    "categorical_features = [\"speaker\", \"receiver\", \"season\"]\n",
    "# Ensure correct types for categorical columns\n",
    "cat_types = {col: 'object' for col in categorical_features}\n",
    "test_df = test_df.astype(cat_types)\n",
    "\n",
    "X_test = test_df.drop('target', axis=1)\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "def evaluate_model(y_test, y_pred, model_name=\"Model\"):\n",
    "    print(f\"\\n--- {model_name} Results ---\")\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, target_names=['Truth', 'Deception'], output_dict=True)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Truth', 'Deception']))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Truth', 'Deception'], yticklabels=['Truth', 'Deception'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247400eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\u00fckleniyor: c:\\work environment\\Projects\\amazon-spam-review\\models\\deceptency\\logistic_regression_diplomacy.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 5000 features, but ColumnTransformer is expecting 12 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2806\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2806\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m _num_features(X)\n\u001b[0;32m   2807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:362\u001b[0m, in \u001b[0;36m_num_features\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(message)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to find the number of features from X of type pandas.core.series.Series with shape (2741,)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     y_pred_lr \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict(X_test_text)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py:658\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 658\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1094\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2809\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reset \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 2809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2810\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX does not contain any features, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2811\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is expecting \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2812\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2813\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2814\u001b[0m \u001b[38;5;66;03m# If the number of features is not defined and reset=True,\u001b[39;00m\n\u001b[0;32m   2815\u001b[0m \u001b[38;5;66;03m# then we skip this check\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: X does not contain any features, but ColumnTransformer is expecting 12 features",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     y_pred_lr \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict(X_test_text)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     y_pred_lr \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n\u001b[0;32m     12\u001b[0m plot_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test, y_pred_lr)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py:658\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 658\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1094\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[0;32m   1097\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 5000 features, but ColumnTransformer is expecting 12 features as input."
     ]
    }
   ],
   "source": [
    "# --- LOGISTIC REGRESSION ---\n",
    "path = os.path.join(model_dir, \"logistic_regression_diplomacy.pkl\")\n",
    "print(f\"Loading: {path}\")\n",
    "model = joblib.load(path)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test, y_pred, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NAIVE BAYES ---\n",
    "model_path = os.path.join(model_dir, \"naive_bayes_deceptency.pkl\")\n",
    "print(f\"Y\u00fckleniyor: {model_path}\")\n",
    "nb_model = joblib.load(model_path)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "plot_results(\"Naive Bayes\", y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SVM ---\n",
    "model_path = os.path.join(model_dir, \"svm_diplomacy.pkl\")\n",
    "print(f\"Y\u00fckleniyor: {model_path}\")\n",
    "svm_model = joblib.load(model_path)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "plot_results(\"SVM\", y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RANDOM FOREST ---\n",
    "model_path = os.path.join(model_dir, \"random_forest_diplomacy.pkl\")\n",
    "print(f\"Y\u00fckleniyor: {model_path}\")\n",
    "rf_model = joblib.load(model_path)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "plot_results(\"Random Forest\", y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LIGHTGBM ---\n",
    "model_path = os.path.join(model_dir, \"lightgbm_diplomacy.pkl\")\n",
    "print(f\"Y\u00fckleniyor: {model_path}\")\n",
    "lgbm_model = joblib.load(model_path)\n",
    "\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "plot_results(\"LightGBM\", y_test, y_pred_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEURAL NETWORK (MLP) ---\n",
    "model_path = os.path.join(model_dir, \"neural_network_diplomacy.pkl\")\n",
    "print(f\"Y\u00fckleniyor: {model_path}\")\n",
    "nn_model = joblib.load(model_path)\n",
    "\n",
    "y_pred_nn = nn_model.predict(X_test)\n",
    "\n",
    "plot_results(\"Neural Network (MLP)\", y_test, y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52312dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM MODEL ---\n",
    "model_path = os.path.join(model_dir, \"lstm_diplomacy.h5\")\n",
    "print(f\"Y\u00fckleniyor: {model_path}\")\n",
    "\n",
    "# LSTM Haz\u0131rl\u0131\u011f\u0131 (E\u011fitimdeki parametrelerle AYNI olmal\u0131)\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 150\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['message_text']) # Tokenizer'\u0131 e\u011fitilmi\u015f veriden kuruyoruz\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['message_text'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Modeli Y\u00fckle ve Tahmin Et\n",
    "lstm_model = load_model(model_path)\n",
    "y_pred_prob_lstm = lstm_model.predict(X_test_pad)\n",
    "y_pred_lstm = (y_pred_prob_lstm > 0.5).astype(int)\n",
    "\n",
    "plot_results(\"LSTM\", y_test, y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d343ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BERT MODEL ---\n",
    "model_path = os.path.join(model_dir, \"bert_diplomacy\") # Klas\u00f6r yolu\n",
    "print(f\"Y\u00fckleniyor: {model_path}\")\n",
    "\n",
    "# BERT Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LEN_BERT = 128\n",
    "\n",
    "def encode_bert(texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = bert_tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=MAX_LEN_BERT,\n",
    "            padding='max_length', truncation=True,\n",
    "            return_attention_mask=True, return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'][0])\n",
    "        attention_masks.append(encoded['attention_mask'][0])\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "X_test_ids, X_test_masks = encode_bert(test_df['message_text'])\n",
    "\n",
    "# Modeli Y\u00fckle\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(model_path)\n",
    "y_pred_logits_bert = bert_model.predict([X_test_ids, X_test_masks]).logits\n",
    "y_pred_prob_bert = tf.sigmoid(y_pred_logits_bert).numpy().flatten()\n",
    "y_pred_bert = (y_pred_prob_bert > 0.5).astype(int)\n",
    "\n",
    "plot_results(\"BERT\", y_test, y_pred_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}