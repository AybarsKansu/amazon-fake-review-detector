{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b042ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3918468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\work environment\\Projects\\amazon-spam-review\n"
     ]
    }
   ],
   "source": [
    "base_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_path = os.path.join(base_path, \"data\", \"processed\", \"diplomacy\")\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f08e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"speaker\", \"receiver\", \"season\"]\n",
    "col_types = {col: 'object' for col in categorical_features}\n",
    "\n",
    "data = pd.read_parquet(os.path.join(data_path, \"diplomacy_processed.parquet\"))\n",
    "data = data.astype(col_types)\n",
    "train_df = pd.read_parquet(os.path.join(data_path, \"train_processed.parquet\"))\n",
    "train_df = train_df.astype(col_types)\n",
    "val_df = pd.read_parquet(os.path.join(data_path, \"val_processed.parquet\"))\n",
    "val_df = val_df.astype(col_types)\n",
    "test_df = pd.read_parquet(os.path.join(data_path, \"test_processed.parquet\"))\n",
    "test_df = test_df.astype(col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9e56ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['game_id', 'speaker', 'receiver', 'message_text', 'sender_intention',\n",
      "       'game_score', 'game_score_delta', 'year', 'season', 'original_fold',\n",
      "       'target', 'cleaned_text', 'message_length'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK FOR EXISTING MODEL\n",
    "model_path = os.path.join(model_dir, 'lightgbm_diplomacy.pkl')\n",
    "try:\n",
    "    best_model = joblib.load(model_path)\n",
    "    print(f'Model loaded from {model_path}')\n",
    "    print('Skipping training and proceeding to evaluation...')\n",
    "    skip_training = True\n",
    "except FileNotFoundError:\n",
    "    print('No existing model found. Will train a new model.')\n",
    "    skip_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0bedf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('target', axis=1)\n",
    "y_train = train_df['target']\n",
    "X_val = val_df.drop('target', axis=1)\n",
    "y_val = val_df['target']\n",
    "X_test = test_df.drop('target', axis=1)\n",
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1085aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 13132\n"
     ]
    }
   ],
   "source": [
    "print(f\"train set size: {len(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b6ba425",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"game_score\", \"game_score_delta\", \"year\", \"message_length\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "  (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "  (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = [\"speaker\", \"receiver\", \"season\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "  (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"most_frequent\")),\n",
    "  (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "text_feature = \"cleaned_text\"\n",
    "text_transformer = TfidfVectorizer()\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "  ('text', text_transformer, text_feature),\n",
    "  ('num', numeric_transformer, numeric_features),\n",
    "  ('cat', categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b30527",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "# because gridsearch expects the formal as x and y, we concat train and validation set; but modify how gridsearch splits data\n",
    "split_index = [-1] * len(X_train) + [0] * len(X_val)\n",
    "ps = PredefinedSplit(test_fold=split_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b77365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to cover data imbalance with generating artificial lies with smote\n",
    "pipeline_lgbm_smote = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', SMOTE(random_state=42)),\n",
    "    ('model', lgb.LGBMClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88614ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    'preprocessor__text__max_features': [5000, 15000],\n",
    "    'model__n_estimators': [100, 200, 500],\n",
    "    'model__learning_rate': [0.1, 0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62e513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipeline_lgbm_smote, \n",
    "    param_grid_lgbm, \n",
    "    cv=ps,\n",
    "    scoring='f1_weighted', \n",
    "    n_jobs=4, \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee505aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 18 candidates, totalling 18 fits\n",
      "[LightGBM] [Info] Number of positive: 13901, number of negative: 13901\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115719\n",
      "[LightGBM] [Info] Number of data points in the train set: 27802, number of used features: 1990\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training time: 1.36 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth       0.91      0.99      0.95      2501\n",
      "   Deception       0.33      0.03      0.05       240\n",
      "\n",
      "    accuracy                           0.91      2741\n",
      "   macro avg       0.62      0.51      0.50      2741\n",
      "weighted avg       0.86      0.91      0.87      2741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train_val, y_train_val)\n",
    "    end_time = time.time()\n",
    "    training_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training time: {training_time_minutes:.2f} minutes\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Truth', 'Deception']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4790e1",
   "metadata": {},
   "source": [
    "Not good at all. LightGBM model with smote calls everything truth so deceptency detection is miserable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d6d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- SAVE MODEL AND REPORT ---\n",
    "# model_dir = os.path.join(base_path, \"models\", \"deceptency\")\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "# print(\"Model saved.\")\n",
    "\n",
    "# # Metrics\n",
    "# from sklearn.metrics import classification_report\n",
    "# report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "# result_data = {\n",
    "#     'category': 'Deceptency_lightgbm_diplomacy',\n",
    "#     'best_cv_f1_score': grid_search.best_score_,\n",
    "#     'best_params': str(grid_search.best_params_),\n",
    "#     'test_accuracy': report['accuracy'],\n",
    "#     'test_f1_truth': report['0']['f1-score'],\n",
    "#     'test_precision_truth': report['0']['precision'],\n",
    "#     'training_time_minutes': training_time_minutes\n",
    "# }\n",
    "# results_file = os.path.join(base_path, \"reports\", \"model_results_deceptency_lightgbm_diplomacy.csv\")\n",
    "# result_df = pd.DataFrame([result_data])\n",
    "# header = not os.path.exists(results_file)\n",
    "# result_df.to_csv(results_file, mode='a', header=header, index=False)\n",
    "# print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, os.path.join(model_dir, \"lightgbm_diplomacy.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a23d8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to punish missing lies more\n",
    "pipeline_lgbm_weighted = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e19df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    'preprocessor__text__max_features': [500, 1000, 5000],\n",
    "    'model__n_estimators': [2, 5, 10, 100, 500],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03640dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipeline_lgbm_weighted, \n",
    "    param_grid_lgbm, \n",
    "    cv=ps, \n",
    "    scoring='f1',\n",
    "    n_jobs=4, \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc898c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 75 candidates, totalling 75 fits\n",
      "[LightGBM] [Info] Number of positive: 647, number of negative: 13901\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37845\n",
      "[LightGBM] [Info] Number of data points in the train set: 14548, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth       0.93      0.70      0.80      2501\n",
      "   Deception       0.13      0.45      0.20       240\n",
      "\n",
      "    accuracy                           0.68      2741\n",
      "   macro avg       0.53      0.58      0.50      2741\n",
      "weighted avg       0.86      0.68      0.75      2741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_val, y_train_val)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Truth', 'Deception']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d4270",
   "metadata": {},
   "source": [
    "Although results are significantly better than last time, deceptency detection is still not good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b7f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__learning_rate': 0.1, 'model__n_estimators': 10, 'preprocessor__text__max_features': 5000}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b96ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lgbm_enriched = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e2e6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    'preprocessor__text__max_features': [5000],\n",
    "    'model__n_estimators': [100],\n",
    "    'model__learning_rate': [0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f263b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipeline_lgbm_enriched, \n",
    "    param_grid_lgbm, \n",
    "    cv=ps, \n",
    "    scoring='f1_macro', \n",
    "    n_jobs=4, \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e45ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[LightGBM] [Info] Number of positive: 647, number of negative: 13901\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 37845\n",
      "[LightGBM] [Info] Number of data points in the train set: 14548, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "--- FİNAL TEST SETİ PERFORMANS RAPORU ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Doğru Mesaj (0)       0.92      0.90      0.91      2501\n",
      "Yalan Mesaj (1)       0.17      0.23      0.19       240\n",
      "\n",
      "       accuracy                           0.84      2741\n",
      "      macro avg       0.55      0.56      0.55      2741\n",
      "   weighted avg       0.86      0.84      0.85      2741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_val, y_train_val)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(\"\\n--- FİNAL TEST SETİ PERFORMANS RAPORU ---\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Doğru Mesaj (0)', 'Yalan Mesaj (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0a83898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\work environment\\Projects\\amazon-spam-review\n"
     ]
    }
   ],
   "source": [
    "BASE_PROJECT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "print(BASE_PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0cb3d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "BASE_PROJECT_PATH = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "model_dir = os.path.join(BASE_PROJECT_PATH, \"models\", \"deceptency\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "joblib.dump(best_model, os.path.join(model_dir, \"lightgbm_diplomacy.pkl\"))\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Save Results to CSV\n",
    "report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "result_data = {\n",
    "    'category': 'Deceptency_LightGBM',\n",
    "    'best_cv_f1_score': grid_search.best_score_,\n",
    "    'test_accuracy': report['accuracy'],\n",
    "    'test_f1_truth': report['0']['f1-score'],\n",
    "    'test_precision_truth': report['0']['precision'],\n",
    "    'best_params': str(grid_search.best_params_),\n",
    "    'training_time_minutes': training_time_minutes or 0\n",
    "}\n",
    "\n",
    "results_file = os.path.join(BASE_PROJECT_PATH, \"reports\", \"model_results_deceptency_lightgbm.csv\")\n",
    "result_df = pd.DataFrame([result_data])\n",
    "header = not os.path.exists(results_file)\n",
    "result_df.to_csv(results_file, mode='a', header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_section_enriched",
   "metadata": {},
   "source": [
    "## Training on Enriched Data (Additional Experiment)\n",
    "Retraining LightGBM using the enriched dataset (Linguistic + Moves features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "load_enriched",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Enriched Data...\n",
      "Enriched Train/Val Shape: (14548, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Enriched Data...\")\n",
    "train_df_en = pd.read_parquet(os.path.join(data_path, \"train_enriched.parquet\"))\n",
    "val_df_en = pd.read_parquet(os.path.join(data_path, \"val_enriched.parquet\"))\n",
    "test_df_en = pd.read_parquet(os.path.join(data_path, \"test_enriched.parquet\"))\n",
    "\n",
    "# Ensure columns are object if needed for categories\n",
    "for col in categorical_features:\n",
    "    train_df_en[col] = train_df_en[col].astype('object')\n",
    "    val_df_en[col] = val_df_en[col].astype('object')\n",
    "    test_df_en[col] = test_df_en[col].astype('object')\n",
    "\n",
    "X_train_en = train_df_en.drop('target', axis=1)\n",
    "y_train_en = train_df_en['target']\n",
    "X_val_en = val_df_en.drop('target', axis=1)\n",
    "y_val_en = val_df_en['target']\n",
    "X_test_en = test_df_en.drop('target', axis=1)\n",
    "y_test_en = test_df_en['target']\n",
    "\n",
    "X_train_val_en = pd.concat([X_train_en, X_val_en])\n",
    "y_train_val_en = pd.concat([y_train_en, y_val_en])\n",
    "split_index_en = [-1] * len(X_train_en) + [0] * len(X_val_en)\n",
    "ps_en = PredefinedSplit(test_fold=split_index_en)\n",
    "\n",
    "print(f\"Enriched Train/Val Shape: {X_train_val_en.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "new_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Numeric Features\n",
    "new_numeric_features = [\n",
    "    'sentiment_polarity', 'sentiment_subjectivity',\n",
    "    'n_question_marks', 'n_exclamation_marks', 'n_ellipses',\n",
    "    'n_i_pronouns', 'n_we_pronouns', \n",
    "    'avg_word_length', 'uppercase_ratio',\n",
    "    'n_orders', 'n_support', 'n_hold', 'n_move_fails'\n",
    "]\n",
    "all_numeric_features = numeric_features + new_numeric_features\n",
    "\n",
    "preprocessor_en = ColumnTransformer(transformers=[\n",
    "  ('text', text_transformer, text_feature),\n",
    "  ('num', numeric_transformer, all_numeric_features),\n",
    "  ('cat', categorical_transformer, categorical_features)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "pipeline_lgbm_en = Pipeline([\n",
    "    ('preprocessor', preprocessor_en),\n",
    "    ('model', lgb.LGBMClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "train_en",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM on Enriched Data...\n",
      "Fitting 1 folds for each of 2 candidates, totalling 2 fits\n",
      "[LightGBM] [Info] Number of positive: 647, number of negative: 13901\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 38958\n",
      "[LightGBM] [Info] Number of data points in the train set: 14548, number of used features: 1012\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Enriched Training Time: 0.05 min\n",
      "\n",
      "--- ENRICHED TEST SET PERFORMANCE ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth       0.92      0.91      0.91      2501\n",
      "   Deception       0.18      0.21      0.19       240\n",
      "\n",
      "    accuracy                           0.85      2741\n",
      "   macro avg       0.55      0.56      0.55      2741\n",
      "weighted avg       0.86      0.85      0.85      2741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aybars\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train on Enriched Data\n",
    "print(\"Training LightGBM on Enriched Data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Using smaller grid for speed\n",
    "param_grid_lgbm_en = {\n",
    "    'preprocessor__text__max_features': [5000],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "grid_search_en = GridSearchCV(\n",
    "    pipeline_lgbm_en, \n",
    "    param_grid_lgbm_en, \n",
    "    cv=ps_en, \n",
    "    scoring='f1_macro', \n",
    "    n_jobs=4, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search_en.fit(X_train_val_en, y_train_val_en)\n",
    "\n",
    "end_time = time.time()\n",
    "train_time_en = (end_time - start_time) / 60\n",
    "print(f\"Enriched Training Time: {train_time_en:.2f} min\")\n",
    "\n",
    "best_model_en = grid_search_en.best_estimator_\n",
    "y_pred_test_en = best_model_en.predict(X_test_en)\n",
    "\n",
    "print(\"\\n--- ENRICHED TEST SET PERFORMANCE ---\")\n",
    "print(classification_report(y_test_en, y_pred_test_en, target_names=['Truth', 'Deception']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c792500",
   "metadata": {},
   "source": [
    "Enriched data does not provide enough information to train a model when we consider the additional burden that it would add to the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
